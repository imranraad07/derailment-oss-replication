title,speaker,issue_id,comment_sequence,comment_unique_id,toxic,text,is_first_comment,tbdf
"reading the variable ""latest_release"" before ""deploy"" task happens makes capistrano BREAK",knocte,13258430,1,13258430,0,"If you set up some task this way:

before 'deploy', 'some_task', 

And in some_task you simply read the ""latest_release"" variable (in a puts line for instance), then:

Current results:
a) If it's the first deploy ever in capistrano (the www folder doesn't exist), capistrano breaks completely, it cannot deploy.
b) If it's not the first deploy, then it will make capistrano change its behaviour with regards to the ""current"" symlink, because it will point to the previous release instead of the last one (after the deploy happened).

Expected results:
a) It should work.
b) It should point current to the latest release.

This is a big fuckup IMHO.
",True,Vulgarity
"reading the variable ""latest_release"" before ""deploy"" task happens makes capistrano BREAK",leehambley,13258430,2,16461106,0,"> This is a big fuckup IMHO.

Profanity aside, you may have a point. I'll take a patch.
",False,0
"reading the variable ""latest_release"" before ""deploy"" task happens makes capistrano BREAK",knocte,13258430,3,16461191,0,"Why do you close the issue?
",False,0
"reading the variable ""latest_release"" before ""deploy"" task happens makes capistrano BREAK",leehambley,13258430,4,16461276,1,"Because you don't offer a patch, and profanity really pisses me off.
",False,Bitter frustration
Security issue with param escaping?,thekiur,27120442,1,27120442,0,"Hi, im running node-mysql latest on node-latest.
Somebody using the acunetix vulnerability scanner has triggered this error:
UNKNOWN COLUMN '$acunetix' IN WHERE CLAUSE.
The query: SELECT id, email FROM accounts WHERE username = ?

How is this possible? Its very dangerous to our application, please respond quickly.
",True,Impatience
Security issue with param escaping?,mk-pmb,27120442,2,34424093,0,"The problem seems to be about params that are not strings. Although I'll continue to sanitize all my user inputs (to avoid username impersonation attacks like `adm—ñn` posing as `admin`), I'd expect the query engine to convert any param to a string if it should have been one in the first place. If it already was, `String(param)` should be of low cost.
",False,0
Security issue with param escaping?,sidorares,27120442,3,34425569,0,"@thekiur @mk-pmb can you post code samples?
",False,0
Security issue with param escaping?,thekiur,27120442,4,34425679,0,"We can confirm that the problem is caused by passing objects to the query call.
The objects come from the express bodyParser middleware.
We were simply passing req.body.username as the parameter for that query.
The acunetic vulnerability tester injected an object there.
We are not sure on the severity of this issue, but its unexpected to say atleast.
As we experienced, this can crash a live application in production mode if you dont expect any db errors.

There is no code to show: its as simple as passing a req.body.something to the .query call of node-mysql when using express with the bodyparser middleware. Running the vulnerability scanner against https://gist.github.com/ssafejava/9a2d77704712a8769322 causes the exception to be thrown.
",False,0
Security issue with param escaping?,dougwilson,27120442,5,34441093,0,"This is not an issue with escaping with this library; this library is properly escaping all values and column names. The security issue is just with the way you are combining express and this library, such that you were expecting to get a string from express, so you were only expecting the `?` to expand according to string rules.

`req.body` properties can be anything with `bodyParser` and as such you need to at least verify what you are using is a string before passing to your query.
",False,0
Security issue with param escaping?,mk-pmb,27120442,6,34467169,0,"I consider prepared statements as intended to mitigate lack of input validation in the params in general. Therefor, limiting it to the case where input has already been validated as being a string, in my opinion misses the point.
Yours, MK
",False,0
Security issue with param escaping?,dougwilson,27120442,7,34470964,0,"These are not prepared statements, they are done client-side and have various rules for how `?` is replaced depending on the data type, which is documented. If you want to be sure you are using the string-based `?` replacement though the API, you have to give the API a string. If you don't want to validate at all, you can use the `String()` function:
`conn.query('SELECT * FROM user WHERE username = ?', [String(req.body.username)]')`

The _purpose_ if it doing stuff different for objects is to help people who want to easily use `SET`:
`conn.query('UPDATE user SET ? WHERE id = ?', [{username: 'bob', password: '1234'}, 43])`

Please see the ""Different value types are escaped differently, here is how:"" section in https://github.com/felixge/node-mysql#escaping-query-values
",False,0
Security issue with param escaping?,mk-pmb,27120442,8,34478411,0,"I see. Looks like an unlucky case of embrace and extend. I wish you had opted for something like `??` in that case. Probably too late to change the interface?

Edit: Not really embrace and extend, as you wrote they aren't prepared statements. Rather just a pitfall for people who learn from tutorials and conclude topical similarity from visual similarity.
Edit 2: I see, `??` is already used for column names.
",False,0
Security issue with param escaping?,mk-pmb,27120442,9,38685690,1,"I can't see how it's the type system's fault when programmers assume that a mechanism that looks like prepared statements will defuse any data they pass in. Let's at least blame it at the programmers for trusting visual similarity instead of reading the manual thoroughly.
",False,Identity attacks/Name-Calling
Feature Request: xBR shader and xBRZ filter for GSDX in both PS2 and PSX mode.,ghost,30381958,1,30381958,0,"Could you please implement  xBR shader or xBRZ filter or both in GSDX plugin. It would be very beneficial for both PS2 and PSX 2D and sprite-based games.
xBR and xBRZ are pixel art scaling algorithms ,they give best results in 2D/sprite based games with low resolution textures and games with pre-rendered backgrounds which dont upscale well with higher internal resolutions but they also give good results in 3D games.  xBR/xBRZ are already used with good results in emulators like Retroarch , Higan, Desmume and PPSSPP.
Here is explanation:http://code.google.com/p/2dimagefilter/wiki/ImageScaling#xBR
http://www.vogons.org/viewtopic.php?t=34125
Here is newest xBR source code including hybrid variants: https://github.com/libretro/common-shaders
Source code for xBRZ is in source code of HqMAME : https://sourceforge.net/projects/hqmame/  ,http://sourceforge.net/projects/hqmame/files/xBRZ.zip/download,  Spline36: http://code.google.com/p/remote-joy-lite-fix/source/browse/trunk/RemoteJoyLite_pc/spline36.psh
https://github.com/xbmc/xbmc/tree/master/xbmc/cores/VideoRenderers/VideoShaders
Here is comparison for 3D graphics:http://blog.metaclassofnil.com/?p=306
Here is official tutorial about xBR: http://www.libretro.com/forums/viewtopic.php?f=6&t=134
http://forum.zdoom.org/viewtopic.php?f=19&t=37373&sid=57269f5e32514a88a5d5252839c9ff6a&start=45
Some 2D graphics of old version of xBR: http://imgur.com/a/ZZiiH
I also found interesting algorithm Libdepixelize: http://bazaar.launchpad.net/~vinipsmaker/libdepixelize/trunk/revision/184
http://vinipsmaker.wordpress.com/tag/libdepixelize/
https://sourceforge.net/projects/inkscape/
There is also ''Ours'' but I cant find source code for it anywhere: http://research.microsoft.com/en-us/um/people/kopf/pixelart/supplementary/
 But Libdepixelize and Ours both use Kopf-Lischinski algorithm so they should have similiar effects.
http://www.mediafire.com/download/22o6ahnchkbzhef/Shaders.rar
http://www.mediafire.com/download/86bo6bl66cnwv2j/chromaNEDI.rar
https://github.com/jpsdr/NNEDI3
http://forum.doom9.org/showthread.php?t=170727
",True,0
Feature Request: xBR shader and xBRZ filter for GSDX in both PS2 and PSX mode.,DonelBueno,30381958,2,38958157,0,"It would work pretty well as a texture scaler too.

PPSSPP already does this.
",False,0
Feature Request: xBR shader and xBRZ filter for GSDX in both PS2 and PSX mode.,ghost,30381958,3,39003420,0,"I think best option would be to use  xBR in hardware mode and xBRZ can work in both software and hardware modes.
",False,0
Feature Request: xBR shader and xBRZ filter for GSDX in both PS2 and PSX mode.,Squall-Leonhart,30381958,4,39018418,0,"Instead of asking, try submitting a patch with your desired changes.
",False,0
Feature Request: xBR shader and xBRZ filter for GSDX in both PS2 and PSX mode.,ghost,30381958,5,39020604,0,"Unfortunately I am not programmer and I am not skilled enough to implement it myself.
",False,0
Feature Request: xBR shader and xBRZ filter for GSDX in both PS2 and PSX mode.,DanAE111,30381958,6,42581963,0,"This is something i would like to see in the future, the way its implemented in PPSSPP is great. Although really not very necessary for PS2 emulation just a nice extra.
",False,0
Feature Request: xBR shader and xBRZ filter for GSDX in both PS2 and PSX mode.,ghost,30381958,7,42582779,0,"It would be great for textures.
",False,0
Feature Request: xBR shader and xBRZ filter for GSDX in both PS2 and PSX mode.,Squall-Leonhart,30381958,8,42586983,0,"It wouldn't actually... since its design/function is for sprites, not textures.
",False,0
Feature Request: xBR shader and xBRZ filter for GSDX in both PS2 and PSX mode.,ghost,30381958,9,42634624,0,"Its possible to use it for textures as well.
",False,0
Feature Request: xBR shader and xBRZ filter for GSDX in both PS2 and PSX mode.,ghost,30381958,10,42634674,0,"PPSSPP is using xBRZ for textures and it looks very good.
",False,0
Feature Request: xBR shader and xBRZ filter for GSDX in both PS2 and PSX mode.,Dokman,30381958,11,42645331,0,"yes but one more option that the team don't wan't to do is mipmap at hardware mode in gsdx but i think the mipmap are better than xBRZ
",False,0
Feature Request: xBR shader and xBRZ filter for GSDX in both PS2 and PSX mode.,ghost,30381958,12,42647630,0,"Mipmapping would be useful but is pretty basic xBRZ may be better. Techniques which are clearly better are Tessellation, Displacement Mapping and Parallax Occlusion Mapping.
",False,0
Feature Request: xBR shader and xBRZ filter for GSDX in both PS2 and PSX mode.,LasagnaPie,30381958,13,43626599,0,"xBRZ makes textures and sprites ugly, I rather have Nearest. 
",False,Mocking
Feature Request: xBR shader and xBRZ filter for GSDX in both PS2 and PSX mode.,ghost,30381958,14,43628117,0,"xBR/xBRZ looks ugly only in your subjective opinion. Nearest-neighbor is primitive technique which looks blurry and pixelated,
",False,Impatience
Feature Request: xBR shader and xBRZ filter for GSDX in both PS2 and PSX mode.,LasagnaPie,30381958,15,43635582,0,"xBR/xBRZ are ugly because computers are not artists, they ruin the artwork of games. 
",False,Bitter frustration
Feature Request: xBR shader and xBRZ filter for GSDX in both PS2 and PSX mode.,ghost,30381958,16,43637627,0,"I am not sure what you mean but not everyone wants to use emulators with ugly native graphics.
PCSX2 already has option to increase internal resolution but xBR/xBRZ would be very useful for 2D sprites, pre-rendered backgrounds and textures.
",False,Bitter frustration
Feature Request: xBR shader and xBRZ filter for GSDX in both PS2 and PSX mode.,Dokman,30381958,17,43638741,0,"yes that's true ratchet and clank games have this problem and are very ugly only works in software mode and it's freaking me out with a amd fx 8350 ¬¨¬¨ a points of fps of 20 or 30
",False,Impatience
Feature Request: xBR shader and xBRZ filter for GSDX in both PS2 and PSX mode.,LasagnaPie,30381958,18,43639134,0,"You mean Native graphics, most of which created professionally by artist then you want to ruin it with an over-exaggerated interpolation. 
",False,Bitter frustration
Feature Request: xBR shader and xBRZ filter for GSDX in both PS2 and PSX mode.,LasagnaPie,30381958,19,43639820,0,"Ratchet & Clank was my first PS2 game I ever owned, i have being waiting years for it to be fixed in PCSX2 
",False,0
Feature Request: xBR shader and xBRZ filter for GSDX in both PS2 and PSX mode.,ghost,30381958,20,43645481,0,"Developers wanted games to look like they look in high resolution but were limited by underpowered hardware.
",False,0
Feature Request: xBR shader and xBRZ filter for GSDX in both PS2 and PSX mode.,LasagnaPie,30381958,21,43650840,1,"Developers certainly not want to apply a silly image interpolation like xBR to ruin all their artwork 
",False,Insulting
Vagrant box downloads extremely slow,spkane,57258770,1,57258770,0,"When relying on vagrant to download a box I frequently see connection speeds like this:

```
default: Downloading: http://boxes.example.com/vagrant/boxes/c6/packer_c6_2.5.2_virtualbox.box
default: Progress: 20% (Rate: 179k/s, Estimated time remaining: 0:41:37)
```

(Rate: **179k/s**)

Yet when I use wget to the same URL:

```
wget http://boxes.example.com/vagrant/boxes/c6/packer_c6_2.5.2_virtualbox.box
--2015-02-10 09:52:12--  http://boxes.example.com/vagrant/boxes/c6/packer_c6_2.5.2_virtualbox.box
Resolving boxes.example.com... 10.1.0.17
Connecting to boxes.example.com|10.1.0.17|:80... connected.
HTTP request sent, awaiting response... 200 OK
Length: 830674320 (792M) [text/plain]
Saving to: 'packer_c6_2.5.2_virtualbox.box'

packer_c6_2.5.2_virtualbox.bo   0%[                                                         ]   7.12M   696KB/s   eta 19m 50s
```

(Rate: **696KB/s**) or often higher.

This particular example was pulled when on Wifi and connected to an IPSEC VPN.
",True,0
Vagrant box downloads extremely slow,sethvargo,57258770,2,73818901,0,"Hi @spkane 

Some boxes are hosted on Atlas and sometimes Atlas is just acting as a proxy to a user-hosted box. If you give more information on the specific box(es) you're downloading, we can do some research.
",False,0
Vagrant box downloads extremely slow,spkane,57258770,3,74010969,0,"@sethvargo This box is actually a box I built using packer and it is hosted on a remote server. I'm trying to understand why the download in significantly slower using vagrant then using wget to the exact same URL.
",False,0
Vagrant box downloads extremely slow,sethvargo,57258770,4,74088452,0,"@spkane sorry - I misread your original issue.

I would suspect (and maybe @mitchellh could elaborate more) a few things:
1. Ruby is slow and somehow throttling the subprocess
2. Wget is faster than curl (which is what Vagrant is using)
3. Vagrant is also allocating time to unpack the box
4. Wget is allowing for some type of compressed download

It would be helpful if you could benchmark this with curl for reference.
",False,0
Vagrant box downloads extremely slow,mitchellh,57258770,5,75818017,0,"I really can't explain this. Vagrant doesn't do anything during the subprocess Ruby-wise: it subprocesses to `curl`. It doesn't even do the download in Ruby. Perhaps wget is using multiple connections to download multiple parts? I really don't know, but unless we get more information I have to assume that Vagrant is fine here. 

Is `curl` just as slow? Vagrant is just subprocessing to curl until it completes.
",False,0
Vagrant box downloads extremely slow,keeprock,57258770,6,161091836,0,"I'm experience the same slow experience. Anyone can try aria - http://aria2.sourceforge.net/ and http://stackoverflow.com/questions/3430810/wget-download-with-multiple-simultaneous-connections

It's seems a little bit faster, but, man, you can set this up using default vagrant download mechanism and take a walk or make yourself a sandwich. Get way from screen for a little bit.
",False,0
Vagrant box downloads extremely slow,karlkfi,57258770,7,175213111,0,"Having the same problem here:
1. Upload a box manually to atlas
2. Create a new Vagrantfile with just `vm_cfg.vm.box_url = <user>/box-name`
3. `vagrant up` - box downloads slowly
4. wget box url from atlas (see `vagrant up` output) - box downloads lightening fast
",False,0
Vagrant box downloads extremely slow,clakeb,57258770,8,178110677,0,"I wish there was just a +1 for this. Me too. Same connection for all 3 attempts. VPN turned off.
- `vagrant up` took 25+ minutes.
- `wget` took 3 minutes.
- `curl` took 4 minutes. 
",False,0
Vagrant box downloads extremely slow,milhaus,57258770,9,178823565,0,"Ubuntu vivid64 is downloading at ~56kbps. I'm on a 100mbit symmetric connection.
edit: it timed out before it could finish.
edit2: I can confirm that https://atlas.hashicorp.com/ubuntu/boxes/vivid64/versions/20160128.0.0/providers/virtualbox.box downloads dramatically faster over wget than via ""vagrant up"".
",False,0
Vagrant box downloads extremely slow,ghost,57258770,10,180714186,0,"I'm trying to download the scotch/box and current download speeds using vagrant are less than 10kbps.

default: Progress: 0% (Rate: 2603/s, Estimated time remaining: 33:17:38)

However just as bad using wget.
",False,0
Vagrant box downloads extremely slow,tehmaspc,57258770,11,184283188,0,"ditto; some popular boxes are very slow to download - i'm updating ubuntu/trusty64 as we speak and it's dropping below 1Kb/s. Been seeing this for a couple wks now.
",False,0
Vagrant box downloads extremely slow,JasonTheAdams,57258770,12,193369924,0,"+1 -- exact same as last comment
",False,0
Vagrant box downloads extremely slow,winni2k,57258770,13,194075475,0,"Same here:

```
$ vagrant box add lazygray/heroku-cedar-14
==> box: Loading metadata for box 'lazygray/heroku-cedar-14'
    box: URL: https://atlas.hashicorp.com/lazygray/heroku-cedar-14
==> box: Adding box 'lazygray/heroku-cedar-14' (v1.0.6) for provider: virtualbox
    box: Downloading: https://atlas.hashicorp.com/lazygray/boxes/heroku-cedar-14/versions/1.0.6/providers/virtualbox.box
==> box: Box download is resuming from prior download progress
    box: Progress: 3% (Rate: 281k/s, ...
```
",False,0
Vagrant box downloads extremely slow,k1ng440,57258770,14,196060860,0,"same here

```
vagrant box update
==> default: Checking for updates to 'laravel/homestead'
    default: Latest installed version: 0.4.1
    default: Version constraints: >= 0
    default: Provider: vmware_desktop
==> default: Updating 'laravel/homestead' with provider 'vmware_desktop' from version
==> default: '0.4.1' to '0.4.2'...
==> default: Loading metadata for box 'https://atlas.hashicorp.com/laravel/homestead'
==> default: Adding box 'laravel/homestead' (v0.4.2) for provider: vmware_desktop
    default: Downloading: https://atlas.hashicorp.com/laravel/boxes/homestead/versions/0.4.2/providers/vmware_desktop.box
    default: Progress: 0% (Rate: 42210/s, Estimated time remaining: 6:10:54))
```
",False,0
Vagrant box downloads extremely slow,richard-scott,57258770,15,196242779,0,"Is there any way to use something like axel to stream downloads in quicker?
",False,0
Vagrant box downloads extremely slow,winni2k,57258770,16,197252297,0,"I guess there's nothing preventing people from sharing boxes via torrent.  For example, below is a magnet link for the heroku-cedar-14 box:

> magnet:?xt=urn:btih:5bb1480d5316f229bb71be55b56b06278de41a67&dn=heroku-cedar-14.box&tr=http%3A%2F%2F9.rarbg.com%3A2710%2Fannounce&tr=http%3A%2F%2Fannounce.torrentsmd.com%3A6969%2Fannounce&tr=http%3A%2F%2Fbt.careland.com.cn%3A6969%2Fannounce&tr=http%3A%2F%2Fexplodie.org%3A6969%2Fannounce&tr=http%3A%2F%2Fmgtracker.org%3A2710%2Fannounce&tr=http%3A%2F%2Ftracker.tfile.me%2Fannounce&tr=http%3A%2F%2Ftracker.torrenty.org%3A6969%2Fannounce&tr=http%3A%2F%2Ftracker.trackerfix.com%2Fannounce&tr=http%3A%2F%2Fwww.mvgroup.org%3A2710%2Fannounce&tr=udp%3A%2F%2F9.rarbg.com%3A2710%2Fannounce&tr=udp%3A%2F%2F9.rarbg.me%3A2710%2Fannounce&tr=udp%3A%2F%2F9.rarbg.to%3A2710%2Fannounce&tr=udp%3A%2F%2Fcoppersurfer.tk%3A6969%2Fannounce&tr=udp%3A%2F%2Fexodus.desync.com%3A6969%2Fannounce&tr=udp%3A%2F%2Fglotorrents.pw%3A6969%2Fannounce&tr=udp%3A%2F%2Fopen.demonii.com%3A1337%2Fannounce&tr=udp%3A%2F%2Ftracker.coppersurfer.tk%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.glotorrents.com%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.leechers-paradise.org%3A6969%2Fannounce&tr=udp%3A%2F%2Ftracker.openbittorrent.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker.opentrackr.org%3A1337%2Fannounce&tr=udp%3A%2F%2Ftracker.publicbt.com%3A80%2Fannounce&tr=udp%3A%2F%2Ftracker4.piratux.com%3A6969%2Fannounce

Anyone know a good website where one can search for torrents of vagrant boxes?
",False,0
Vagrant box downloads extremely slow,tehmaspc,57258770,17,197372009,0,"@wkretzsch - I personally don't know at the moment about any torrent sites - but for me I wouldn't want to trust torrent links as the source for my infrastructure testing. It's a possible option but security is also important. For me official vagrant boxes from folks like puppetlabs hosted on Atlas are so slow to download at times that I wish this issue could be resolved. For internal vagrant boxes that I build for my company we have the option to host on S3 or Artifactory or private Atlas org.

@mitchellh - yes - curl is just as slow (for me). I don't think it is a Vagrant issue - but a backed server hosting issue. Granted - not a Vagrant issue per se. 
",False,0
Vagrant box downloads extremely slow,faddat,57258770,18,197734440,0,"![screenshot from 2016-03-17 14-04-58](https://cloud.githubusercontent.com/assets/7142025/13838783/486408ba-ec49-11e5-9903-19cc1e031395.png)

Yes, this is because curl can only use one of my 3 connections at the same time.  No, that's not the connection's rated speed.  The rated speed is 45mbps.  Yes, bittorrent does perform better.  Just sayin-- your rationale for not supporting bittorrent is kinda thin here.  
",False,0
Vagrant box downloads extremely slow,winni2k,57258770,19,197847596,0,"@tehmaspc surely there must be a way for a website to publish the hash of their box along with a torrent link?  
",False,0
Vagrant box downloads extremely slow,darkn3rd,57258770,20,199516958,0,"I wish in general, there was a way to have incremental images, like docker images, with vagrant boxes.  For the provisioners, which bootstrap (cfengine, chef, salt, puppet, docker, etc) by downloading their platform, I wish there was a way to download a packaged up installer, so that other fresh images that use that provisioner, e.g. ubuntu + docker, would not need to download the goods again.  Box updates and provisioner downloads were already painful, but recently, have been beyond notoriously slow.
",False,0
Vagrant box downloads extremely slow,PorterBytes,57258770,21,207881297,0,"Just went to update my box for the first time (trusty64 - noticed the warning on my vagrant up command output), and it's going to take my 1.5 hours on a 150MBps connection - pathetic. It's 2016 - I don't know the specifics of what's going on here, but surely we can fix this, like, by the end of next week? The tech that goes into modern technologies like vagrant is amazing, something this basic should be overcome in mere hours.
",False,0
Vagrant box downloads extremely slow,faddat,57258770,22,207896576,0,"Amen, Matt, Amen.  This is about UX.

There should be a recognition that line speed != line speed and practical
steps can be taken to overcome the daunting issue of line speed != line
speed.

Jacob Gadikian
E-mail: faddat@gmail.com
SKYPE: faddat
Phone/SMS: +84 167 789 6421

On Sun, Apr 10, 2016 at 6:32 AM, Matt Porter notifications@github.com
wrote:

> Just went to update my box for the first time (noticed the warning on my
> vagrant up command output), and it's going to take my 1.5 hours on a
> 150MBps connection - pathetic. It's 2016 - I don't know the specifics of
> what's going on here, but surely we can fix this, like, by the end of next
> week? The tech that goes into modern technologies like vagrant is amazing,
> something this basic should be overcome in mere hours.
> 
> ‚Äî
> You are receiving this because you commented.
> Reply to this email directly or view it on GitHub
> https://github.com/mitchellh/vagrant/issues/5319#issuecomment-207881297
",False,0
Vagrant box downloads extremely slow,briancline,57258770,23,208706127,0,"I just tried asking Vagrant to download ubuntu/trusty64, and was getting speeds of <= 5 KiB/sec. I killed it and tried again using the exact same command, and got 29 MiB/sec. 

I think @mitchellh is correct in that this doesn't really seem like a Vagrant issue. If anything, it seems more like an Atlas issue (so possibly the ELB and/or whatever's sitting behind it). I highly doubt it has anything to do with the routes or hops between end-users and the ELB VIPs -- you wouldn't typically see such a polarizing set of speeds in that case, especially considering both VIPs terminate in us-east-1.

If for no other reason, it'd be highly desirable to see these made available through a CDN rather than a centrally-located ELB. Then again, I'm just one guy (who isn't paying for this service), so take that for what it's worth. Pretty thankful it's there either way.
",False,0
Vagrant box downloads extremely slow,karlkfi,57258770,24,209974166,0,"It's not just an Atlas issue. I have boxes and metadata.json on S3, with a Fastly CDN in front and regularly have the exact same issue: sometimes vagrant downloads at 100kbps and sometimes it downloads at > 5mbps. You can cancel a slow download and half the time a retry gets you the faster speeds. 
",False,0
Vagrant box downloads extremely slow,milhaus,57258770,25,210578465,0,"I contacted support about this around the same time I chimed in here initially. Their response is that Vagrant uses curl to download things so they don't see this as a Vagrant problem. IMO that's an unprofessional cop-out because they chose to use curl, know that there are problems and aren't considering swapping out with an alternative to eliminate the problem for their users.
",False,Impatience
Vagrant box downloads extremely slow,Haroenv,57258770,26,213288936,0,"I can confirm that this is still an issue. All my peers also report times of >1h, while the connection here for other connections is around 200MB/s.

```
vagrant up
Bringing machine 'default' up with 'virtualbox' provider...
==> default: Box 'ubuntu/trusty32' could not be found. Attempting to find and install...
    default: Box Provider: virtualbox
    default: Box Version: >= 0
==> default: Loading metadata for box 'ubuntu/trusty32'
    default: URL: https://atlas.hashicorp.com/ubuntu/trusty32
==> default: Adding box 'ubuntu/trusty32' (v20160406.0.0) for provider: virtualbox
    default: Downloading: https://atlas.hashicorp.com/ubuntu/boxes/trusty32/versions/20160406.0.0/providers/virtualbox.box
    default: Progress: 11% (Rate: 43801/s, Estimated time remaining: 1:36:50)
```
",False,0
Vagrant box downloads extremely slow,faddat,57258770,27,213619117,0,"While I am unsure of the origin of the problem, I really do wish that Hashicorp would get back to its unrelenting focus on user experience with this one.  **Muli-hour downloads (that should take 1-10 minutes)==bad ux.**
",False,Impatience
Vagrant box downloads extremely slow,dacodekid,57258770,28,214512394,0,"Currently downloading an image for the 5th time (@13Xk/s, even with `wget`). Keep disconnecting me while around 50-90%. But it ALWAYS downloads at full speed either early morning / late night EST.  Assuming it is a traffic  issue, but regardless very bad UX.

```
    box: Progress: 47% (Rate: 106k/s, Estimated time remaining: 0:14:50)
```
",False,0
Vagrant box downloads extremely slow,brazitech,57258770,29,215832266,0,"I have been trying for 2 day's now and still can not get it to download... its a shame.. it is really not impressing new comers to  laravel .. i can only get 34ks speed.........
",False,Impatience
Vagrant box downloads extremely slow,richard-scott,57258770,30,216503206,0,"Speeds ok from the UK:

```
Bringing machine 'default' up with 'virtualbox' provider...
==> default: Box 'bento/centos-7.2' could not be found. Attempting to find and install...
    default: Box Provider: virtualbox
    default: Box Version: >= 0
==> default: Loading metadata for box 'bento/centos-7.2'
    default: URL: https://atlas.hashicorp.com/bento/centos-7.2
==> default: Adding box 'bento/centos-7.2' (v2.2.6) for provider: virtualbox
    default: Downloading: https://atlas.hashicorp.com/bento/boxes/centos-7.2/versions/2.2.6/providers/virtualbox.box
    default: Progress: 11% (Rate: 7728k/s, Estimated time remaining: 0:01:19)
```

What is your location?
",False,0
Vagrant box downloads extremely slow,richard-scott,57258770,31,216505071,0,"Also, https://atlas.hashicorp.com/ URL's are delivered from Amazon Web Services (atlas-frontend-atlas-230110478.us-east-1.elb.amazonaws.com) so I doubt they are tight for bandwidth ;-)

Are the slow downloads being made from locations a long distance away from the AWS us-east-1 DC, perhaps thats the root cause of the issue?

Maybe the AWS CDN could be used to cache files around the world?
",False,0
Vagrant box downloads extremely slow,PorterBytes,57258770,32,216507036,0,"I'm located in Vermont, which is pretty us-east-1 last I checked :dart: 
",False,0
Vagrant box downloads extremely slow,brazitech,57258770,33,216520958,0,"I am in Brazil.... got it to download.... 10m connection here took 4.6 hours!!!!! my wife just gave birth to our 8th little girl.. It only took her 40 minutes !!!!! lol There is a big problem with there download!!!!!
",False,0
Vagrant box downloads extremely slow,thokari,57258770,34,217642878,0,"Same here, I have a 50Mbps connection...

`
    default: Progress: 44% (Rate: 102k/s, Estimated time remaining: 0:15:01))
`
",False,0
Vagrant box downloads extremely slow,macnibblet,57258770,35,217673852,0,"Sign me upp here, 100mb symmetric connection (Fiber) sloooow as shit, doing 150kb/s
",False,Vulgarity
Vagrant box downloads extremely slow,brazitech,57258770,36,217725427,0,"after looking at the years of complaints of slow download with no effort of resolving the issue,,, i think its time to start emailing Laravel to stop endorsing homestead until the issue is resolved..... maybe that will get their attention!!!! this is a real problem... 15 retries and then 4.6 hours to download a file is irresponsible on their part........
",False,Impatience
Vagrant box downloads extremely slow,richard-scott,57258770,37,218598736,0,"I bet it gets closed, but if you don't ask you don't get:

https://github.com/mitchellh/vagrant/issues/7307
",False,0
Vagrant box downloads extremely slow,bradisbell,57258770,38,225645550,0,"Just snagged a box at 85mbit.  You all fix something recently?  Much better than it used to be.
",False,0
Vagrant box downloads extremely slow,voycey,57258770,39,230386144,0,"This is painful to do anything on any more - On 100mbps synchronous connection and getting 168kb, either overloaded servers or throttling
",False,Entitlement
Vagrant box downloads extremely slow,mccorkle,57258770,40,233045389,0,"In looking to debug curl being slow -- I found a [stackoverflow post  ](http://stackoverflow.com/questions/30984641/debugging-slow-download-with-curl) that suggests that --trace-ascii /dev/null makes your curl go at the speed you'd expect.  For me, I'm trying to download [CentosOS 7](http://cloud.centos.org/centos/7/vagrant/x86_64/images/CentOS-7-x86_64-Vagrant-1606_01.VirtualBox.box) and here are my results:

NO --trace-ascii option:

```
$ curl http://cloud.centos.org/centos/nt/x86_64/images/CentOS-7-x86_64-Vagrant-1606_01.VirtualBox.box -o CentOS-7-x86_64-Vagrant-1606_01.VirtualBox.box
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0  483M    0  489k    0     0  77724      0  1:48:44  0:00:06  1:48:38 69721
```

With trace-ascii the first time:

```
$ curl http://cloud.centos.org/centos/7/vagrant/x86_64/images/CentOS-7-x86_64-Vagrant-1606_01.VirtualBox.box -o CentOS-7-x86_64-Vagrant-1606_01.VirtualBox.box --trace-ascii /dev/null
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  5  483M    5 24.5M    0     0  4259k      0  0:01:56  0:00:05  0:01:51 4599k
```

Does anyone else see the same behavior?
",False,0
Vagrant box downloads extremely slow,codehaiku,57258770,41,240311022,0,"The download is extremely slow on my end too. I'm trying vagrant for the very first time. Might ditch this software and go back to my native apache2 instead.

<img width=""913"" alt=""screen shot 2016-08-17 at 12 31 40 pm"" src=""https://cloud.githubusercontent.com/assets/4960876/17724673/926a8510-6476-11e6-82ab-fb7276c448c4.png"">
",False,0
Vagrant box downloads extremely slow,daryn-k,57258770,42,268513565,0,Help. I have same problem. I can't wait 3 hours! Very slow! Stupid!,False,Impatience
Vagrant box downloads extremely slow,brazitech,57258770,43,268532571,0,i gave up a year ago..download to slow.. problems after down load.. have to download for 3 hours again.... Vagrant will not fix the problem that has been there for several years now.. you would think that after 3 or 4 years of this problem they would address the issue.....,False,Bitter frustration
Vagrant box downloads extremely slow,daryn-k,57258770,44,268540307,0,8 hours to download! I hate you all!,False,Irony
Vagrant box downloads extremely slow,winni2k,57258770,45,268653949,0,"Lol, I hate you too @daryn-k :)",False,Mocking
Vagrant box downloads extremely slow,ameyaagashe,57258770,46,269612087,0,"Guys why is this issue closed? This is still an outstanding issue and needs to be addressed ASAP. I am experiencing the same issue.
",False,Impatience
Vagrant box downloads extremely slow,grassjedi,57258770,47,272380689,0,Wow! Downloading boxes is painful please fix this. PLEASE?,False,Impatience
Vagrant box downloads extremely slow,JasonTheAdams,57258770,48,272505864,0,"I think it really has to do with time of day, traffic, alignment of the planets, etc. I haven't had slow speeds in a while. It seems very hit or miss. In fact, if it is slow you and start and stop it with the chance of getting a better connection. I'm not sure this is really the fault of the vagrant framework as much as it is the nature of large bottlenecked downloads.",False,0
Vagrant box downloads extremely slow,ryanaslett,57258770,49,273679937,0,"@mitchellh These errant speed symptoms from hashicorp's servers could be indicative of bumping up against AWS's IOPS credits for GP2 filesystems.  https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html#IOcredit

We had some testing infrastructure on drupal.org that would run fine for a long time, then suddenly drop to a crawl because we had ""spent"" all of our IO credits.  It could be possible that hashicorps' servers are bumping up against the same limit. 

`sar -b` could give some insight as to whether or not this explains the random performance drops.
",False,0
Vagrant box downloads extremely slow,richard-scott,57258770,50,273765494,0,It could be an idea for Hashicorp to move the images for download into S3 and use that for downloads... that would save on running instances specifically for downloads.,False,0
Vagrant box downloads extremely slow,babadofar,57258770,51,279201272,0,"Downloading boxes used to be quick, now it's so slow it makes vagrant a no-go for quick and simple developer environments.  ",False,Impatience
Vagrant box downloads extremely slow,DeadlySystem,57258770,52,280154083,0,"Trying to download ubuntu/xenial64. Download speed maxes out at 150 KB/s on a 1 Gbps symmetrical fiber connection. WTF. Remaining time 1 hour? I could probably download the ISO, read the guide on how to set up my own box, and finish earlier.

EDIT: Interestingly, speed went up by factor 10 when I tried to download the same box in the browser simultaneously.",False,Vulgarity
Vagrant box downloads extremely slow,matti,57258770,53,287034242,0,">use latest devops tools to speed things up
>spend days watching max 420k/s download speeds",False,0
Vagrant box downloads extremely slow,minhphuc429,57258770,54,288285617,0,"Same here, I have a 30Mbps connection

> default: Adding box 'ubuntu/trusty64' (v20170313.0.5) for provider: virtualbox
default: Downloading: https://atlas.hashicorp.com/ubuntu/boxes/trusty64/versions/20170313.0.5/providers/virtualbox.box
default: Box download is resuming from prior download progress
default: Progress: 0% (Rate: 80568/s, Estimated time remaining: 1:26:22)",False,0
Vagrant box downloads extremely slow,tersmitten,57258770,55,289866508,0,"@DeadlySystem We have the same experience, when I download the same box (url) using `curl` from the commandline (during the `vagrant up`)",False,0
Vagrant box downloads extremely slow,dqlopez,57258770,56,290985383,0,"Any update on this, fetching box from Hashicorp is painfully slow.

![screenshot_2017-04-02_21-08-15](https://cloud.githubusercontent.com/assets/1684989/24587412/8f0329d8-17e8-11e7-9743-f793458b7daf.png)
",False,0
Vagrant box downloads extremely slow,faddat,57258770,57,291019093,0,"Hey, quick thought:

If this uses curl (not libcurl) through some sort of ruby-controlled, bash-mediated process, why not just remove curl for one of:

* ipfs
* aria2

Both would do the job better than curl.  ",False,0
Vagrant box downloads extremely slow,voycey,57258770,58,291026688,1,"It honestly looks like they dont give a shit, rules this out as an option
for me!

On 3 Apr 2017 8:15 AM, ""Jacob Gadikian"" <notifications@github.com> wrote:

> Hey, quick thought:
>
> If this uses curl (not libcurl) through some sort of ruby-controlled,
> bash-mediated process, why not just remove curl for one of:
>
>    - ipfs
>    - aria2
>
> Both would do the job better than curl.
>
> ‚Äî
> You are receiving this because you commented.
> Reply to this email directly, view it on GitHub
> <https://github.com/mitchellh/vagrant/issues/5319#issuecomment-291019093>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/ABBAihR9ng4t2Jq1XTmAjMyMCnlEFtxRks5rsB4WgaJpZM4Deq5d>
> .
>
",False,Vulgarity
File extension filter,TristisOris,93644527,1,93644527,0,"would be great to make filter for auto skip marked extention types from download.

ex: i need only music without covers, etc. it annoying to deselect it all time (imagine 50 albums, 4Gb covers and 1 seed). so here can help filter to .jpg, .png, etc.
if need to download photos - turn off filter checkbox in file list.
you can do sets of rules, but one is enough.",True,0
File extension filter,thalieht,93644527,2,119556239,0,"Sounds useful :thumbsup:
",False,0
File extension filter,Symbai,93644527,3,304447200,0,"2015? :-( This will very likely never added then, that sucks.",False,Bitter frustration
File extension filter,shula,93644527,4,427604310,0,"This feature could help security:

If a user downloads only known files (e.g. audio/video), it is wise to block pontentially dangerous extensions, e.g. EXE, COM, BAT, LNK, VBS, (PY?), etc.

Less savvy users can't easily spot a ponetial threat, e.g. Matrix.avi   <> Matrix.avi.exe

Also, some video torrents are bundeled with trojans, e.g. ""codec.zip"" or ""driver.exe"" containing malware. Today I spotted the attached file, an LNK file, an extension that is hidden on Windows.

None of my users download software via torrent, so I'd like to block it for them, or set blocked extensions; potentially, i'd like to block the whole torrent altogether, if a potential software extension is found in one of its files.

(My personal block list would be: EXE, COM, BAT, VBS, VBE, JS, CMD, PY, CPL, DLL, LNK, SCR)

In the screenshots below: 
Windows hides LNK extension, a trojan disguised as AVI video:

![lnk-virus](https://user-images.githubusercontent.com/124651/46575528-43e7c780-c9bf-11e8-9b2a-0cbd5efa8d36.png)
![virus2](https://user-images.githubusercontent.com/124651/46575529-43e7c780-c9bf-11e8-8399-db13d6ec90ef.png)
",False,0
File extension filter,Smithwright,93644527,5,480271998,0,"Id like to throw my support behind this feature request. Many torrents come with useless .txt files that do nothing but clutter up a directory.  Also, a popular site I use has started to include an .exe file that I now have to deselect every time I download something. It would be great to be able to have these files automatically excluded.  Being able to blacklist certain file names would also be a great addition to this feature.",False,0
File extension filter,gohamstergo,93644527,6,489335579,0,Throwing my hat in..,False,0
File extension filter,phuein,93644527,7,491777593,0,"I suggest a feature for a simple list of file names ```do_not_download.exe``` and extensions ```*.exe``` that get marked as **Priority** -> **Do Not Download** automatically for all torrents. List may be accessed in **Options** -> **Downloads**.

Searching for references:
https://github.com/qbittorrent/qBittorrent/blob/master/src/base/bittorrent/torrenthandle.cpp#L684
https://github.com/qbittorrent/qBittorrent/blob/master/src/base/bittorrent/torrenthandle.cpp#L2068
https://www.libtorrent.org/reference-Core.html find ```file_priorities``` under ```add_torrent_params``` header.
https://github.com/qbittorrent/qBittorrent/blob/2d7b833ae6cb2145465cc7e47df398628ac95651/src/base/bittorrent/session.cpp#L1949",False,0
File extension filter,IIIdefconIII,93644527,8,495734088,0,"Glad i found this post, really like this feature to, are we sure (i couldn't find it) that there is no such option already?

",False,0
File extension filter,RaveWolf1,93644527,9,507021284,0,Definitely have my vote. It will also save (not much but) some space and unnecessary Data download for Countries that charge per Mb on top of per Speed.,False,0
File extension filter,Garbonzo17,93644527,10,507086205,0,"Love it. You would need to be able to override it on a per torrent bassis,
but for people who download primarily just a couple of different file types
(cough).

On Sat, Oct 6, 2018, 4:35 PM shula <notifications@github.com> wrote:

> This feature could help security:
>
> If a user downloads only known files (e.g. audio/video), it is wise to
> block pontentially dangerous extensions, e.g. EXE, COM, BAT, LNK, VBS,
> (PY?), etc.
>
> Less savvy users can't easily spot a ponetial threat, e.g. Matrix.avi <>
> Matrix.avi.exe
>
> Some video torrents are bundeled with fake ""codec.zip"" ""driver.exe""
> containing malware. Today I spotted the attached file, an LNK file, an
> extension that is hidden on Windows.
>
> None of my users are downloading software via torrent, so I'd like to
> block it for them, or set blocked extensions; potentially, i'd like to
> block the whole torrent altogether, if a potential software is found in it.
>
> My personal block list would be: EXE, COM, BAT, VBS, VBE, JS, CMD, PY,
> CPL, DLL, LNK, SCR.
>
> In the screenshots below:
> How windows hides LNK extension, which is a sure malware when only
> downloading media:
>
> [image: lnk-virus]
> <https://user-images.githubusercontent.com/124651/46575528-43e7c780-c9bf-11e8-9b2a-0cbd5efa8d36.png>
> [image: virus2]
> <https://user-images.githubusercontent.com/124651/46575529-43e7c780-c9bf-11e8-8399-db13d6ec90ef.png>
>
> ‚Äî
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/qbittorrent/qBittorrent/issues/3369#issuecomment-427604310>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AGbY-5e2u-o8-6NhlR-Z9UlUBj9q6YGgks5uiRPzgaJpZM4FT95h>
> .
>
",False,0
File extension filter,skylar01,93644527,11,518449345,0,"Please add this feature since lot's of trackers now put lots of ""junk files"". Yes we can use other clients but qb has a lot to offer and you can always disable this feature if it bothers you, so it's a win-win.",False,0
File extension filter,amccarter,93644527,12,520924450,0,"I'd love this, too. ",False,0
File extension filter,ElusiveZatchmo,93644527,13,527210363,0,This please! We need this!,False,0
File extension filter,Lonli-Lokli,93644527,14,547628421,0,Is it possible now to completely fail specific torrent if it contains not appropriate file name?,False,0
File extension filter,ctrl-escp,93644527,15,557791768,0,"+1 on the feature request.
In the meantime, I wrote a short cmd script which can be referenced in the _Tools->Options->Downloads->Run external program on torrent completion_ which renames files with a suspicious extension (.exe \ .scr \ .cmd \ .bat) to prevent them from running when double clicked.

```
rem Find suspicious files in directory and rename them
rem Usage: fsus.cmd <dirname>

@echo off
SETLOCAL
set extensions=""\.lnk \.exe \.cmd \.scr \.bat""
echo looking in %1 for %extensions%
for /f %%F in ('dir %1 /s /b') do (
    (echo %%F | findstr /r %extensions% > NUL) && move %%F bad_%%F.BAD && echo Renamed %%F
)
ENDLOCAL
```",False,0
File extension filter,oltodosel,93644527,16,559159403,0,"Plugin in python3 for this:
https://gist.github.com/oltodosel/566e051191f3a58b905db2cc6980656f",False,0
File extension filter,Unix-Kernel,93644527,17,596234216,0,Has there been any updates if/when this feature would be added?,False,0
File extension filter,Zerauskire,93644527,18,610647608,0,Since 2015 and this still isn‚Äôt added yet? Come on. This would be such a useful feature.,False,Bitter frustration
File extension filter,unimpededglare,93644527,19,622215134,0,"I would love to have this feature implemented. There could be a global file extension filter, or have the filter setup by category (so categories can have different filters).",False,0
File extension filter,ArtBond,93644527,20,622429807,0,2015-2020 is not create this function. qBit - Shit!,False,Vulgarity
File extension filter,simo1994,93644527,21,652546406,0,"Why isn't this issue considered critical?

Distracted users shouldn't run viruses so easily.",False,0
File extension filter,FranciscoPombal,93644527,22,652558395,0,"> Why isn't this issue considered critical?

To be fair, the ""better"" motivation for this feature request should be some kind of automation purpose like ""typically I download a lot of ebook pack torrents with epub + azw3, but I don't want any azw3"", and not ""distracted users clicking files with hidden extensions"" - that is a Windows problem, easily fixed by disabling `Hide extensions for known file types` in the control panel. If you still click ""dangerous"" files accidentally, that's PEBCAK. Alternatively, just use better sites and download better torrents.

This is why this isn't ""critical"".",False,0
File extension filter,unimpededglare,93644527,23,652560009,0,"> To be fair, the ""better"" motivation for this feature request should be some kind of automation purpose like ""typically I download a lot of ebook pack torrents with epub + azw3, but I don't want any azw3""

That's definitely my motivation for wanting this feature. I use the RSS downloader and it would be nice to automatically exclude unnecessary files.",False,0
File extension filter,simo1994,93644527,24,652573874,0,"> > Why isn't this issue considered critical?
> 
> that is a Windows problem, easily fixed by disabling `Hide extensions for known file types` in the control panel. If you still click ""dangerous"" files accidentally, that's PEBCAK.

As shown above, this isn't enough for .lnk files. Windows hides the .lnk extension. If you usually use smaller thumbnails, the minuscule difference in the icon is barely visible.

Also, it is arguably because torrent clients aren't smart that these types of malicious torrents are still going around, and that's why I think qBittorrent should provide this feature.

IMHO certain extensions should also be skipped by default, for the same reason. It would be a great security and usability improvement.

This also allows you to skip certain extensions because you prefer so, but the security issue should be considered prominently.",False,0
File extension filter,FranciscoPombal,93644527,25,652578286,0,"> As shown above, this isn't enough for .lnk files. Windows hides the .lnk extension. If you usually use smaller thumbnails, the minuscule difference in the icon is barely visible.

If you are downloading torrents with malicious .lnk files, you need a solution for a more urgent problem: don't download such torrents, use better sites/sources, or pay more attention. After all, it is the user's responsibility to not fall for phishing emails as well. Inspect URLs/files you click.

Alternatively, you could try to enable showing `.lnk` extensions: https://www.tenforums.com/customization/111886-how-show-lnk-extension.html
But because Windows is Windows, this might lead to undesirable presentation elsewhere (such as the start menu).",False,0
File extension filter,simo1994,93644527,26,652582537,0,"Your solution is for power users, has usability drawbacks, and doesn't address the fact that malicious users are taking advantage of an easily fixable flaw in qBittorrent.

I think the qBittorrent team should step up and fix this. There is almost never a good reason to download certain file extensions, and users should actively check those files for download.",False,0
File extension filter,PatrickVeenstra,93644527,27,652589829,0,"You can't expect people that execute random files to know how to use power features. That something can (and should) be done another way isn't a reason to not include a security feature.
These are the same people that download from the first torrent site that shows up in Google search. So the ""use a better site"" isn't a valid argument either.

Obviously all this is only true if your target is the mass and not just tech-savvy people.",False,0
File extension filter,FranciscoPombal,93644527,28,652589953,1,"> Your solution is for power users, has usability drawbacks, 

Fighting phishing emails is something everyone has to learn to do, no matter the occupation. I think it is reasonable to demand a certain level of proficiency and common sense.

> and doesn't address the fact that malicious users are taking advantage of an easily fixable flaw in qBittorrent.

""Malicious users are taking advantage of distracted/careless users"" would be a more accurate statement. Do you think the possibility of receiving phishing emails is a flaw of E-mail? If so, is the possibility of hearing the voice of a scammer in real life, believing what they say, and giving them money, a flaw of your ears? Should your auditory system should autoblock certain words/sentences on its own? Perhaps it should be the brain acting on the information instead.

Furthermore, the greater issue of downloading these kinds of torrents should not be underestimated. You have to go out of your way, even when searching for illegal content, to find these kinds of torrents. And no, the `.exe` in RARBG torrents does not count as an example of this practice in a popular site; it is actually just a harmless text file with the `.exe` extension designed to prevent mirroring  by software that, ironically, relies on ""file extensions"" to make assumptions about their content.

Not to mention that if anyone actually accidentally clicks a dangerous exe, it should be caught by UAC anyway. If the user has disabled UAC or blindly clicks through it, then they either know what their doing or they ""know enough to be dangerous"", in which case whatever happens is their own fault and there's nothing we can really do.",False,Mocking
Decimal: Check attribute has value (with attrname?),Sega100500,119143420,1,119143420,0,"I use RoR 4.2.5

in migration:

```
t.decimal :price, precision: 12, scale: 2
```

Convenient to use `@value.price?` to check for the value of any. Check for is not nil and not zero.

```
@value.price = nil
@value.price? #=> false

@value.price = 0
@value.price? #=> false
```

All Ok!

But!

```
@value.price = 0.01
@value.price? #=> false

@value.price = 0.9
@value.price? #=> false

@value.price = -0.9
@value.price? #=> false

@value.price = 1.0
@value.price? #=> true

@value.price = -1.0
@value.price? #=> true
```

WHY?
",True,0
Decimal: Check attribute has value (with attrname?),anujaware,119143420,2,160124198,0,"@Sega100500 Can you print what value it assigns to price when you assign ""0.9""? i.e. 

``` ruby
  @value.price = 0.9
  p @value.price
```

and paste the output here.
I tried above code and its working. I have doubt if it assigns 0 (floors the number) when you try to assign 0.9 (decimal number).
",False,0
Decimal: Check attribute has value (with attrname?),Sega100500,119143420,3,160135264,0,"@anujaware 

```
0.9
```

What are you expecting something else?
",False,0
Decimal: Check attribute has value (with attrname?),anujaware,119143420,4,160135874,0,"Then it should return true. I was thinking if its assigning 0 in your case. I am not able to reproduce this error.
",False,0
Decimal: Check attribute has value (with attrname?),Sega100500,119143420,5,160135879,0,"@anujaware 

Web console:

```
>> p @product.price
=> 0.9
>> p @product.price?
=> false
```
",False,0
Decimal: Check attribute has value (with attrname?),Sega100500,119143420,6,160136170,0,"@anujaware 

Exactly!

""Then it should return true""

But it return false
",False,0
Decimal: Check attribute has value (with attrname?),anujaware,119143420,7,160136383,0,"@Sega100500 You can debug this in lib/active_record/attribute_methods/query.rb#query_attribute(attr_name) in activerecord gem.
",False,0
Decimal: Check attribute has value (with attrname?),Sega100500,119143420,8,160136962,0,"@anujaware 
Web console:

```
>> @product.price = 0
=> 0
>> p @product.price?
=> false
>> @product.price = 1.0
=> 1.0
>> p @product.price?
=> true
>> @product.price = 0.9
=> 0.9
>> p @product.price?
=> false
```
",False,0
Decimal: Check attribute has value (with attrname?),Sega100500,119143420,9,160137489,0,"@anujaware 
ruby -v
ruby 2.2.3p173 (2015-08-18 revision 51636) [x86_64-linux]

RoR 4.2.5

May be it help ?
",False,0
Decimal: Check attribute has value (with attrname?),Sega100500,119143420,10,160140132,0,"@anujaware 

BUG!!! in `query_attribute(attr_name)`

```
          if column.nil?
            if Numeric === value || value !~ /[^0-9]/
              !value.to_i.zero?
            else
              return false if ActiveRecord::ConnectionAdapters::Column::FALSE_VALUES.include?(value)
              !value.blank?
            end
          elsif column.number?
            !value.zero?
          else
            !value.blank?
          end
```

Steps:

```
>> @product.price = 0.9
=> 0.9
>> @product.price.zero?
=> false
>> @product.class.columns_hash['price']
=> nil
>> @product.class.columns_hash[:price]
=> nil
>> p @product[:price]
=> 0.9
>> 0.9 === Numeric
=> false
>> Numeric === 0.9 || 0.9 !~ /[^0-9]/
=> true
>> 0.9.to_i
=> 0
>> 0.9.to_i.zero?
=> true
```
",False,0
Decimal: Check attribute has value (with attrname?),sivagollapalli,119143420,11,160143689,0,"I think this line `Numeric === 0.9 || 0.9 !~ /[^0-9]/` creating the issue. As `===` checks the case equality so when we compare `Numeric === 0.9` it returns `true` as `Numeric` is an ancestor for `Float` numbers. I think the fix would be something like this:

```
if column.nil?
    if Numeric === value || value !~ /[^0-9]/
         return !value.to_f.zero? if Float === value
         !value.to_i.zero?
      else
          return false if ActiveRecord::ConnectionAdapters::Column::FALSE_VALUES.include?(value)
           !value.blank?
       end
    elsif column.number?
         !value.zero?
    else
         !value.blank?
     end
```
",False,0
Decimal: Check attribute has value (with attrname?),Sega100500,119143420,12,160144062,0,"@sivagollapalli 
Not the most elegant solution. But even so.

PLEASE FIX it
",False,Impatience
Decimal: Check attribute has value (with attrname?),Sega100500,119143420,13,160144867,0,"@sivagollapalli 
""As === checks the case equality so when we compare Numeric === 0.9 it returns true as Numeric is an ancestor for Float numbers.""

FALSE!

```
>> 0.9 === Numeric
=> false
>> 0.9 !~ /[^0-9]/
=> true

```
",False,0
Decimal: Check attribute has value (with attrname?),anujaware,119143420,14,160163371,0,"Still I am not able to reproduce this issue with ruby 2.2.3p173 (2015-08-18 revision 51636) [x86_64-linux] and rails 4.2.5. Before having solution. I am not getting why you are not getting that column in column_hash. And why we have Numericality check. 
Which database are you using?
",False,0
Decimal: Check attribute has value (with attrname?),adityashedge,119143420,15,160166632,0,"@Sega100500 I am not able to reproduce this issue.
I have tried both Ruby 2.2.2 and 2.2.3. Also I double checked with Sqlite3 and Postgresql on both Ruby versions.

```
$ ruby -v
ruby 2.2.3p173 (2015-08-18 revision 51636) [x86_64-darwin14]

$ rails -v
Rails 4.2.5
```

``` ruby
2.2.3 :003 > pr
 => #<Product id: nil, price: nil, created_at: nil, updated_at: nil>
2.2.3 :004 > pr.price
 => nil
2.2.3 :005 > pr.price?
 => false
2.2.3 :006 > pr.price = 0
 => 0
2.2.3 :007 > pr.price?
 => false
2.2.3 :008 > pr.price = 0.01
 => 0.01
2.2.3 :009 > pr.price?
 => true
2.2.3 :010 > pr.price = 0.09
 => 0.09
2.2.3 :011 > pr.price?
 => true
2.2.3 :012 > pr.price = -0.09
 => -0.09
2.2.3 :013 > pr.price?
 => true
```

Can you create a new Github repo to reproduce this error?
",False,0
Decimal: Check attribute has value (with attrname?),Sega100500,119143420,16,160168806,0,"@anujaware 
""Which database are you using?""
MySQL
",False,0
Decimal: Check attribute has value (with attrname?),Sega100500,119143420,17,160171122,0,"@anujaware 
""I am not getting why you are not getting that column in column_hash""

Because this SQL:

```
SELECT products.*, contents.*, products.id AS product_id FROM `contents` INNER JOIN products ON products.content_id = contents.id WHERE (contents.visible != 0) AND `contents`.`visible` = 1 AND `contents`.`furl` = 'apple-iphone-5s-16gb' AND `products`.`id` = 191 AND `products`.`category_id` = 312 GROUP BY contents.id
```

to get with related content-page

```
>> @product.class.columns_hash.keys
=> [""id"", ""parent_id"", ""level"", ""position"", ""node_type"", ""user_id"", ""group_id"", ""restrictive_group_id"", ""visible"", ""system"", ""list"", ""no_children"", ""no_images"", ""no_files"", ""key_lock"", ""key"", ""url"", ""furl"", ""alias"", ""name"", ""brief"", ""text"", ""title"", ""seo"", ""keywords"", ""description"", ""special"", ""content_parameter"", ""content_date"", ""created_at"", ""updated_at""]
```

```
>> @product.price = 0.9
=> 0.9
>> @product[:price]
=> 0.9
```

It is VALID COE! But you have not considered this possibility.
columns_hash NOT contain joined columns but access to they - it possible!
",False,0
Decimal: Check attribute has value (with attrname?),Sega100500,119143420,18,160171625,0,"@adityashedge 
Too simple example for ALL possible usages.
",False,0
Decimal: Check attribute has value (with attrname?),Sega100500,119143420,19,160172289,0,"@anujaware 
Anyway
the code

```
            if Numeric === value || value !~ /[^0-9]/
              !value.to_i.zero?
            else
```

It is BUG! Not correct to check Float numbers.
",False,0
Decimal: Check attribute has value (with attrname?),sivagollapalli,119143420,20,160175341,0,"@Sega100500 

Looks like there is no issue. Just now I have checked with `mysql` and `sqlite3` with `4.2.5` and `ruby-2.2.3`.  All my tests are passing. If you still face issue please update below template and submit so that we can check.

Please ignore my previous comments. 

```
begin
  require 'bundler/inline'
rescue LoadError => e
  $stderr.puts 'Bundler version 1.10 or later is required. Please update your Bundler'
  raise e
end

gemfile(true) do
  source 'https://rubygems.org'
  gem 'rails', '4.2.5' 
  #gem 'sqlite3'
  gem 'mysql'
end

require 'active_record'
require 'minitest/autorun'
require 'logger'

ActiveRecord::Base.establish_connection(adapter: 'mysql', database: 'bug_test')
ActiveRecord::Base.logger = Logger.new(STDOUT)

ActiveRecord::Schema.define do
  create_table :products do |t| 
    t.decimal :price, precision: 12, scale: 2
  end 
end

class Product < ActiveRecord::Base
end

class BugTest < Minitest::Test
  def test_price
    p = Product.new
    p.price = 0.9
    assert_equal true, p.price?

    p.price = 0.01
    assert_equal true, p.price?

    p.price = -0.9
    assert_equal true, p.price?

    p.price = 1.0
    assert_equal true, p.price?

    p.price = -1.0
    assert_equal true, p.price?
  end
end

```
",False,0
Decimal: Check attribute has value (with attrname?),Sega100500,119143420,21,160176466,0,"@sivagollapalli 
See above
1. joined tables
2. bug code for check Float is:

```
            if Numeric === value || value !~ /[^0-9]/
              !value.to_i.zero?
            else
```

Also any questions?
",False,0
Decimal: Check attribute has value (with attrname?),Sega100500,119143420,22,160176995,1,"@sivagollapalli 
Are you crazy?

```
  create_table :products do |t| 
    t.string :price
  end 
```

At BEGIN

```
t.decimal :price, precision: 12, scale: 2
```
",False,Insulting
DDC-2891: Impossible to pass a limit to a subquery,doctrinebot,120669472,1,120669472,0,"Jira issue originally created by user johnconnor:

It seems that passing the limit to a subquery is not working

```
$subquery = $em->createQueryBuilder()->from('...')->where('...')->setMaxResults(5);
$query = $em->createQueryBuilder()->from('...')->where(
   $qb->expr()->in('p.id', $subquery->getDQL())
);
$query->getQuery()->getResult();
```

The query works but the is no specified limit in the resulting SQL.
I am aware that DQL does not support the limits and offsets, so i guess there should be another way to get this working?
",True,0
DDC-2891: Impossible to pass a limit to a subquery,doctrinebot,120669472,2,162364613,0,"Comment created by @deeky666:

Can you please tell which database platform you are using? Because limiting results heavily depends on the platform used.
",False,0
DDC-2891: Impossible to pass a limit to a subquery,doctrinebot,120669472,3,162364614,0,"Comment created by johnconnor:

MySql
",False,0
DDC-2891: Impossible to pass a limit to a subquery,doctrinebot,120669472,4,162364615,0,"Comment created by @deeky666:

Hmmm I am not quite sure if the limit/offset is invoked for subqueries but I don't see why it shouldn't. Also I think this is not a DBAL issue because the limit/offset support for MySQL is the easiest we have on all platform. See: https://github.com/doctrine/dbal/blob/master/lib/Doctrine/DBAL/Platforms/MySqlPlatform.php#L51-L63
The query doesn't have to be modified but instead only the limit clause is appended to the query. Can you maybe provide the generated SQL for that query?
",False,0
DDC-2891: Impossible to pass a limit to a subquery,doctrinebot,120669472,5,162364618,0,"Comment created by johnconnor:

I think if you try to build any query with QueryBuilder, set a limit to it with setMaxResults then call getDQL method, you should see that the output contains no info about limit.
So if you look at my code example , at $qb->expr()->in('p.id', $subquery->getDQL()), then you will see that the getDQL passes to the IN expression a query which already DOES NOT have limit. So this is the place where any info about limits and offsets gets lost.

So I fail to see what it has to do with any specific db engine,however I can provide the mysql resulting query if you want,though it looked perfectly normal to me,just lacks the LIMIT part.
",False,0
DDC-2891: Impossible to pass a limit to a subquery,cesardmoro,120669472,6,289810384,0,You any news on this ? please maybe a hack ?,False,0
DDC-2891: Impossible to pass a limit to a subquery,vladimmi,120669472,7,397550790,0,Just to note - it's still impossible to use subqueries with limits.,False,0
DDC-2891: Impossible to pass a limit to a subquery,adrienpayen,120669472,8,903259409,0,"New note 3 years later, it is still impossible to use subqueries with limits. Any workaround ?",False,0
DDC-2891: Impossible to pass a limit to a subquery,greg0ire,120669472,9,903270466,0,"@adrienpayen @vladimmi if I had a contribution graph like yours, I would refrain from complaining about a lack of contributions. You've done nothing to fix an issue that directly impacts you, so why would you expect people that are not impacted to do the work for you? Please stop the spamming and start being constructive.

You could provide a failing test case, or use a debugger to pinpoint where the issue is.",False,Bitter frustration
DDC-2891: Impossible to pass a limit to a subquery,vladimmi,120669472,10,903549117,1,"@greg0ire If I was such a tender snowflake, I would refrain from using public services having comments - especially bug trackers. And especially I would at least read ticket before posting anything there - because ""failing case"" was provided 7 years ago in the very first message and ""pinpointed where the issue is"" in the same day several messages later.",False,Mocking
DDC-3730: Embeddable hydrates to object instead of null,doctrinebot,120671163,1,120671163,0,"Jira issue originally created by user pcnc:

Good afternoon,

When hydrating an Embeddable with nullable attributes the result is an instance of the Embeddable class , this is obviously correct and expected behavior. 

If all the attributes are null the hydrator will still return an instance of the class with all of its properties null , even if I persist and flush my Entity with the Embeddable being set as null . 

For clarification :

```

class MyEntity
{
    protected $myEmbeddable;

    public function setMyEmbeddable(MyEmbeddable $myEmbeddable = null)
    {
        $this->myEmbeddable = $myEmbeddable;
    }
    [...]
}

$newEntity = new MyEntity();
$newEntity->setMyEmbeddable(null);

$em->persist($newEntity);
$em->flsuh($newEntity);

```

Calling $newEntity->getMyEmbeddable() will return an instance of the MyEmbeddable object with all of it's attributes set to null .

I expected $newEntity->getMyEmbeddable() to be NULL . 

Can someone clarify is this is expected behaviour ? In case it is , how can I achieve what I'm looking for ? 

Best regards
",True,0
DDC-3730: Embeddable hydrates to object instead of null,doctrinebot,120671163,2,162368530,0,"Comment created by eugene-d:

See https://github.com/doctrine/doctrine2/pull/1275
",False,0
DDC-3730: Embeddable hydrates to object instead of null,boekkooi,120671163,3,170110183,0,"This is actually really confusing definitely when you combine it with the [docs](http://doctrine-orm.readthedocs.org/projects/doctrine-orm/en/latest/tutorials/embeddables.html#initializing-embeddables)
",False,0
DDC-3730: Embeddable hydrates to object instead of null,afoeder,120671163,4,182756650,0,"I must say I find this rather a bug than an improvement... agree with @boekkooi regarding the docs...
",False,0
DDC-3730: Embeddable hydrates to object instead of null,Ocramius,120671163,5,182907949,0,"Can anyone confirm if this is a dupe of #4670 and #4568?
",False,0
DDC-3730: Embeddable hydrates to object instead of null,afoeder,120671163,6,182917775,0,"well this _is_ #4568; and #4670 is rather a duplicate of #1275, both actually ""features"" or ""improvements"".

This here however is a Bug because embeddables whose all properties are nullable and being null hydrate to an empty object when being stored as null. This is misleading (saving != retrieving) and not according to the docs.
",False,0
DDC-3730: Embeddable hydrates to object instead of null,Ocramius,120671163,7,182934107,0,"@afoeder the order of the tickets is scrambled due to the fact that they were imported from Jira in December, heh
",False,0
DDC-3730: Embeddable hydrates to object instead of null,Harrisonbro,120671163,8,244704155,0,"Hi @Ocramius 

Has there been any further discussion on this topic? We've just hit into the same problem as described here on our first Doctrine project. 

Let me know if there's anything we can do to help ‚Äî provide usage examples, code samples, discussions, etc.
",False,0
DDC-3730: Embeddable hydrates to object instead of null,Ocramius,120671163,9,245402433,0,"@Harrisonbro as it currently stands, doctrine will not support nullable embeddables. That functionality may be implemented later, by implementing embeddables as hidden one-to-one records.
",False,0
DDC-3730: Embeddable hydrates to object instead of null,Harrisonbro,120671163,10,245548631,0,"OK. Is there any way for us to implement this on a case-by-case basis (eg. by hooking into the hydration process of an embeddable somehow) so we can manually check whether specific embeddables have enough data in the database to be considered 'valid' and therefore hydratable? Obviously we're not keen to allow value objects to be instantiated in an invalid state and then check an `isValid` method.
",False,0
DDC-3730: Embeddable hydrates to object instead of null,Ocramius,120671163,11,245548941,0,"> so we can manually check whether specific embeddables have enough data in the database to be considered 'valid' and therefore hydratable? 

Then just use the lifecycle system to (de-)hydrate VOs on your own, no?
",False,0
DDC-3730: Embeddable hydrates to object instead of null,Harrisonbro,120671163,12,245583515,0,"> Then just use the lifecycle system to (de-)hydrate VOs on your own, 
> no?

Do you mean lifecycle callbacks ‚Äî maybe `postLoad` ‚Äî as shown in 
http://docs.doctrine-project.org/projects/doctrine-orm/en/latest/reference/events.html#lifecycle-callbacks? 
Looks like those only work on entities, not value objects, as far as I 
can see? Eg. if I used `postLoad` an embeddable will already have been 
hydrated with invalid data (if the data in the database is all `null`, 
for example). Alternatively, if I move the VO properties onto the entity 
directly I‚Äôve lost the nice encapsulation that embeddable so usefully 
provides (eg. if I had a `Product` entity with a `SalePrice` with 2 
properties, `value` and `currency` I‚Äôd have to move those 2 properties 
onto the entity. Whilst I could then have those properties be private 
and do an `is_null` check for those 2 properties before instantiating 
and returning the VO from `getSalePrice() : SalePrice { ‚Ä¶ }` it does 
rather compromise my entity.

I‚Äôm almost certainly missing something here, sorry. Rather new to 
Doctrine so still learning!
",False,0
DDC-3730: Embeddable hydrates to object instead of null,Ocramius,120671163,13,245583905,0,"@Harrisonbro the idea is to NOT use embeddables there, and use a lifecycle listener to replace fields with embeddables then (manually). Doctrine will not implement nullability for embeddables for now.
",False,0
DDC-3730: Embeddable hydrates to object instead of null,Harrisonbro,120671163,14,245585486,0,"> @Harrisonbro the idea is to NOT use embeddables there, and use a lifecycle listener to replace fields with embeddables then (manually). Doctrine will not implement nullability for embeddables for now.

OK, gotcha.

So in the example I gave ‚Äî a `Product` entity which wants to use a `SalePrice` VO with 2 fields, `amount` and `currency` ‚Äî would you suggest simply putting a `sale_price_amount` and `sale_price_currency` property on the `Product` entity, make those private, and then have `Product::getSalePrice() : SalePrice` first check whether the 2 properties are `null` before attempting to instantiate and return the VO?

If so, that seems workable and means the entity is responsible for checking if the VO should be instantiated (rather than having the VO able to be ‚Äòinvalid‚Äô and have to implement an `isValid()` method).

Example code of what I mean:

``` php
    class Product
    {
        private $sale_price_amount;
        private $sale_price_currency;

        public getSalePrice() : SalePrice
        {
            if (
                is_null($this->sale_price_currency) || 
                is_null($this->sale_price_amount)
            ) {
                return null;
            }

            return new SalePrice(
                $this->sale_price_currency, 
                $this->sale_price_amount
            );
        }
    }
```

Is that something like what you‚Äôre suggesting instead of nullable embeddables?
",False,0
DDC-3730: Embeddable hydrates to object instead of null,Ocramius,120671163,15,245585880,0,">  So in the example I gave ‚Äî a `Product` entity which wants to use a `SalePrice` VO with 2 fields, `amount` and `currency` ‚Äî would you suggest simply putting a `sale_price_amount` and `sale_price_currency` property on the `Product` entity, make those private, and then have `Product::getSalePrice() : SalePrice` first check whether the 2 properties are `null` before attempting to instantiate and return the VO?

Correct.

Basically, since this is a scenario that Doctrine can't cover right now (because of how RDBMS DDL works), you can just implement it in userland for the few times where it pops up.
",False,0
DDC-3730: Embeddable hydrates to object instead of null,Harrisonbro,120671163,16,245594377,0,"OK great, thanks a lot for the help.
",False,0
DDC-3730: Embeddable hydrates to object instead of null,afoeder,120671163,17,245595610,0,"Thanks, I silently kept up reading your conversation :)
At the moment, my workaround is the following:

```
class Site
{
    /**
     * @var DomainName
     * @ORM\Embedded(class=""DomainName"", columnPrefix=""domain_"")
     */
    private $domainName;

    public function domainName()
    {
        return ((string)$this->domainName === '' ? null : $this->domainName);
    }
}

/**
 * @ORM\Embeddable
 */
class DomainName
{
    /**
     * Note this is only nullable in order to get the whole embeddable nullable (see [1] and [2]
     *
     * @var string
     * @ORM\Column(nullable=true)
     * @see http://doctrine-orm.readthedocs.org/projects/doctrine-orm/en/latest/tutorials/embeddables.html#initializing-embeddables [1]
     * @see https://github.com/doctrine/doctrine2/pull/1275 [2]
     */
    private $name;

    /**
     * Note this is only nullable in order to get the whole embeddable nullable (see [1] and [2]
     *
     * @var string
     * @ORM\Column(name=""escaped_name"", nullable=true)
     * @see http://doctrine-orm.readthedocs.org/projects/doctrine-orm/en/latest/tutorials/embeddables.html#initializing-embeddables [1]
     * @see https://github.com/doctrine/doctrine2/pull/1275 [2]
     */
    private $escapedName;

    public function __construct($name)
    {
        Assertion::notEmpty($name, 'The domain name must be provided.');
        Assertion::regex($name, '/^(?!www\.)([\pL\pN\pS-]+\.)+[\pL]+$/u', 'The domain name ""%s"" must be a valid domain name without the www. subdomain, but might have others.');

        $this->name = $name;
        $this->escapedName = static::escapeDomainName($name);
    }

    public function containsSubdomain()
    {
        return substr_count($this->name, '.') >= 2;
    }

    public static function escapeDomainName($name)
    {
        return preg_replace('/\./', '-', $name);
    }

    public function __toString()
    {
        return (string)$this->name;
    }
}

```
",False,0
DDC-3730: Embeddable hydrates to object instead of null,Harrisonbro,120671163,18,245597857,0,"I like that approach, @afoeder ‚Äî you still do an `is_null` check in the entity's getter method (`Site::domainName()` in your case) but you can still use an embeddable rather than having to hydrate your VOs yourself.

I suppose the major downside of your approach is that you do still have a VO in an inconsistent state, whereas if you don't let Doctrine hydrate the VO as an embeddable you avoid this; a bit more boilerplate & checking code, but you never have a VO in an invalid state.

Really it's just a trade-off between the 2 options. Others reading this should just be aware of the 2 options and their various merits.
",False,0
DDC-3730: Embeddable hydrates to object instead of null,BenMorel,120671163,19,277280210,0,"I would have preferred to comment on #1275, but the present issue has the benefit to be still open.

My 2 cents on the sensitive subject of  nullable embedded properties:

- when the Embeddable has at least one non-nullable `@Column`, and this field is null in the database, **there should be no ambiguity** and **`null` should be assigned to the embedded property**. Otherwise (currently!) you get an empty, invalid value object that has non-nullable properties set to `null`. IMHO, the current implementation is broken here.
- when all `@Column` in the Embeddable are nullable, there should be a boolean setting in the `@Embedded` annotation that controls whether or not you want an empty value object or a `null` value when all fields are `null` in the database. **Your choice**.

Finally, you only have a real problem when you have a fully nullable embeddable, **and** want to make the distinction between a `null` property and an empty object. People have suggested to add an extra column in the table, which would work, but would add a ton of complexity for what I think is an edge case. To clarify, **I think this edge case should not be supported by Doctrine**.

The previous two bullet points, however, **I would strongly suggest working on them ASAP**. I'll be happy to help, provided that lead developers are happy with the concept.",False,0
DDC-3730: Embeddable hydrates to object instead of null,Harrisonbro,120671163,20,277317820,0,"Thanks for that clear explanation, @benmorel. I quite agree with your suggested specification of how Doctrine _should_ behave, and that it should be worked on. This issue is the top priority I'd like to see addressed in doctrine. 

I too would be happy to help out with the development and testing of this, if the Doctrine team agree. ",False,0
DDC-3730: Embeddable hydrates to object instead of null,havvg,120671163,21,289269154,0,"We came across the same issue, in our domain a `StreetAddress` is optional, but if given, it has to contain all fields. All fields on the `Embeddable` are `nullable: true`, so the DB is working. The domain is ensuring valid state. So I created that listener to make Doctrine load the objects the way the domain contains objects prior persisting: https://gist.github.com/havvg/602055f1488271f68e5bc82f9a828b4d

Well, it only requires knowledge on the embeddable itself, but easy workaround for now. I hope this helps other developers until the issue will be resolved by Doctrine.",False,0
DDC-3730: Embeddable hydrates to object instead of null,szepczynski,120671163,22,297694497,0,"@havvg How to use this workaround?

For example I have User entity with embeddable class Gender. Something like that:
https://gist.github.com/szepczynski/d3028eb9f92fd7aadd08a578c7a92ad3

I know that I need the User entity should have registered postLoad listener NullableEmbeddableListener but I have no idea how to register it. Can you provide any example? I guess that I need somewhere call addMapping? ",False,0
DDC-3730: Embeddable hydrates to object instead of null,Evertt,120671163,23,299954666,0,"@BenMorel did you by any chance work on a PR for this? I'd really hope to have this ""bug"" resolved, but I don't understand doctrine well enough to be able to write a pretty PR for this issue.",False,0
DDC-3730: Embeddable hydrates to object instead of null,BenMorel,120671163,24,300000046,0,"@Evertt Not yet, and I won't until I get the green light from lead developers. I've invested a lot of time in other pull requests, that have been open for years and are still not merged. I can't waste any more time on this project I'm afraid!",False,Impatience
DDC-3730: Embeddable hydrates to object instead of null,Evertt,120671163,25,300000499,0,@BenMorel that's too bad and I totally understand! It's sad that huge projects like these at some point seem to slow down to a point that it just seems frozen.,False,0
DDC-3730: Embeddable hydrates to object instead of null,BenMorel,120671163,26,300003098,0,"That's the dark side of open source: projects rely solely on the free time developers can invest in them, and at some point they're just too busy on other businesses and/or family life to carry on with developments.

I, too, feel like Doctrine is slowing down; it's just unfortunate that there aren't enough (available) lead developers to keep up the pace with pull requests: many developers are there to offer their help, but without enough consideration from project leaders, it's just wasted brain processing time.",False,0
DDC-3730: Embeddable hydrates to object instead of null,theofidry,120671163,27,300015633,0,"@BenMorel the pace did slow down a bit last year, but the last months have been quite active :) Also see #6211",False,0
DDC-3730: Embeddable hydrates to object instead of null,szepczynski,120671163,28,300114518,0,doctrine 2.x is frozen because doctrine 3 is actively developing (I read somewhere post by @Ocramius),False,0
DDC-3730: Embeddable hydrates to object instead of null,Evertt,120671163,29,300130032,0,@szepczynski except that there's no ETA for doctrine 3 so that could take years for all we know. If it really takes that long it would be nice for improvements to still be added to doctrine 2.,False,0
DDC-3730: Embeddable hydrates to object instead of null,Ocramius,120671163,30,300130645,0," >  If it really takes that long it would be nice for improvements to still be added to doctrine 2.

It would make it a mess to migrate these additions to something completely redesigned. From what I can see in the last dozen releases, doctrine functionality already abundantly covers the 90% of use-case scenarios, so we could even call it ""feature complete"", if it wasn't for some rough edges that you encounter when you explore more shady features.",False,0
DDC-3730: Embeddable hydrates to object instead of null,Evertt,120671163,31,300138604,0,"@Ocramius I wouldn't call this issue right here a ""shady feature"". I think this is a very essential part of the embeddables system.",False,0
DDC-3730: Embeddable hydrates to object instead of null,Ocramius,120671163,32,300141600,0,"Right, and embeddables have barely been added in `2.5`, and are already removed in `develop` (`3.x`), as their fundamental internal working mechanisms need to be rewritten",False,0
DDC-3730: Embeddable hydrates to object instead of null,BenMorel,120671163,33,300148264,0,"@Ocramius Sorry to pollute this thread, but would the [transaction object](https://github.com/doctrine/dbal/pull/634) and [default lock mode](https://github.com/doctrine/doctrine2/pull/949) fit in 3.0?",False,0
DDC-3730: Embeddable hydrates to object instead of null,Ocramius,120671163,34,300148480,0,"@BenMorel most likely, yes",False,0
DDC-3730: Embeddable hydrates to object instead of null,Evertt,120671163,35,300172309,0,"> Right, and embeddables have barely been added in 2.5, and are already removed in develop (3.x), as their fundamental internal working mechanisms need to be rewritten

@Ocramius I'm not sure I understand you right. Do you mean they will come back in 3.x after their internal working mechanisms have been rewritten?",False,0
DDC-3730: Embeddable hydrates to object instead of null,Ocramius,120671163,36,300174407,0,"@Evertt yes, but likely as completely rewritten/redesigned.",False,0
DDC-3730: Embeddable hydrates to object instead of null,Harrisonbro,120671163,37,300602464,0,"@Ocramius Are there ways we can help the development of Doctrine, either v2 or v3? It's a tool we all use so would love to,support development if a can. 

_(Sorry to pollute this thread but not sure where else to write.)_",False,0
DDC-3730: Embeddable hydrates to object instead of null,Ocramius,120671163,38,300603229,0,"@Harrisonbro https://github.com/doctrine/doctrine2/milestones/3.0

Let's please stop going further OT. If you have a question, make a new issue.",False,Impatience
DDC-3730: Embeddable hydrates to object instead of null,dbu,120671163,39,320079515,0,FTR: there is a small 3rd party library that provides a listener for setting embedded entities that are all null to null (the gist that was discussed above): https://github.com/tarifhaus/doctrine-nullable-embeddable,False,0
DDC-3730: Embeddable hydrates to object instead of null,alcaeus,120671163,40,320163084,1,"@BenMorel With all due respect, I have to go r/quityourbullshit on you here:

> I've invested a lot of time in other pull requests, that have been open for years and are still not merged. I can't waste any more time on this project I'm afraid!

For Doctrine 2, [there are 26 pull requests from you](https://github.com/doctrine/doctrine2/pulls?utf8=%E2%9C%93&q=is%3Apr%20author%3Abenmorel): 2 Open, 2 Closed without merge (one was fixed differently, one would introduce a lot of pain with future pull requests), and 22 merged.
For DBAL, [there are 15 pull requests from you](https://github.com/doctrine/dbal/pulls?utf8=%E2%9C%93&q=is%3Apr%20author%3Abenmorel): 1 open, 2 Closed without merge, and 12 merged. A quick peek into other repositories (common, annotations, bundle, etc.) shows merged pull requests only.

Feel free to point out pull requests that you are waiting to get merged, but please don't say stuff like that without backing it up when other people sacrifice lots of free time to get you free software. Thank you.",False,Insulting
composer my nightmare,emiglobetrotting,124545273,1,124545273,0,"i don't know how to make it work; i'm trying to install a project that requires a composer installation. when i try to install the composer i'm having the is error

```
Some settings on your machine make Composer unable to work properly.
Make sure that you fix the issues listed below and run this script again:

The json extension is missing.
Install it or recompile php without --disable-json

The phar extension is missing.
Install it or recompile php without --disable-phar

The filter extension is missing.
Install it or recompile php without --disable-filter

The hash extension is missing.
Install it or recompile php without --disable-hash

The openssl extension is missing, which means that secure HTTPS transfers are impossible.
If possible you should enable it or recompile php with --with-openssl
```

To solve the openssl extension missing problem; i've installed openssl 1.0.1q and link ssl/bin/openssl to usr/bin/openssl;  meanwhile openssl is in my usr/local/src/php-5.6.13/ext.

The other missing i enabled them via the ./configure command option with, --enable-filter, --enable-hash, --enable-json, --enable-phar. To be precise you will find the configuration

./configure \
    --prefix='/usr/local/src/php-5.6.13' \
    --disable-cgi \
    --with-libdir='/lib/x86_64-linux-gnu/' \
    --with-config-file-path='/etc/php5' \
    --with-zlib \
    --with-pcre-regex \
    --with-mysql \
    --with-pdo-mysql \
    --with-mysqli \
    --enable-simplexml \
    --enable-xml \
    --enable-mysqlnd \
    --enable-pcntl \
    --enable-debug \
    --enable-maintainer-zts \
    --enable-mbstring \
    --enable-bcmath \
    --enable-exif \
    --enable-ftp \
    --enable-soap \
    --enable-sockets \
    --enable-opcache \
    --enable-zip \
    --enable-embedded-mysqli \
    --with-fpm-user=www-data  \
    --with-fpm-group=www-data \
    --enable-fpm \
    --enable-session \
    --enable-sysvmsg \
    --enable-sysvsem \
    --enable-sysvshm \
    --enable-xmlreader \
    --enable-xmlwriter \
    --enable-phar \
    --enable-json \
    --enable-hash \
    --enable-libxml \
    --enable-filter \
    --with-openssl \
    --with-curl

I've done the following below before running the ./configure and when i run make && make install i'm having this underneath some long running text that stops then this message appears. the message is this ""No rule to make target `pharcmd', needed by`all'. Stop.""

rm ./configure
rm -rf autom4te.cache/
rm -rf aclocal.m4
./buildconf --force
make clean.

Please how can i solve my problem.
",True,0
composer my nightmare,staabm,124545273,2,168311566,0,"You also need to load the extensions in your php.ini
",False,0
composer my nightmare,alcohol,124545273,3,169050276,0,"Composer has nothing to do with how you compile PHP. It simply points out what it needs/expects. If your compiled version does not offer that, then either recompile, or use any of the common default binaries provided for Linux/OSX/Windows which do contain everything that is necessary.

This issue is more about installing and configuring PHP though, so I'm closing it.
",False,Impatience
composer my nightmare,gilcierweb,124545273,4,378762344,1,"Composer is garbage this kind of mistake should not happen, error unacceptable.",False,Bitter frustration
Documenting how to install NVM for all users,AndreSteenveld,230340780,1,230340780,0,"I needed a global install of NVM because I have some node based cron jobs and a few legacy applications that are rather picky about which version of node they are able to work on. For a time I worked around this by sourcing the nvm script everywhere but that seems to be a somewhat unmaintainable solution. 

The requirements I have:
- NVM should just work in non-interactive sessions
- Every user should be able to select a installed version of node (or use ```$ nvm exec```)
- Some users should be able to install newer versions of node

This is the solution I came up with: (shout out to @icecoldphp, for the initial version)

0. I've done this on a debian 8 machine, as the root user
1. Create a group called ""nvm"", ```# groupadd nvm```
2. Add root to the nvm group ```# usermod -aG nvm root```
3. Goto the ```/opt``` directory and create a directory called nvm  
    - Make sure the groupd owner is nvm ```# chown :nvm ./nvm```
    - Set the permissions so that the group is allowed to write in there and all file will inherit the group ```# chmod g+ws ./nvm```
4. Follow the [git install](https://github.com/creationix/nvm#git-install) steps using ```/opt/nvm``` as the directory 
    - To make sure the group can also write aliases, cache downloads and install global packages make sure the directories exist and have the correct permissions:
    ```
    # mkdir /opt/nvm/.cache
    # mkdir /opt/nvm/versions
    # mkdir /opt/nvm/alias 
    
    # chmod -R g+ws /opt/nvm/.cache
    # chmod -R g+ws /opt/nvm/versions
    # chmod -R g+ws /opt/nvm/alias
    ```
5. Using the following snippet create ```/etc/profile.d/nvm.sh```:
    ```#/etc/profile.d/nvm.sh
    #!/bin/bash

    export NVM_DIR=""/opt/nvm""
    [ -s ""$NVM_DIR/nvm.sh"" ] && . ""$NVM_DIR/nvm.sh""

    ```
6. Ensure that the script is executable ```# chmod +x /etc/profile.d/nvm.sh```
7. If you want to use nvm in non-interactive sessions as well make sure to source the nvm file in ```/etc/bash.bashrc``` before the line saying ```# If not running interactively, don't do anything``` by adding ```. /etc/profile.d/nvm.sh```.
8. For bash completion (which is inherently interactive ;) add ```[ -s ""$NVM_DIR/bash_completion"" ] && \. ""$NVM_DIR/bash_completion""``` after the section about bash completion.

Every user can select a version of node (as the permissions for public are ```r-x```) and users in the nvm group can install and remove versions of node (permissions for the group are ```rwx```).

My questions are:
- As a developer I know next to nothing about linux, could this be improved, is it bad style, etc? Any feedback is welcome.
- Should this be documented in the NVM README.md?",True,0
Documenting how to install NVM for all users,ljharb,230340780,2,303203372,0,"nvm is not intended to be global or system-wide - it's per-user, per-shell-session.

Thus, each user account must have its own `$NVM_DIR`. They can certainly share an `nvm.sh`, but I'd recommend they all have their own one of those too.

I would not want to document anything in the readme that encourages people to use nvm across user accounts - there's other tools for that.",False,0
Documenting how to install NVM for all users,AndreSteenveld,230340780,3,303216648,0,"There is of course [n](https://github.com/tj/n) (with [n-install](https://github.com/mklement0/n-install)) which with a coaching could do the same. I'll give that a shot and create a gist of the process, what are the possible other tools, other than say apt-get, brew or some system level package manager which you are usable to manager the version of node in your shell?",False,0
Documenting how to install NVM for all users,ljharb,230340780,4,303219792,0,"Yes, `n` is the sole system-wide node manager I'd recommend.

However, I'd suggest just installing `nvm` in the cronjob user, and invoking the cronjobs such that `nvm.sh` is sourced.",False,0
Documenting how to install NVM for all users,rtwk,230340780,5,304429494,0,Yes it is always good to go with what nvm is intended for. per user. Go for per user installation. I have a similar kind of a situation (I used to install node without nvm previously) and did the same.,False,0
Documenting how to install NVM for all users,Spown,230340780,6,356898753,0,"> nvm is not intended to be global or system-wide - it's per-user, per-shell-session.

You know, for an util that should eliminate version discrepancies and staff it sure does increase it a lot... I mean I have server that deploys web projects on git pushes via hooks (i.e user = git). But sometimes I need to log in and redeploy the same things manually by invoking the git hooks manually (user = me). And sometimes my colleagues have to do the same (user = foo)... And then there is a process manager (PM2) that should be central for everyone, but it relies on node as well...

And everybody have it's own node&npm. Except root, so trying to sudo yields even less results, i.e ``node: command not found``. Ok, your util is the wrong one for these kind of  things, but NPM in it's [official docs explicitely says to use nvm](https://docs.npmjs.com/getting-started/installing-node) to avoid permission problems that unavoidably accompany multiuser usage.

",False,Bitter frustration
Documenting how to install NVM for all users,ljharb,230340780,7,357039868,0,"Yes - the hazard is ""multiuser usage"". In your use case, everyone should be running `node` as the same user.",False,0
Documenting how to install NVM for all users,hparadiz,230340780,8,410773213,1,"I'm writing a PHP wrapper for a NodeJS binary and I have this exact same problem. PHP is running as ""www"" so it **has no home directory**.

PHP can't ""see"" the NVM environment.

I had this exact same problem with getting node running in crontab but at least there I could control the user to be anything.

---
Thank you @AndreSteenveld 

This is yet another example of open-source project maintainer arrogance. Your use case doesn't matter so jump through these hoops.",False,Insulting
Allow decrypting of files with vaulted variables,jhkrischel,239227953,1,239227953,0,"##### ISSUE TYPE
 - Feature Idea

##### COMPONENT NAME
ansible-vault

##### ANSIBLE VERSION
```
ansible 2.3.1.0
  config file = 
  configured module search path = Default w/o overrides
  python version = 2.7.13 (default, Apr 23 2017, 16:50:35) [GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)]
```

##### CONFIGURATION
n/a

##### OS / ENVIRONMENT
n/a

##### SUMMARY
ansible-vault decrypt allows the decryption of completely encrypted yaml files, but it will not decrypt vaulted variables in an unencrypted yaml file with encrypted variables.

It would be nice, for CLI purposes, to have decrypt take a partially encrypted file, and give us the decrypted text.

##### STEPS TO REPRODUCE
* create `test.yml` file with single encrypted variable encrypted by `~/.vault_pass.txt`
* ansible-vault decrypt file

```
ansible-vault decrypt test.yml --vault-password-file ~/.vault_pass.txt
```

##### EXPECTED RESULTS
* Expected plain text output with encrypted variable decrypted.

##### ACTUAL RESULTS
```
ERROR! input is not vault encrypted data for test.yml
```
",True,0
Allow decrypting of files with vaulted variables,gundalow,239227953,2,311905076,0,@alikins I believe you look after vault,False,0
Allow decrypting of files with vaulted variables,krzysztof-magosa,239227953,3,313492245,0,Would be also good if 'ansible-vault view' worked for such files.,False,0
Allow decrypting of files with vaulted variables,alikins,239227953,4,317448528,0,"This might be something that will get covered in https://github.com/ansible/ansible/blob/devel/docs/docsite/rst/roadmap/ROADMAP_2_4.rst#id25

As a user, what would you expect the decrypted file to look like?

First thought is just to replace the !vault yaml scalar with the decrypted text. That probably makes the most sense for 'view'.

For 'decrypt' and especially 'edit', I'm not sure that will be sufficient. For 'edit', the re-encrypt phrase is going to need to be able to figure out which variable values originally came from a vaulted value. Especially if the file is edited significantly (reordering lines for example, or changing the variable name). 

So the file presented for editing would need to include some markers indicating the text that was decrypted/should be re-encrypted. A couple of ways to do that:

   1) Add comments to mark the text, and doing some text manipulation/regexes to replace it with encrypted text in place. Something like:

``` yaml
# START VAULT PLAINTEXT - my_var
my_var: my text goes here
# END VAULT PLAINTEXT - my_var
some_plain_var: blippy
```

  2) Add a new yaml type indicating text to be encrypted. Something like:

``` yaml
my_var: !vault-plaintext |
    my text goes here
some_plain_var: blippy
```

It would be best if we could yaml parse the input, decrypt the value, serialize the yaml to a file for editing, let user edit it, then yaml parse the results, encrypt the value, and serialize to yaml and save.

But... doing that with the available yaml parser would lose comments and ordering of maps.

So likely some in place string/text manipulations will be required.",False,0
Allow decrypting of files with vaulted variables,alikins,239227953,5,329495428,0,"Not going to happen for 2.4, so bumped to 2.5.",False,0
Allow decrypting of files with vaulted variables,ansibot,239227953,6,361973797,0,"@jhkrischel This issue is waiting for your response. Please respond or the issue will be closed.

[click here for bot help](https://github.com/ansible/ansibullbot/blob/master/ISSUE_HELP.md)
<!--- boilerplate: needs_info_base --->",False,0
Allow decrypting of files with vaulted variables,frolundo,239227953,7,362034126,0,"Any news when this is planned to be implemented in ansible?
We have lots of passwords as vaulted variables, hence updating\viewing them is troublesome.
I did some script (based on solution, from alikins last post) to at least parse such yml and decrypt every variable to stdout\file to see a decrypted file at once, but this is just a script that is not a complete solution (and it is decrypting only).
UPD: I ended up going thru ansible code to understand how it works with encrypted variables and wrote some tiny script that I can use in my automation jobs with Jenkins. I hope it would be useful for anyone who is waiting for this issue to be fixed.
https://github.com/andrunah/ansible-vault-variable-updater
It would be nice to have this functionality in ansible out of the box.",False,0
Allow decrypting of files with vaulted variables,csillab,239227953,8,370435473,0,"I do see this in 2.5.

```
root@ubuntu-xenial:~# ansible  --version
ansible 2.5.0rc1 (stable-2.5 36566e62a7) last updated 2018/03/05 13:46:00 (GMT +000)
  config file = /etc/ansible/ansible.cfg
  configured module search path = [u'/root/.ansible/plugins/modules', u'/usr/share/ansible/plugins/modules']
  ansible python module location = /root/git/ansible/lib/ansible
  executable location = /root/git/ansible/bin/ansible
  python version = 2.7.12 (default, Nov 20 2017, 18:23:56) [GCC 5.4.0 20160609]

root@ubuntu-xenial:~# cat vars.yaml
ansible_ssh_pass: !vault |
          $ANSIBLE_VAULT;1.2;AES256;my_user
          31313064366365626535323066613234626234336664333266663161366233396365633063303539
          3066363333666236666335656631666663373037643338630a303763363031373337663733326134
          38336566366535373561373830386638663635363438333633313536333731646331366138383961
          3331346163623661340a663862323337313562376338386539326438323562383136383832376266
          31306663393532323761353761353435373432633632626365633734303335633436
nonpass: pass
root@ubuntu-xenial:~# ansible-vault view vars.yaml
Vault password:
ERROR! input is not vault encrypted datavars.yaml is not a vault encrypted file for vars.yaml
```",False,0
Allow decrypting of files with vaulted variables,ansibot,239227953,9,370438572,0,"@jhkrischel This issue is waiting for your response. Please respond or the issue will be closed.

[click here for bot help](https://github.com/ansible/ansibullbot/blob/master/ISSUE_HELP.md)
<!--- boilerplate: needs_info_base --->",False,0
Allow decrypting of files with vaulted variables,alikins,239227953,10,371898299,0,"I poked at this a little yesterday and braindumped some thoughts in code comments at https://github.com/alikins/ansible/commit/603cac4a041a10ec8186617c95ef539a9ece787a

(copied/paraphrased here for discussion)

>         Open a file, figure out if it is all vault or yaml with vault strings, edit.
> 
>         if yaml with vault strings, parse the yaml with AnsibleYaml 
>         and secret. Replace with '!vault-plaintext vault-id' and plaintext. 
>         Save, open editor. 
>         On save/reencrypt, reparse the file with AnsibleYaml, get the
>         plaintext of the to be reencrypted vaulted string, encrypt it
>         (!vault-plaintext -> !vault,
>          AnsibleVaultUnencryptedUnicode -> AnsibleVaultEncryptedUnicode).
> 
>         And then, things get complicated... we can't just AnsibleYaml.dumps()
>        the data structure out:
>             1. Comments and comment placement is not preserved which 
>                 is kind of annoying
>             2. AnsibleYaml can loads things into data structures
>                 that it can not `dumps()` out.
>                 Ie, we can't serialize a bunch of stuff we can deserialize.
> 
>             So just AnsibleYaml.dumps'ing the datastructure back
>             to a file will usually either fail or do the wrong thing.
> 
>             #2 above is unlikely to get fixed soon if ever.
>             #1 is mostly a limitiation of the PyYaml yaml module ansible uses. 
>                Other implementation like Rueyaml can do this, but it is unlikely for 
>                ansible to change this any time soon.
> 
>         So, since we can't just serialize to yaml, we likely need to do some 
>         string manipulation to replace the '!vault ' blob.
> 
>         We would need to know exactly what the before string looked like
>         and where in the file it is, and what the new !vault will look like.
>         But we don't really know what the new  !vault-plaintext 
>         string will look like.
> 
>         For that  matter, we don't know if it will be in the same place,
>         or if it will exist at all, or if it will be at the same path in the 
>         datastructure after the edit. 
> 
>         We could limit edit to only try to work in cases where
>         those aren't changed. We also have no idea what the
>         plaintext will look like.
>         
>        ideas:
>            - !vault-plaintext is a compound yaml type, with fields for
>               the vault id to use, and for the plaintext. Could also
>              possibly include some identifying info for what the !vault
>              it replaced looked like. An example:
> 
>         some_var: !vault-plaintext:
>                     vault_id: 'dev'
>                     decrypted_from: |
>                                     $ANSIBLE_VAULT;1.1;AES256
>                                     66393964663765613335633461643334393234346231666665306635323635333137306339356232
>                     plaintext: |
>                                 The new plaintext to replace decrypted_from with
> 
>         That would give vault-edit enough info to do a reliable job 
>         of replacing the previous content.
> 

       

The downside to that approach is that it points out the limitations of the current !vault format. It may also be useful to extend !vault to support getting a data structure with info in it instead of just the plain text scalar. At the moment, I'm not sure if it could do both but it seems possible.

Or could just call the extended info version of !vault  !vault-extended or similar.  At that point it might be possible to make !vault-extended the default vault blob format for vaulted files as well. ie, instead of
a vaulted file being:

```
$ANSIBLE_VAULT;1.1;AES256
66393964663765613335633461643334393234346231666665306635323635333137306339356232
3533306631646431663239623762366365663137383435380a393139303161383561303336623962
35373663663036333863373666326634616532376335333133326163376136353636633763623739
3736343064326662390a306438356239386665306437646665323836393032393565666136643362
3663
```

It would be yaml something like

``` yaml
--- 
- !vault-extended:
    vault_id: 'dev'
    cipher: AES256
    encrypted_on: 2018-03-14
    ciphertext: |
        $ANSIBLE_VAULT;1.2;AES256;dev
        66393964663765613335633461643334393234346231666665306635323635333137306339356232
                  
```

ie, more or less like https://github.com/voxpupuli/hiera-eyaml",False,0
Allow decrypting of files with vaulted variables,gail-b-stewart,239227953,11,373044736,0,Would it be possible for the user to tell you which scalars to decrypt - not try to do the whole file?,False,0
Allow decrypting of files with vaulted variables,MarkusTeufelberger,239227953,12,380822610,0,"Right now the usability of encrypted variables compared to whole encrypted files is rather poor unfortunately. Especially in cases where I quickly need to access an encrypted variable (e.g. a password) I really don't want to google for solutions like https://stackoverflow.com/questions/43467180/how-to-decrypt-string-with-ansible-vault-2-3-0.

It is also a problem for `git diff` use cases (https://stackoverflow.com/questions/29937195/how-to-diff-ansible-vault-changes). Is improving this state still on the roadmap? I didn't find it neither for 2.5 nor 2.6...",False,0
Allow decrypting of files with vaulted variables,ghost,239227953,13,389723552,0,"Hi, so I've figured out a way to do this for checking individual values, using a yaml parser, **yq** https://github.com/mikefarah/yq (there's more than one yq project, but I used this). This works with ansible 2.5.2

I have a vars file, with encrypted and unencrypted values, `all.yml`

```
unencrypted_value: 1234
encrypted_value: !vault |
          $ANSIBLE_VAULT;1.1;AES256
          37316535353565313063353530353539666634363834626664366263666538346131653332353932
          3637363030613037316336306466656432353463383230370a396530323164353563363434663238
          30336436396264656663663837346162323762333063376631326633356533376566633563386637
          6531383261396366640a363339616164333630373730613564646434386364396534653063666238
          6131
```

I have a password file, `vault-password`
```
password
```

Using `yq`, I'm able to decrypt the value pretty easily, by selecting the encrypted value and passing it to the decrypt function

```
$ yq read all.yml encrypted_value | ansible-vault --vault-id vault-password decrypt
Decryption successful
secretsecret
```

Hope this helps!",False,0
Allow decrypting of files with vaulted variables,MarkusTeufelberger,239227953,14,389833783,0,"Thanks, this helps if it is possible to install additional software. I would argue that ansible-vault should also have this functionality built-in.",False,0
Allow decrypting of files with vaulted variables,ghost,239227953,15,390048688,0,Yeah definitely that'd be the best option :),False,0
Allow decrypting of files with vaulted variables,brianguy,239227953,16,411567862,0,"Also related, let rekey work on all encrypted variables in a file. There doesn't seem to be a good way to rekey all the encrypted variables, which makes encrypted variables super cumbersome now that we have to rekey (will end up having script this). Even if it just spits it back out to stdout that'd be a huge help instead of modifying the variables in the file directly.",False,0
Allow decrypting of files with vaulted variables,varac,239227953,17,415326258,0,"Why is this issue still assigned to the `2.5 milestone` when `ansible 2.5` is already release a long time ago ? See #44556 for outdated milestones.

Please reassign to a current milestone, this is a really missing feature imo (especially the lack of rekeying functionality).",False,Impatience
Allow decrypting of files with vaulted variables,Goobaroo,239227953,18,425570600,0,It would be nice if rekey worked this way as well.  Updating only the encrypted values in a mixed variable file.,False,0
Allow decrypting of files with vaulted variables,geerlingguy,239227953,19,454476078,0,"Just giving another thumbs up on this; something like the `yq` solution above works okay and can be scripted, but having the functionality be part of `ansible-vault` itself would make management and re-keys so much simpler, and require one fewer dependency.",False,0
Allow decrypting of files with vaulted variables,boormat,239227953,20,476400031,0,"A simple but effective solution would be to keep the existing symantecs of ansible vault encrypt, decrypt and view commands, to detect and encrypt and decrypt values of complete files.

For existing users of encrypted files, it would be trivial to convert to the enrypted values.  It could even be considered best practice is to keep encrypted values in files named such as secrets.yml, to make it easier to spot accidently unencrypted secrets.

During the encrypt phase, it would convert any unencrypted values to encrypted values.  This would allow users to very simply add new values just by editing the ""secrets.yml"", test as required, then run the encypt command.   Users would be able to enforce or check encryption by git hooks or similar.
",False,0
Allow decrypting of files with vaulted variables,MarkusTeufelberger,239227953,21,476406076,0,"That solution would be simple, but likely not enough. For example every variable can be encrypted with a different secret/vault identifier. Also encrypted and unencrypted variables can be mixed.

I'd still like to have a way at least to decrypt all variables belonging to a vault ID transparently using `ansible-vault`. Seriously, this is a usability problem since Ansible 2.3! This makes it nearly impossible for me to use vaulted variables, since being able to run `git diff` on changes is important.",False,0
Allow decrypting of files with vaulted variables,steffann,239227953,22,499032715,0,This is still a problem with Ansible 2.8... A solution would be really appreciated!,False,0
Allow decrypting of files with vaulted variables,steffann,239227953,23,499063784,0,"For others looking for a quick solution I created this script: https://gist.github.com/steffann/240d4170e45aa3cf7cf0df5e9beaf0ba

It uses [ruamel.yaml](https://yaml.readthedocs.io/), which preserves ordering, comments etc in the YAML file. Great when depending on decent git diffs etc :)",False,0
Allow decrypting of files with vaulted variables,varac,239227953,24,499095867,0,maybe a bit unrelated but I like how [sops](https://github.com/mozilla/sops) does it.,False,0
Allow decrypting of files with vaulted variables,fzink,239227953,25,500969741,1,"Running into this issue again and it sucks. Please guys, this issue has been open for almost 2 years now and for people who really use ansible-vault, this is a major pain the butt.",False,Bitter frustration
Decide on a consistent naming of either `Jinja` or `Jinja2`,jeffwidman,245325788,1,245325788,0,"Continuing discussion from https://github.com/pallets/meta/issues/10#issuecomment-209980352

The naming is inconsistent:
- Github repo is `jinja`
- Pypi package name is `jinja2`
- Pallets project calls it ""Jinja"": https://www.palletsprojects.com/p/jinja/
- RTD namespace is jinja2.readthedocs.io
- Pocoo docs (currently the official ones) are ""Jinja"": http://jinja.pocoo.org/docs/2.9/
- file extensions are sometimes `.jinja`, `.j2`, `.jinja2`... Ansible project currently uses `.j2`

We should pick either ""Jinja"" or ""Jinja2"" and use it everywhere for consistency. 

I am open to either, ""Jinja"" is simpler and shorter, but ""Jinja2"" has a more distinctive ring to it and less likely to get confused with any other projects. 
",True,0
Decide on a consistent naming of either `Jinja` or `Jinja2`,davidism,245325788,2,317729724,0,"The Stack Overflow tag is ""jinja2"", ""jinja"" is a synonym that gets invisbly converted. Despite my efforts towards the opposite. (This happened a year or so ago.)

I really want to drop the ""2"" from the name. Start adding v2 builds to the ""jinja"" PyPI page. Deprecate the ""jinja2"" import and go back to the ""jinja"" namespace.",False,0
Decide on a consistent naming of either `Jinja` or `Jinja2`,jeffwidman,245325788,3,318238026,0,@ThiefMaster @mitsuhiko @untitaker do you guys have opinions?,False,0
Decide on a consistent naming of either `Jinja` or `Jinja2`,mitsuhiko,245325788,4,318277715,0,I think we can do that but I would personally propose to align the 3.0 release with that.,False,0
Decide on a consistent naming of either `Jinja` or `Jinja2`,ThiefMaster,245325788,5,318283181,0,":+1: on waiting for 3.0.

---

> The Stack Overflow tag is ""jinja2"", ""jinja"" is a synonym that gets invisbly converted. Despite my efforts towards the opposite. (This happened a year or so ago.)

I may be able to fix that.


Edit: Yes, I can

> **Rename preview**
> jinja2 will be removed from 3486 questions
> jinja will be added to 3486 questions
> 5 commitments to jinja2 Documentation proposal will be moved to the jinja proposal
> A tag synonym mapping jinja2 ‚Üí jinja will be created.
> (these counts include deleted questions and exclude overlapping tags)",False,0
Decide on a consistent naming of either `Jinja` or `Jinja2`,jeffwidman,245325788,6,318497746,0,"What is the timeline for 3.0 release?

The sooner we start giving folks a heads up the better, so what about adding a deprecation warning now on `jinja2` imports and a warning on `jinja` imports that we will soon be pushing v3 out to the `jinja` namespace?

",False,0
Decide on a consistent naming of either `Jinja` or `Jinja2`,jeffwidman,245325788,7,318497949,0,"@davidism are you able to move the RTD namespace over to `jinja`? Per my comment above, it's currently under `jinja2`, and IIRC, you were driving the cleanup/ownership migration of the RTD namespaces for other projects?",False,0
Decide on a consistent naming of either `Jinja` or `Jinja2`,mitsuhiko,245325788,8,318498173,0,In a way the last major release of Jinja2 was a massive change in the engine. Not even sure if there is more stuff we need to break :D,False,0
Decide on a consistent naming of either `Jinja` or `Jinja2`,nixjdm,245325788,9,320456591,0,"Saving breaking changes and name consolidation for a Jinja v3 sounds great to me. We might as well try to find what breaking changes we can slate for it.

I'd like to remind everyone of a potential one - [allowing included block overrides](https://github.com/pallets/jinja/issues/243). That issue doesn't have to mean a breaking change, but if that's the route you all want to go, remaking/opening that issue with a v3 milestone is how I'd do it. Sorry for the tangent. :) Perhaps we can make another ticket for discussing what to break / milestone for Jinja v3.",False,0
Decide on a consistent naming of either `Jinja` or `Jinja2`,jeffwidman,245325788,10,322840927,0,"nudge @davidism - per my comment above, are you able to modify the RTD namespace from jinja2 to jinja?",False,0
Decide on a consistent naming of either `Jinja` or `Jinja2`,davidism,245325788,11,543329245,0,"In the 2.11 release, I'm thinking of renaming the package to `jinja`, with a placholder module for `jinja2` that forwards all imports and issues a deprecation warning.

I'll still have to work out the timing of this next step, but I'd also like to try moving back to the ""Jinja"" name on PyPI. I think what I'd try to do is have a **Jinja** 2.11 build that includes the `jinja2` placeholder, and make the **Jinja2** 2.11 build just depend on `jinja>=2.11`, or have a small shim that explains installing the other name without breaking any code. I'm am willing to take on the extra effort of keeping these builds in sync for a while while we manage a transition.",False,0
Decide on a consistent naming of either `Jinja` or `Jinja2`,mitsuhiko,245325788,12,543341151,0,@davidism this shouldn't happen in a point release. This would break pickle and a bunch of other things.,False,0
Decide on a consistent naming of either `Jinja` or `Jinja2`,mitsuhiko,245325788,13,570631941,0,"Since I gave my blessings before I want to actually qualify this somewhat. I have some stomach ulcers with this change. Ultimately I don't think it's particularly useful for users (it just drops one character), introduces some backwards incompatibility concerns and it undoes a learning I made back when Jinja2 was originally released.

The reason the package renamed with 2.0 was that there was no way (and there still is no way) to have parallel installations of Python libraries that are incompatible unlike node or rust can.  Because of that I think we're going to be sooner or later again in a stupid situation where Jinja 4.0 would need to be named ""Jinja4"" on pypi.

So I think while this rename is somewhat okay I generally don't think anymore that it's a good idea. I think this change would be without concerns if the Python import system were to support imports with different versions which however I gave up hoping for.",False,Impatience
Decide on a consistent naming of either `Jinja` or `Jinja2`,untitaker,245325788,14,574765174,1,"@coleifer I really have no idea what you're suggesting other than ""let's just revert this"". We won't release this as a patch/bugfix release, so I guess you are not happy that this will land in 2.11. Are you expecting us to release Jinja 3 for this? That would cause even more problems in a dependency tree that has multiple package dependant on Jinja.

Honestly I find your behavior completely unacceptable and hope it will have consequences.",False,Threat
Trademark violation,yroc92,257408022,1,257408022,0,"The word ""Caddy"" in the context of software is under a pending trademark application. By using the name Caddy in your repo, along with the associated logos, you're in violation of this trademark. Please remove all such references :)",True,Mocking
Trademark violation,lol768,257408022,2,329191018,0,"> The word ""Caddy"" in the context of software is under a pending trademark application

So it's not a registered trademark?

> By using the name Caddy in your repo, along with the associated logos, you're in violation of this trademark. Please remove all such references :)

See issue #1 - this is planned üòÉ ",False,0
Trademark violation,yroc92,257408022,3,329192024,0,"Technically, there is an implicit trademark because of the continued use and recognition of the brand over the years, and it is legally enforceable. But we have also submitted a formal trademark request as well, which is pending. Just FYI, we have no problem with anyone forking Caddy. We enjoy open source!

Can I ask what your use case for Caddy is? ",False,0
Trademark violation,ddevault,257408022,4,329193098,0,"Pretty disingenous for you to refer to yourself as a member of Caddy's ""we"" when so far as I can tell you're not involved: https://github.com/mholt/caddy/graphs/contributors",False,Insulting
Trademark violation,yroc92,257408022,5,329193635,0,"@SirCmpwn I'm part owner of Light Code Labs, the legal entity that filed for the trademark.",False,0
Trademark violation,ddevault,257408022,6,329193792,1,"I see. Well, in any case, be patient, and maybe also try contributing to your own web server?",False,Insulting
Updating only one locked dependency,k4nar,268049628,1,268049628,0,"Sometimes I'm doing a PR and I want to update a specific dependency but I don't want to deal with updates of all my dependencies (aiohttp, flake8, etc‚Ä¶). If any breaking change was introduced in those dependencies, I want to deal with it in another PR.

As far as I know, the only way to do that would be to pin all the dependencies that I don't want to update in the Pipfile. But I find it to defeat the purpose of Pipenv in the first place :) .

So my feature request would be to be able to do something like:
```shell
$ pipenv lock --only my-awesome-dep
```
That would generate a Pipfile.lock with updates for only `my-awesome-dep` and its dependencies.

I can probably make a PR for that, but I would like to get some feedback first.",True,0
Updating only one locked dependency,k4nar,268049628,2,339006877,0,"That could also be useful for `pipenv install`, as sometimes I want to install a new dependency without updating others.",False,0
Updating only one locked dependency,vphilippon,268049628,3,339018289,0,"There's a little thing to take into account here: Changing a single dependency could change the overall set of requirements.
Ex: Updating `foo` from 1.0 to 2.0 could require to update `bar` to >=2.0 (while it was <2.0 before), and so on.

I know that in the context of `pip-tools` itself (from which `pipenv` takes its dependency resolution algorithm), running the dependency resolution will only ""update"" the required packages when ""re-locking"" if there's an existing lock file. It does so by checking if the existing pins in the lockfile are valid candidate first when selecting candidate in the resolving. `pipenv` could probably do the same.

I think its a reasonable idea. Otherwise, if you want to update absolutely only one dependency, `pipenv` would have to have a mode to block if changing a dependency causes other changes, or else you would loose the guarantee of a valid environment.

I hope this helps!",False,0
Updating only one locked dependency,k4nar,268049628,4,339032761,0,"Indeed, that was what I meant by:
> That would generate a Pipfile.lock with updates for only my-awesome-dep and its dependencies.",False,0
Updating only one locked dependency,brettdh,268049628,5,339117289,0,"Agree 100% - and I'll go a bit farther: this should be the default.

That is, `pipenv install foo` should never touch anything besides `foo` and its dependencies. And `pipenv lock` should certainly never upgrade anything - it should just lock what's already installed.

AFAICT, this is how `npm`, `yarn`, `gem`, etc. work; it makes no sense to have a lockfile that doesn't actually lock packages, but trusts package authors to not break things in patch releases, and therefore upgrades them without being asked. I can see the use of allowing upgrades, but that should be opt-in, since it's more surprising than not upgrading them.

I apologize if I'm hijacking this issue for something else, but since this is so closely related to an issue I was about to create, I thought I'd start the conversation here. Feel free to tell me I should make a new one.",False,0
Updating only one locked dependency,brettdh,268049628,6,339121458,0,"Just found this related issue as well: https://github.com/kennethreitz/pipenv/issues/418

Being able to specify `pipenv install --upgrade-strategy=only-if-needed` seems like what I'm looking for, though of course as I mentioned I think that should be the default, as it's becoming in pip 10. But being able to specify it semi-permanently via env var would be something, anyway.

I would be surprised if that change breaks anyone's workflow ([famous last words](https://xkcd.com/1172/)), since it's more conservative than `--upgrade-strategy=eager`.",False,0
Updating only one locked dependency,brettdh,268049628,7,339312709,0,"Tried to work around this by setting `export PIP_UPGRADE_STRATEGY=only-if-needed` in my shell config. This doesn't work, and `pipenv lock` exhibits these surprising behaviors:
1. It ""upgrades"" packages that don't need to be upgraded (but...)
1. It actually doesn't upgrade the installed versions! i.e. `pip freeze` and `Pipfile.lock` show different versions!

Guessing pipenv is delegating to pip for the install, and pip respects its environment variable settings, but `pipenv lock` doesn't.",False,0
Updating only one locked dependency,techalchemy,268049628,8,339545831,0,"@k4nar What happens right now that you are finding undesirable?  Because if you upgrade a dependency that has cascading requirements obviously it will have consequences for other dependencies.  Are you suggesting some kind of resolver logic to determine the most current version of a specific package _in the context of the current lockfile_? I am hesitant to encourage too many hacks to resolution logic, which is already complicated and difficult to debug.

@brettdh I think I can shed some light because you have most of the pieces.  `pipenv lock` doesn't install anything, and it doesn't claim to.  It only generates the lockfile given your host environment, python version, and a provided `Pipfile`.  If you manipulate your environment in some other way or if you use pip directly/manipulate pip settings outside of pipenv / are not using `pipenv run` or using `pip freeze` inside a pipenv subshell, it is quite easy for a lockfile to be out of sync from `pip freeze`.  The two aren't really related.

To be clear:

1. `Pipfile.lock` is a strictly-pinned dependency resolution using the pip-tools resolver based on the user's `Pipfile`
2. If you want to maintain strict pins of everything while upgrading only one package, I believe you can do this by strictly pinning everything in your `Pipfile` except for the one thing you want to upgrade (correct me if I'm wrong @vphilippon)

As for your lockfile and `pip freeze` disagreeing with one another, I'd have to know more information, but I believe we have an open issue regarding our lockfile resolver when using non-system versions of python to resolve.",False,0
Updating only one locked dependency,k4nar,268049628,9,339591307,0,"@techalchemy : If I have a Pipfile.lock with A, B and C where B is a dependency of A, I would like to be able to update A and B without updating C, or C without updating A and B.
Again of course I can pin all my dependencies & their dependencies in my Pipfile in order to do that, but that would be a burden to maintain (like most `requirements.txt` are).",False,0
Updating only one locked dependency,brettdh,268049628,10,339625024,0,"I concur with everything @k4nar wrote. Sure, I could even just pin
everything in requirements.txt and not use pipenv. The point of pipenv is
to have one tool that makes that (and the virtualenv stuff, of course)
simpler to manage; i.e. all packages are locked by default to a version
that‚Äôs known to work, but it should be straightforward to upgrade a select
few (without unexpectedly upgrading others).
On Thu, Oct 26, 2017 at 4:28 AM Yannick P√âROUX <notifications@github.com>
wrote:

> @techalchemy <https://github.com/techalchemy> : If I have a Pipfile.lock
> with A, B and C where B is a dependency of A, I would like to be able to
> update A and B without updating C, or C without updating A and B.
> Again of course I can pin all my dependencies & their dependencies in my
> Pipfile in order to do that, but that would be a burden to maintain (like
> most requirements.txt are).
>
> ‚Äî
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/kennethreitz/pipenv/issues/966#issuecomment-339591307>,
> or mute the thread
> <https://github.com/notifications/unsubscribe-auth/AAFlnqUOEKARiFD8kEk3GVczF3NXBdVOks5swEKcgaJpZM4QEf-->
> .
>
",False,0
Updating only one locked dependency,techalchemy,268049628,11,339652646,0,"Hm I see what you guys are saying. The premise of passing a setting to pip is not what I‚Äôm worried about, it‚Äôs resolving with pip-tools that concerns me. What does this behavior look like right now?",False,0
Updating only one locked dependency,brettdh,268049628,12,339676768,0,"@techalchemy I mentioned the `pip freeze` difference as a shorthand for ""the package versions that `pipenv install` installs differ from the package versions that `pipenv lock` saves to `Pipfile.lock`.""

True, this only happens when I've changed pip's default args via environment variable; I was just pointing out that it was surprising that pipenv delegated to pip for installation but not for version locking; i.e. rather than locking what's installed, it locks what it thinks *should* be installed, potentially with unrequested upgrades.

Could you clarify your question a bit? I think ""resolving with pip-tools"" is referring to what `pipenv lock` is doing, and the reason it's not affected when I set pip defaults? And could you be more specific about what you mean by ""this behavior""?",False,0
Updating only one locked dependency,vphilippon,268049628,13,339707418,0,"@brettdh The locking mechanism include a notion of ""dependency resolution"" that does not exist in `pip`. Its handled by `pip-tools` (or rather, a patched version of it, integrated in a special way by `pipenv` that bring a few differences with the original tool). In short, the locking mechanism reads the `Pipfile` and performs a full dependency resolution to select a full set of package that will meet *every* constraints defined by the required packages *and their dependencies*.

@techalchemy 
> [...] it‚Äôs resolving with pip-tools that concerns me.

I'm not sure how those `--upgrade-strategy` would affect `pip-tools`, because it works on some low-level internals of `pip`. I have the feeling this would not give the expected result, as these option take into account what's installed, and that's not what's being dealt with in that mechanism. But we have another approach to this in `pip-tools` that could be done here.

The ""original"" `pip-tools` behavior is that it only updates what's is needed in the lockfile (in its context, its the requirements.txt), but this was ""lost"" in the way the resolver was integrated in `pipenv`. Let me explain why.

Pointing back to my resume of how `pip-tools` works: https://github.com/kennethreitz/pipenv/issues/875#issuecomment-337717817
Remember the ""select a candidate"" part? That's done by querying the `Repository` object.
In `pipenv`, we directly configure a `PyPIRepository` for the `Resolver`, but `pip-tools` does something else, it uses a `LocalRequirementsRepository` object, which keeps the existing pins from the previously existing `requirements.txt` (if found), and ""fallbacks"" on `PyPIRepository`.

So in `pip-tools`, the following happens when selecting a candidate:
1. Query `LocalRequirementsRepository` for a candidate that match `foobar>=1.0,<2.0`.
1. Check if an existing pin meets that requirements:
    - If yes, return that pin as the candidate.
    - If not, query the `proxied_repository` (`PyPIRepository`) for the candidate.
1. Use the candidate returned

Effectively, it means that existing pins are given a ""priority"" as candidate to try first.

But in `pipenv`, currently, it simply:
1. Query `PyPIRepository` (directly) for a candidate that match `foobar>=1.0,<2.0`.
1. Use the candidate returned.

So, I think the same behavior for the locking in `pipenv` could be done by parsing the `Pipfile.lock` to get the existing pins and use a `LocalRequirementsRepository`, like `pip-tools` does in its `pip-compile` command.",False,0
Updating only one locked dependency,techalchemy,268049628,14,339749088,0,@vphilippon do you have a sense of how difficult implementation on that would be?,False,0
Updating only one locked dependency,vphilippon,268049628,15,339791934,0,"@techalchemy 
- Parsing the `Pipfile.lock` to extract the existing pins: Haven't looked at that. Depends on how things are structured in `pipenv`. We need a set of `InstallRequirements` that represents the pins in the `Pipfile.lock`.
- Using `LocalRequirementsRepository`: Fairly easy: change our current `PyPIRepository` for a `LocalRequirementsRepository`.

---------------

But, as I'm looking into this, and following @brettdh comments, I realize a few things:
1. The current default `pipenv install` behavior doesn't match the `pipenv lock` behavior. Doing `pipenv install requests` alone won't update `requests` if a new version comes out (much like  straight `pip install`). However, doing `pipenv lock` will update the `Pipfile.lock` with the latest version of `requests` that matches the `Pipfile` specifier, and the dependency constraints.
There's 2 main way to see this:
    - A) The `Pipfile.lock` should stay as stable as possible by default, not changing pins unless required, in order to stay like the current environment, and only change in the event that we change the environment.
    - B) The `Pipfile.lock` should get the newest versions that respect the environment constrains/dependencies in order to freely benefit from the open ranges in the `Pipfile` and lib dependencies, allowing to continuously acquire new compatible versions in your environment. You can then run `pipenv update` to benefit from the fresh lock.

    IMHO, I would align the default behavior, which would be to go with A) by default. Because right now, everytime a lock is performed (i.e. after each installation), new versions can come in, which make the lockfile *drive the update of the environment*, which seems weird. But, this is arguable of course. While in development, I might want to continuously update my requirements to no get stale, like with B), so that should also be easily doable.

2. Even if we use `LocalRequirementsRepository` to avoid updating correct existing pins, and end up aligning the default behaviors, we then need to address the equivalent of `--upgrade` and `--upgrade-strategy` for the locking part. Currently, defining some environment variable (like `PIP_UPGRADE` and `PIP_UPGRADE_STRATEGY`) will affect the `pipenv install` behavior, but will not affect `pipenv lock`, as it doesn't affect the behavior of `pip-tools` (I confirmed that, as I was unsure at first).
Otherwise, there will be no way to update the environment without either deleting the `Pipfile.lock` (feels clunky, and ""all or nothing"") or *requiring* a newer version (I mean doing an explicit `pipenv install requests>2.18.4`, which requires you to *know* that a new version is out, and changes the specifier in the `Pipfile` itself, increasing the lower bound), which is wrong. As the ""original `pip-tools`"" doesn't deffer to `pip` to deal with this (as it's not related that what is currently installed), it offers an option to specify the dependencies to update in the lockfile, and simply remove the pins for these packages (or all) from the existing_pins list, effectively falling back to querying PyPI. I'm not sure how we can match the notion of ""--upgrade-strategy"" with this.

---------------

@techalchemy 
So while I was saying it was fairly easy to just ""align the default behavior"", I now realize that this would cause some major issue with being able to update the packages (as in: just fetch the latest version that match my current constraints).

If there's something unclear, ask away, a lot of editing went on when writing this.

(Dependency resolution is not easy. Good and practical dependency resolution is even worst üòÑ )",False,0
Updating only one locked dependency,techalchemy,268049628,16,339807198,0,"@vphilippon that's exactly what I meant.  Keeping the things that pip installs in sync with the things that pip-tools resolves is non-trivial unless you drive the process backwards, using the resolved lockfile to do the installation.  I'm pretty sure that was why things were designed the way they were.  

> B) The Pipfile.lock should get the newest versions that respect the environment constrains/dependencies in order to freely benefit from the open ranges in the Pipfile and lib dependencies, allowing to continuously acquire new compatible versions in your environment. You can then run pipenv update to benefit from the fresh lock.

This workflow can possibly work with the current configuration.  You can use `pipenv lock` to generate a lockfile, but `pipenv update` will reinstall the whole environment.  I'm pretty sure we can use one of our various output formats to resolve the dependency graph (we already have a json format as you know) and only reinstall things that don't align to the lockfile.  This might be more sensible, but I would be curious about the input of @nateprewitt or @erinxocon before making a decision ",False,0
Updating only one locked dependency,brettdh,268049628,17,339954048,0,"@vphilippon Totally agree that A and B are desirable workflows in different situations. Some of your phrasing around B confused me a bit, though, seeming to say that `pipenv lock` might result in a lockfile that doesn't actually match the environment - I particularly heard this in that one would need to ""run `pipenv update` to benefit from the fresh lock"" - as if the lock is ""ahead"" of the environment rather than matching it.

Regardless of whether you are in an A workflow or a B workflow, a few things seem constant to me, and I think this squares with what @techalchemy is saying as well:
* The result of `pipenv lock` should always be a lockfile that matches the environment.
* The result of `pipenv install` should always be an environment that matches the lockfile.

I'm ignoring implementation details, but that's kind of the baseline behavior I expect from a package manager with a lockfile feature.

Running `pipenv update` periodically allows you to stay in B mode as long as you want everything to be fresh, and having the ability to `pipenv install --upgrade requests` would allow specific updates of one package and its dependencies, without affecting packages that don't need to be upgraded unnecessarily.

Am I missing any use cases? I can think of optimizations for B - e.g. a flag or env var that tells it to always update eagerly - but I think that covers the basics. I also know I'm retreading ground you've already covered; it's just helpful for me to make sure I understand what you're talking about. :)",False,0
Updating only one locked dependency,techalchemy,268049628,18,340098804,0,"> Some of your phrasing around B confused me a bit, though, seeming to say that pipenv lock might result in a lockfile that doesn't actually match the environment

@brettdh this is correct -- the `pip-tools` resolver we use to generate `Pipfile.lock` doesn't ask the virtualenv for a list of which packages have been installed.  Instead, it compiles a list of packages that meet the criteria specified in the list of pins from the `Pipfile`.  Because the resolver itself runs using the system or outer python / pipenv / pip-tools install, we are doing some supreme fuckery to convince it to resolve packages with the same version of python used in the virtualenv.  The assumption would be that `pip install` would resolve things similarly, but that isn't always the case, although even I'm not 100% sure about that.  But yes, `pipenv lock` is not generated based on the virtualenv, it is generated based on the `Pipfile`.  It is a dependency resolution lockfile, not an environment state pin.",False,0
Updating only one locked dependency,ncoghlan,268049628,19,344127642,0,"As a potential resolution to this: something that pip itself currently supports, but `pip-compile` doesn't, is the notion of a constraints file.

A constraints file differs from a requirements file, in that it says ""*If* this component is installed, then it *must* meet this version constraint"". However, if a particular package in the constraints file doesn't show up in the dependency tree anywhere, it doesn't get added to the set of packages to be installed.

This is the feature that's currently missing from `pipenv`, as the desired inputs to the `Pipfile.lock` generation are:

1. The updated `Pipfile` contents as a new requirements input file
2. The full set of existing dependencies from `Pipfile.lock` as a constraints file, excluding the packages specifically named in the current command

Constraints file support at the pip-tools resolver level would then be enough for `pipenv` to support a mode where attempted implicit upgrades of dependencies would fail as a constraint violation, allowing the user to decide whether or not they wanted to add that package to the set being updated.",False,0
Updating only one locked dependency,kennethreitz,268049628,20,346204439,0,"currently not supported, thanks for the feedback",False,0
Updating only one locked dependency,taion,268049628,21,346209021,0,"@kennethreitz 

Do you mean:

1. This behavior should be changed, but it's not currently a priority,
2. This behavior should be added as something optional, but it's not currently a priority, or
3. This behavior should not be added?

This is a sufficient inconvenience given the inconsistency with how other similar locking package managers work that it would be good to keep this open as a solicitation for PRs.

If instead it's (3), and this will not be added, then I think a number of us on the issue will need to adjust our plans for our choice of Python package management tools.",False,0
Updating only one locked dependency,kennethreitz,268049628,22,346209452,0,"I mean that this is currently not supported, and I appreciate the feedback.",False,0
Updating only one locked dependency,taion,268049628,23,346209683,0,I understand that it's not supported. Are you also saying that you would not accept PRs either changing this behavior or adding this as an option?,False,0
Updating only one locked dependency,kennethreitz,268049628,24,346209785,0,I have no idea.,False,0
Updating only one locked dependency,brettdh,268049628,25,347191873,0,"@k4nar still interested in doing a PR for this? Specifically, something like `pipenv install --only <dep-to-update` which prevents unrelated deps from being updated. Since @kennethreitz seems uninterested in discussing further, it seems to me that that's the only way to find out whether that behavior addition/change could be acceptable (and, by extension, whether folks like @taion and I can continue using pipenv).",False,0
Updating only one locked dependency,k4nar,268049628,26,347213486,0,"I'm interested but I'm not sure to know how would be the best way to implement this. There are a lot of components in action (pip, pip-tools, pipfile, pipenv‚Ä¶) and probably a lot of possible solutions.",False,0
Updating only one locked dependency,taion,268049628,27,347299006,0,"Per https://github.com/kennethreitz/pipenv/issues/966#issuecomment-339707418, it should be relatively straightforward. That dep resolution logic is largely just from pip-tools. I was planning on submitting a PR, but I can't justify spending the work if we're not willing to talk about how we want the API to look before we spend time writing code.

I'm currently looking at taking an alternative approach ‚Äì as Pipfile is a standard, interactions with it don't need to go through pipenv, and I'd like to work around some of the other odd semantics here like wiping existing virtualenvs per https://github.com/kennethreitz/pipenv/issues/997.",False,0
Updating only one locked dependency,rfleschenberg,268049628,28,366333367,0,"Sorry to comment on a closed issue, but I'd like to point out that, to my understanding, using pipenv in my projects currently requires a workflow like this:

```
pipenv install foo
vim Pipfile.lock  # Manually remove all the unwanted updates
git add && git commit && git push
```

I find it really annoying having to communicate this to my team members. The alternative seems to be to pin everything to exact versions in `Pipfile`, but that defeats much of the purpose of using pipenv in the first place.

IIUC, this behavior is the equivalent of `apt` performing an implicit `apt dist-upgrade` whenever you run `apt install foo`.

This is made worse by the fact that `pipenv install` updates stuff in `Pipfile.lock`, but does not install the updates into the local virtualenv. If the developer does not carefully examine the diff of `Pipfile.lock`, they are still using the older versions locally, but once they share the code, all other environments see the surprising updates. People have a tendency to ignore the diff of `Pipfile.lock` because it's considered an auto-generated file.

I am strongly convinced that ""update everything to the latest version allowed by `Pipfile`"" should be an explicitly requested operation that is separate from ""install foo"".",False,0
Updating only one locked dependency,kennethreitz,268049628,29,368236234,0,should be fixed in master,False,0
Updating only one locked dependency,marius92mc,268049628,30,374521528,0,"The behaviour is still present, I tested it in `pipenv 11.8.3`, @kennethreitz. ",False,0
Updating only one locked dependency,ncoghlan,268049628,31,374554723,0,"@marius92mc The ""fixed in master"" comment is referring to the `--selective-upgrade` and `--keep-outdated` options added in recent releases: https://docs.pipenv.org/#cmdoption-pipenv-install-keep-outdated

That allows folks that need or want more control over exactly when upgrades happen to opt in to that behaviour, while the default behaviour continues to respect [OWASP A9](https://www.owasp.org/index.php/Top_10-2017_A9-Using_Components_with_Known_Vulnerabilities) and push for eager upgrades at every opportunity.",False,0
Updating only one locked dependency,simonpercivall,268049628,32,374559333,0,"@ncoghlan I think one thing that is needed (easy to ask for, not as easy to do) is an FAQ on _how_ those options behave (at least it's still confusing for me).

For instance: Using `--selective-upgrade` and `--keep-outdated` will still cause outdated libraries in the `Pipfile.lock` to be updated, if they're not directly related to the ""selected"" package to be updated.",False,0
Updating only one locked dependency,kennethreitz,268049628,33,374561183,0,"It sounds like there may be implementation bugs, then. ",False,0
Updating only one locked dependency,kennethreitz,268049628,34,374561261,0,"They are intended to leave the pipfile.lock as-is, except for the new change. ",False,0
Updating only one locked dependency,simonpercivall,268049628,35,374565042,0,Let me know if it's helpful to provide a test Pipfile+.lock.,False,0
Updating only one locked dependency,kennethreitz,268049628,36,374565268,0,I think you've provided enough information for us to investigate. I'll try to do that now.,False,0
Updating only one locked dependency,kennethreitz,268049628,37,374565352,0,"Actually, your pipfile/lock would be great, if it contains outdated results. ",False,0
Updating only one locked dependency,marius92mc,268049628,38,374569759,0,"@ncoghlan, thank you for providing the details. 
I tried again with your mentioned options and the result seems to be the same, it still updates the other packages as well, changing them in the `Pipfile.lock` file. ",False,0
Updating only one locked dependency,simonpercivall,268049628,39,374577149,0,@kennethreitz https://github.com/simonpercivall/pipenv-selective-upgrade-test,False,0
Updating only one locked dependency,marius92mc,268049628,40,375683022,0,"There are any updates about this issue, @kennethreitz? 
",False,0
Updating only one locked dependency,techalchemy,268049628,41,375953797,0,"Sorry for the slow answers on this. We haven‚Äôt nailed down the root cause for the regression here yet (I know I personally have been handling a data center migration this weekend so I‚Äôve been kinda slow) but we will get this sorted in the next few days. 

Contributions welcome as always!",False,0
Updating only one locked dependency,wichert,268049628,42,379626266,0,"I think there is a missing use case that can use this same change: when I am developing an application I often need to upgrade a single dependency's version. The steps I would like to follow are:

1. Update the version restriction for the dependency in setup.py
2. Run either `pipenv lock --selective-upgrade ; pipenv sync`  or `pipenv install --selective-upgrade ""-e .""`

",False,0
Updating only one locked dependency,ncoghlan,268049628,43,379703418,0,"@wichert If `Pipfile` has been edited in a way that increases the minimum required version beyond what's in the current lock file, then `--keep-outdated` should already cover what you need. `--selective-upgrade` is for the case where `Pipfile` *hasn't* changed, but you want to update to a new pinned version anyway.",False,0
Updating only one locked dependency,wichert,268049628,44,379799190,0,"@ncoghlan `Pipfile` has not changed in this scenario, only `setup.py` by changing the minimum version requirement for a dependency, typically to something more recent and currently in `Pipfile.lock`.",False,0
Updating only one locked dependency,techalchemy,268049628,45,379806969,0,@wichert pipenv doesn't capture changes to your `setup.py` automatically because it isn't setuptools.  You have to run `pipenv lock` if you want that to happen.,False,0
Updating only one locked dependency,benkuhn,268049628,46,408138832,0,"What's the current status of this? On March 25th someone said they thought implementation issues would be resolved ""in the next couple days"", and other bug reports have been closed due to being tracked here; but as of 2018.7.1 I still see the bug reported by Simon Percivall (indirect dependencies are always updated) and that bug hasn't been discussed since the original report. Is the problem still being tracked?

(I'm currently living in a second-tier city in Senegal so my Internet is terrible and it would be a game changer not to blow my data cap on updating indirect dependencies if possible :P )

PS: Thanks for making Pipenv, it's awesome <3",False,Impatience
Updating only one locked dependency,techalchemy,268049628,47,408140890,0,Yes for sure. We are rewriting the resolver to support this right now. Whether it lands in this release or next release remains to be seen,False,0
Updating only one locked dependency,uranusjr,268049628,48,408161811,0,"I‚Äôm not that confident with my coding skill to estimate when the resolver would land :p Seriously, this is a completely volunteer project, and we don‚Äôt have a deadline mechanism as you would in commercial settings (we don‚Äôt even have a boss or a project manager or whatever you have in your company that decides when a thing needs to be done). If you want a thing to be done in a timeframe you desire, you need to do it yourself, or at least provide real motivation for others to do it.",False,Bitter frustration
Updating only one locked dependency,brettdh,268049628,49,408175329,0,"@uranusjr FWIW, I didn't see any demands for expediency in @benkuhn 's comment above - just a question about where things are at; i.e. what work has been done, so that outside observers can make their own estimates/decisions.

I understand that pipenv is a volunteer project and that non-contributors cannot ask for a thing to be done by a date without signing up to make it happen. I do wonder, whether there is room for more transparency in the project's development process, or if I'm just not looking in the right places. Usually the answer is either ""if the issue hasn't been updated, there's been no movement"" or ""look at this WIP pull request,"" but this issue in particular seems to have triggered a much larger effort, so the dots can be difficult to connect for those not directly involved.

As always, much thanks to you and everyone who gives their valuable time towards the improvement of pipenv. üëè ",False,0
Updating only one locked dependency,techalchemy,268049628,50,408325763,0,"For sure, this one doesn‚Äôt have activity or a work in progress PR because it is a lot more complicated than that. We are talking internally mostly about how we even want to structure this with respect to the larger project, and working iteratively to establish an approach that might even begin to work properly. Once we can sort that out we can build resolution logic. 

In the meantime the resolver stack in pipenv is super convoluted and I wouldn‚Äôt be comfortable asking people to invest too much effort trying to untangle it for this purpose. Even the simplest use case here will take a significant refactor. We‚Äôd be happy to review / discuss any proposed refactor if someone is interested in helping tackle this, but the two things are tightly coupled. 

If someone has expertise in dependency resolution and sat solving we would certainly be interested in input but there just isn‚Äôt a single concrete idea yet. We‚Äôve been through several iterations that we never planned to carry forward as more than proof of concept. Not all code becomes a PR, and not all code organization decisions happen on the issue tracker. Sometimes we chat synchronously and propose and scrap ideas in real time. ",False,0
Updating only one locked dependency,alecbz,268049628,51,408420493,0,"Something I was _going_ to suggest as an alternative workflow that might address this is making it easy to pin to a specific version in the _Pipfile_ when installing.

I think it's slightly surprising but not completely unreasonable that pipenv interprets `foo = ""*""` to mean ""I just need to make sure _some_ version of foo is installed, the user doesn't care which"". To that end, having something like `pipenv install --pin foo` which results in `foo = ""==1.2.3""` instead of `foo = ""*""` in the Pipfile (where 1.2.3 is the current latest version of foo) seems like it might help.

The issue with this though is that the behavior of a lot of packages can change a lot based on their dependencies (e.g., the same version of pylint can do totally different things depending on what version of astroid it's using), and packages don't pin their own deps exactly. So I don't think this actually gets anyone very far. :/",False,0
Updating only one locked dependency,uranusjr,268049628,52,408494095,0,"(Just realised I have been commenting to the wrong issue. Sorry for the mess up, please ignore me) üòû ",False,0
Updating only one locked dependency,l0b0,268049628,53,413027692,0,"An actual use case that I've struggled with for some hours now: I want to measure test coverage in a Django 2.0 project. Even `pipenv install --keep-outdated --selective-upgrade --dev coverage` insists on updating the non-dev Django package to version 2.1, which because of breakage elsewhere I absolutely cannot use yet. **There really *must* be a way to change the set of installed packages without upgrading completely unrelated packages to possibly breaking versions.** The possibility of breakage in the latest version will always exist.

I'll try [@rfleschenberg's workaround](https://github.com/pypa/pipenv/issues/966#issuecomment-366333367), but I don't know whether having a presumably incorrect `_meta hash` property will break anything.",False,0
Updating only one locked dependency,alecbz,268049628,54,413055372,0,"@l0b0 If your application genuinely cannot handle a particular version of Django, I think it makes sense to state that restriction in your Pipfile?",False,0
Updating only one locked dependency,wichert,268049628,55,413118270,0,@AlecBenzer That sounds like something for setup.py to me.,False,0
Updating only one locked dependency,alecbz,268049628,56,413201995,0,"@wichert That might make sense too (I'm actually not totally clear on in what circumstances you'd want to have both a setup.py and a Pipfile), but if you have a line in your Pipfile that says:

```
Django = ""*""
```

you're telling pipenv that you want it to install _any_ version of Django. If what you really want it to do is install 2.0 or lower, tell it that instead:

```
Django = ""<=2.0.0""
```

While in this particular case pipenv is upgrading Django for no real reason, it could be that somewhere down the line you try to install a package that requires Django >2.0.0, at which point pipenv will happily install it if you haven't told it that you need <=2.0.0.",False,0
Updating only one locked dependency,brettdh,268049628,57,413219231,0,"> If what you really want it to do is install 2.0 or lower, tell it that instead

@AlecBenzer on reflection, it now occurs to me that this is what npm/yarn do by default when you install a package; they find the latest major.minor version and specify `^major.minor.0` in package.json, which prevents unexpected major version upgrades, even when an upgrade-to-latest is explicitly requested. I wonder if Pipenv should do the same - but that would be a separate issue. 

Of course, their lock file also prevents accidental upgrades of even minor and patch versions, which is what's being requested here.",False,0
Updating only one locked dependency,brettdh,268049628,58,413221516,0,"I think it's been discussed above and elsewhere, but there is a tension/tradeoff in the design space between npm/yarn and pipenv. Any package manager ostensibly has these goals, with some relative priority:

- Make it easy to install and upgrade packages
- Make it hard to accidentally break your app with an errant dependency upgrade

The trouble with pinning an exact version in the Pipfile is that it's then harder to upgrade packages; this is why [pip-tools](https://github.com/jazzband/pip-tools/) exists (though it's for requirements.txt).",False,0
Updating only one locked dependency,tilgovi,268049628,59,413262709,0,"The `--keep-outdated` flag does not seem to be working as intended, as was stated when the issue was re-opened. Whether that behavior should or should not be the default and how it aligns with other package managers is not really the central issue here. Let's fix the thing first.",False,0
Updating only one locked dependency,alecbz,268049628,60,413273713,0,"@brettdh  

> on reflection, it now occurs to me that this is what npm/yarn do by default when you install a package; they find the latest major.minor version and specify ^major.minor.0 in package.json, which prevents unexpected major version upgrades, even when an upgrade-to-latest is explicitly requested. I wonder if Pipenv should do the same - but that would be a separate issue.

Yeah that's along the lines of what I was trying to suggest in https://github.com/pypa/pipenv/issues/966#issuecomment-408420493",False,0
Updating only one locked dependency,benkuhn,268049628,61,414300377,0,"Really excited to hear this is being worked on!

In the mean time, does anyone have a suggested workaround that's less laborious and error-prone than running `pipenv lock` and hand-reverting the resulting lockfile changes that I don't want to apply?",False,0
Updating only one locked dependency,wichert,268049628,62,414300856,0,@benkuhn Not that I know off - I do the same lock & revert dance all the time.,False,0
Updating only one locked dependency,benkuhn,268049628,63,414321311,0,"Ah ok, you can at least sometimes avoid hand-reverting:

1. `pipenv lock`
2. `git commit -m ""FIXME: revert""`
3. `pipenv install packagename`
4. `git commit -m 'Add dependency on packagename'`
5. `git rebase -i`
6. Drop the `FIXME: revert` commit

Unfortunately it's still possible to create an inconsistent `Pipfile.lock` if your `Pipfile.lock` starts out containing a version of a package that's too old to satisfy the requirements of `packagename`, but perhaps pipenv will complain about this if it happens?",False,0
Updating only one locked dependency,jhrmnn,268049628,64,414837344,0,"`--keep-outdated` seems to *systematically* keep outdated only the explicit dependencies that are specified (unpinned) in Pipfile, while all the implicit dependencies are updated. ",False,0
Updating only one locked dependency,max-arnold,268049628,65,416588930,0,"Am I correct that it is not possible to update/install single dependency using `pipenv==2018.7.1` without updating other dependencies?  I tried different combinations of `--selective-upgrade` and `--keep-outdated` with no success.

Editing Pipfile.lock manually is no fun...",False,0
Updating only one locked dependency,mrsarm,268049628,66,416689831,0,"Same than @max-arnold , it's my first day using the tool in an existing project, and I have to say **I'm really disappointed**, before I started to use it, I checked the doc site and the video demo, it looked impressive to me, and now this: in real project, work with `pip` or `pipenv` is almost the same, i don't see the point, like many other said in the thread, if I have a lock file, why you are updating my other dependencies if there is no need to update them.

Of course, ### if the update is mandatory, it's OK to update all the necessary dependencies, but just those, not all the outdated instead.

Also the options `--selective-upgrade` and `--keep-outdated` are not clear for what are useful for, there is another issue highlighting this here #1554 , and nobody is able to respond what these options do, incredible.

But my **major disappointing** is why this package was **recommended by the Python official documentation** itself, these recommendations should be more careful conducted, I know this can be a great project in the feature, have a lot of potential, but simple things like this (we are not talking about a bug or a minor feature), make this project not eligible for production environments, but suddenly because it was recommended by the Python docs, everybody are trying to use it, instead of looking for other tools that maybe work better, or just stick with `pip`, that doesn't solve also these issues, but at least it's very minimalist and it's mostly included in any environment (does not add extra dependencies).",False,Bitter frustration
Updating only one locked dependency,uranusjr,268049628,67,416696024,0,"@mrsarm Thank you for your opinion. Sorry things don‚Äôt work for you. I don‚Äôt understand where the disappointment comes from, however. Nobody is forcing Pipenv on anyone; if it doesn‚Äôt work for you, don‚Äôt use it. That is how recommendations work.

Your rant also has nothing particularly related to this issue. I understand it requires a little self-control to not dumping trash on people when things don‚Äôt go your way, but please show some respect, and refrain from doing so.",False,0
Updating only one locked dependency,mrsarm,268049628,68,416706018,0,"@uranusjr it's not trash, it's an opinion, and some times it's not an option, like my case, where somebody chose pipenv to create a project where I started to work now, and I have to deal with this.

But things get worst just now, and what I going to say it's not an opinion, it's a fact.

After trying to add one dependency that just I dismissed to avoid to deal with this issue (because it's a dev dependency, so I created a second environment with `pip` and the old `requirements-dev.txt` approach, just with that tool), I needed to add another dependency.

The new dependency is PyYAML, let say the latest version. If you install it in any new environment with `pip`, you will see that the library does not add any dependency, so only PyYAML is installed, is that simple in these cases with Pip. But adding the dependency with Pipenv (because a project that I didn't create is managed with Pipenv) the same issue happened, despite PyYAML doesn't have any dependency, and it's not previously installed in the project (an older version), `pipenv` updates all my dependencies in the lock file and the virtual environment, but I don't want to update the others dependencies, I just one to add a single new module without any dependency.

So the conclusion (and again an opinion, not a fact like pipenv broke all my dependencies) it's that Pipenv instead of help me to deal with the dependencies management, it turn it into hell.",False,Bitter frustration
Updating only one locked dependency,dfee,268049628,69,416707846,0,"I've followed this thread for months, and I think any real project will ultimately stumble upon this issue, because the behavior is unexpected, counter-intuitive, and yes: dangerous.

About a month ago I tried out a more-comprehensive alternative to `pipenv`, [`poetry`](https://github.com/sdispater/poetry); it solved the problems _I_ needed to solve:
1) managing one set of dependencies (setup.py, setup.cfg, pip, and pipfile -> pyproject.toml)
2) future oriented, backwards-compatible (again [pyproject.toml](https://www.python.org/dev/peps/pep-0518/))
3) fairly un-opinionated ([no i'm really not asking to install `redis`](https://github.com/pypa/pipenv/issues/1174))
4) and the solution to the classic Pipenv problem: ""Also, you have to explicitly tell it [pipenv] to not update the locked packages when you install new ones. This should be the default."" [[1](https://github.com/sdispater/poetry#what-about-pipenv)] [[2](https://github.com/pypa/pipenv/issues/966#issuecomment-339117289)]

I weighed sharing these thoughts on the `pipenv` issue, but as @uranusjr said, ""no one is forcing Pipenv on anyone"", and I'm not forcing Poetry. I like it, it works well, and it solves my problems, but I'm just sharing an alternative, more-comprehensive solution to the problem I was having. Just take all that as my 2¬¢.

* as a disclaimer, *I am not* a member of the Poetry team or affiliated with them.

p.s. I think the concern about Pipenv being the ""official"" solution is due to it's [first-class integrations](https://github.com/pypa/pipenv/blob/master/docs/advanced.rst#-community-integrations) ‚Äì¬†something that you, @uranusjr, might see it as a simple recommendation ‚Äì the industry at large is taking it as the ""blessed approach going forward"". Frankly, that recommendation is more authoritative in the community than certain PEPs that have been around for more than a year.",False,0
Updating only one locked dependency,techalchemy,268049628,70,416773434,0,"Nobody is forcing you to participate in our issue tracker; if you don‚Äôt have a productive comment please find a forum that is not for triaging issues.

For users who are interested in trying the alternative resolver @uranusjr and myself have been implementing for several weeks now, please try out https://github.com/sarugaku/passa which will generate compatible lockfiles. Poetry does a lot of different things, but it also has limitations and issues itself, and we have a design philosophy disagreement about scope. 

This is a project we manage in our spare time; if you want to see something fixed or you have a better approach, we are happy to accept contributions. If you are here to simply tell us we ruined your day and your project, I will ask you only once to see yourself out. 

We have not forgotten or ignored this issue, we have a full implementation of a fix in the resolver linked above. Have patience, be courteous, or find somewhere else to talk. To those who have been waiting patiently for a fix, please do try the resolver mentioned above‚Äî we are eager to see if it meets your needs. It implements proper backtracking and resolution and shouldn‚Äôt handle this upgrade strategy 

In the shorter term I think we can get a band aid for this into pipenv if we don‚Äôt wind up cutting over first. ",False,0
Updating only one locked dependency,techalchemy,268049628,71,416774098,0,"@dfee I am not really sure that blurring lines between applications and libraries is the correct answer to dependency management, so I don‚Äôt see poetry‚Äôs approach as an advantage. I wasn‚Äôt involved in whatever your issue was with the recommendation engine, but we took that out some time ago...",False,0
Updating only one locked dependency,sdispater,268049628,72,416780604,0,"@techalchemy

> I am not really sure that blurring lines between applications and libraries is the correct answer to dependency management, so I don‚Äôt see poetry‚Äôs approach as an advantage.

Why though? I never understood this idea that you should manage the dependencies of  a library and an application differently. The only difference between the two is the lock file which is needed for an application to ensure a reproducible environment. Other than that it's the same thing. This is the standard in most other languages and Python seems the exception here for some reason and this is bad from a user experience standpoint since this is making things more complex than they should be.

> it also has limitations and issues itself

Which ones? I am really curious about the issues or limitations you encountered while using Poetry.",False,0
Updating only one locked dependency,mrsarm,268049628,73,416780972,0,"My apologies to bean so rude. Now reading my comments I realize that despite the info I provided and some of my options are still valid (IMHO), it's wasn't appropriated the way I wrote what I wanted to say.

I understand that the issue tracker is most a place where to discuss bugs and improvements, and discuss whether this is a bug or an error by design is not clear in the thread, but again my apologies.

I thing there are two strong topics here:
- Should pipenv update all your outdated dependencies where you are trying just to install a new dependency: the ones that are not needed to update because the new package / version we are trying to install can works with the existent dependencies, and even the ones that aren't dependencies of the new package we are trying to install? Maybe this is out of scope of this ticket, but it's a really important topic to discuss.
- Do one of these parameters `--keep-outdated` `--selective-upgrade` allow us to avoid these behaviour? It's not clear what these options do, there is a lack of documentation about them, and even in the related issue (#1554) nobody is answering that.

In case it's a bug in on one of these params `--keep-outdated` `--selective-upgrade`, I still thinking that do not set whatever param solves the unnecessary update of the dependencies as default it's a really bad idea.

To compare with a similar scenario, imagine that you execute `apt-get install vim` to just install the `vim` tool in your system (and the necessary vim's dependencies or updates if apply), but imagine also that in this situation apt updates all the other dependencies of your system: python, the QT system, the Linux kernel... and so on. It's not that apt shouldn't allow us to updates other dependencies, but there is a clear command to do that: `apt-get upgrade`, while `apt-get install PACKAGE` just install / update PACKAGE and it's dependencies.",False,0
Updating only one locked dependency,techalchemy,268049628,74,416784883,0,"@sdispater the distinction is at the heart of every disagreement we've ever had and it's incredibly subtle but I'd point you at https://caremad.io/posts/2013/07/setup-vs-requirement/ or a good article for the elixir use case: http://blog.plataformatec.com.br/2016/07/understanding-deps-and-applications-in-your-mixfile/

`pyproject.toml` isn't really supported for library definition metadata -- and not at all by any version of pip that doesn't implement peps 517 and 518 (both of which are still having implementation details worked out) as an authoritative library declaration file.  `setup.cfg` exists for that purpose (the actual successor to `setup.py` ) and IMHO both of those should really be supported.  A library is published and intended for consumption with abstract dependencies so that they can play nice in the sandbox with others; applications are usually large, complex beasts with sometimes hundreds of direct dependencies.   So one of our main divergences is that when we design and build our tooling, we take this into account also

@mrsarm For your first question, the update behavior was intentional (and was discussed extensively at the time, /cc @ncoghlan and related to OWASP security concerns).  On the second point, the behavior is currently not properly implemented which is why the issue is still opened, which led us to rewriting the backing resolver behind pipenv, which I mentioned above.  It simply didn't support this behavior.  `--selective-upgrade` is supposed to selectively upgrade only things that are dependencies of the new package, while `--keep-outdated` would hold back anything that satisfied the dependencies required by a new package.  Slightly different, but I am fairly sure neither works correctly right now.  ",False,0
Updating only one locked dependency,mmerickel,268049628,75,416790015,0,"> pyproject.toml isn't really supported for library definition metadata -- and not at all by any version of pip that doesn't implement peps 517 and 518 (both of which are still having implementation details worked out) as an authoritative library declaration file. setup.cfg exists for that purpose (the actual successor to setup.py ) and IMHO both of those should really be supported.

Well this is certainly off topic but it's an important discussion so I can't help myself.

There is actually no standard around `setup.cfg` right now other than the conventions established by distutils and setuptools. `pyproject.toml` is absolutely for library metadata as the successor to `setup.py` or the community would have placed build requirements in `setup.cfg` instead. 

`pyproject.toml` describes how to build a project (PEP 518), and part of building is describing metadata. I'm NOT saying that `pyproject.toml` needs a standard location for this metadata, but PEP 518 uses this file to install a build tool and from there it's very reasonable to expect that the build tool will use declarative configuration from somewhere else in the file to determine how to build the project.

Anyway, going back to pipenv vs poetry - there seems to be some idea floating around that applications don't need certain features that libraries get, like entry points, and this is just incorrect. It should be straightforward for an application to be a python package.

The only true difference between an application and a library in my experience with python and with other ecosystems is whether you're using a lockfile or not. Of course there's a third case where you really just want a `requirements.txt` or `Pipfile` and no actual code and that seems to be all that pipenv has focused on so far (`pipenv install -e .` falls into this category as pipenv is still afraid to try and support the package metadata). Unfortunately, while the design of pipenv is cleaner with this approach, it's also way less useful for most applications because PEP 518 decided to punt on how to install projects into editable mode so in order to continue using pipenv we will be stuck on setuptools quite a while longer as you cannot use `pyproject.toml` to switch away from setuptools and still use `pipenv install -e .`.",False,0
Updating only one locked dependency,techalchemy,268049628,76,416813069,0,"> There is actually no standard around setup.cfg right now other than the conventions established by distutils and setuptools. pyproject.toml is absolutely for library metadata as the successor to setup.py or the community would have placed build requirements in setup.cfg instead.

Distutils is part of the standard library and setuptools is installed with pip now, so saying that there is no standard is a bit silly.  Not to mention it uses the standard outlined in pep 345 for metadata, among others, and can also be used to specify build requirements.

> the community would have placed build requirements in setup.cfg instead.

Do you mean the pep authors?  You can ask them why they made their decision, they outline it all in the pep.

> pyproject.toml describes how to build a project (PEP 518), and part of building is describing metadata. I'm NOT saying that pyproject.toml needs a standard location for this metadata, but PEP 518 uses this file to install a build tool and from there it's very reasonable to expect that the build tool will use declarative configuration from somewhere else in the file to determine how to build the project.

This came up on the mailing list recently -- nothing anywhere has declared a standard around `pyproject.toml` other than that it will be used to declare build system requirements. Anything else is an assumption; you can call that ""library definition metadata"", but it isn't.  Try only defining a build system with no additional information about your project (i.e. no pep-345 compliant metadata) and upload it to pypi and let me know how that goes.

> Anyway, going back to pipenv vs poetry - there seems to be some idea floating around that applications don't need certain features that libraries get, like entry points, and this is just incorrect. It should be straightforward for an application to be a python package.

Who is saying that applications don't require entry points?  Pipenv has an entire construct to handle this.

> so in order to continue using pipenv we will be stuck on setuptools quite a while longer as you cannot use pyproject.toml to switch away from setuptools and still use pipenv install -e .

Not following here... we are not going to leave pip vendored at version 10 forever, I've literally been describing our new resolver, and the actual installer just falls back to pip directly... how does this prevent people from using editable installs?",False,0
Updating only one locked dependency,digitalresistor,268049628,77,416818620,0,"> This came up on the mailing list recently -- nothing anywhere has declared a standard around `pyproject.toml`

That's correct, it is not a ""standard"", yet in that same thread recognise that by calling it `pyproject.toml` they likely asked for people to use this file for other project related settings/config.

So by the same logic you invoked here:

> Distutils is part of the standard library and setuptools is installed with pip now, so saying that there is no standard is a bit silly.

`pyproject.toml` is a standard, and the community has adopted it as the standard location to place information related to the build system, and other parts of a Python project.",False,0
Updating only one locked dependency,digitalresistor,268049628,78,416818959,0,"> Not following here... we are not going to leave pip vendored at version 10 forever, I've literally been describing our new resolver, and the actual installer just falls back to pip directly... how does this prevent people from using editable installs?

PEP 517 punted on editable installs... which means there is no standard way to install a project in editable mode if you are not using setup tools (which has a concept known as develop mode which installs the project in editable mode).",False,0
Updating only one locked dependency,mmerickel,268049628,79,416819173,0,"> Distutils is part of the standard library and setuptools is installed with pip now, so saying that there is no standard is a bit silly. Not to mention it uses the standard outlined in pep 345 for metadata, among others, and can also be used to specify build requirements.

Yes, the build system is expected to output the PKG-INFO file described in PEP 345. This is a transfer format that goes in an sdist or wheel and is generated from a setup.py/setup.cfg, it is not a replacement as such for the user-facing metadata. PEP 518's usage of `pyproject.toml` is about supporting alternatives to distutils/setuptools as a build system, no one is trying to replace the sdist/wheel formats right now. Those replacement build systems need a place to put their metadata and fortunately PEP 517 reserved the `tool.` namespace for these systems to do so. It's not an assumption - **both flit and poetry have adopted this namespace for ""library definition metadata"".**

> Try only defining a build system with no additional information about your project (i.e. no pep-345 compliant metadata) and upload it to pypi and let me know how that goes.

How constructive.

> Who is saying that applications don't require entry points? Pipenv has an entire construct to handle this.

Where is this construct? I cannot even find the word ""entry"" on any page of the pipenv documentation at https://pipenv.readthedocs.io/en/latest/ so ""an entire construct"" sounds pretty far fetched? If you mean editable installs then we have reached the point I was making above - with pipenv deciding to couple itself to `pipenv install -e .` as the only way to hook into and develop an application as a package, for the foreseeable future pipenv's support here is coupled to setuptools. I think the entire controversy boils down to this point really and people (certainly me) are frustrated that we can now define libraries that don't use setuptools but can't develop on them with pipenv. To be perfectly clear this isn't strictly pipenv's fault (PEP 518 decided to punt on editable installs), but its refusal to acknowledge the issue has been frustrating in the discourse as poetry provides an alternative that does handle this issue in a way that's compliant with the `pyproject.toml` format. Pipenv keeps saying that poetry makes bad decisions but does not actually attempt to provide a path forward.",False,Irony
Updating only one locked dependency,techalchemy,268049628,80,416827637,0,"https://pipenv.readthedocs.io/en/latest/advanced/#custom-script-shortcuts

Please read the documentation.",False,0
Updating only one locked dependency,techalchemy,268049628,81,416829952,0,"@bertjwregeer:

> pyproject.toml is a standard, and the community has adopted it as the standard location to place information related to the build system, and other parts of a Python project.

Great, and we are happy to accommodate sdists and wheels built using this system and until there is a standard for editable installs we will continue to pursue using pip to build sdists and wheels and handle dependency resolution that way.  Please read my responses in full.  The authors and maintainers of pip, of the peps in question, and myself and @uranusjr are pretty well versed on the differences between editable installs and the implications of building them under the constraints of pep 517 and 518.  So far All I'm seeing is that the peps in question didn't specifically address how to build them because they leave it up to the tooling, which for some reason everyone thinks means pipenv will never be able to use anything but setuptools? 

I've said already this is not correct.  If you are actually interested in the implementation and having a productive conversation I'm happy to have that.  If you are simply here to say that we don't know what we're doing, but not interested in first learning what it is we are doing, this is your only warning. We are volunteers with limited time and I am practicing a 0 tolerance policy for toxic engagements.  I do not pretend my work is perfect and I don't pretend that pipenv is perfect. I will be happy to contribute my time and effort to these kinds of discussions; in exchange I ask that they be kept respectful, that they stick to facts, and that those who participate also be willing to learn, listen, and hear me out.  If you are here just to soapbox you will have to find another platform; this is an issue tracker.  I will moderate it as necessary.

This discussion is **wildly off topic**. If anyone has something constructive to say about the issue at hand, please feel free to continue that discussion.  If anyone has issues or questions about our build system implementations, please open a new issue.  If you have issues with our documentation, we accept many pull requests around documentation and we are aware it needs work. Please defer **all** of that discussion to new issues for those topics.  And please note: the same rules will still apply -- this is not a soapbox, it is an issue tracker.",False,0
Updating only one locked dependency,mmerickel,268049628,82,416835515,1,"> https://pipenv.readthedocs.io/en/latest/advanced/#custom-script-shortcuts
> Please read the documentation.

Entry points are a more general concept than just console scripts and this link is completely erroneous in addressing those concerns. `<soapbox>`Ban away - you're not the only maintainer of large open source projects on here and none of my comments have been a personal attack on you or the project. People commenting here are doing so because they want to use pipenv and appreciate a lot of what it does. My comment was not the first off topic post on this thread, yet is the only one marked. Your snarky comments indicating that you think I don't know what I'm talking about are embarrassing and toxic.",False,Identity attacks/Name-Calling
Makefile: Fix debug builds.,orbea,285132250,1,285132250,0,"## Description

RetroArch builds debug builds by default after commit https://github.com/libretro/RetroArch/commit/ec4b0f90896d9cca2b9eaa0df0e9127b3ca5445d
This is very bad and breaks ./configure && make which would explicitly need `DEBUG=0`.

## Related Issues

Debug support should not be enabled if `DEBUG` is undefined.

## Related Pull Requests

https://github.com/libretro/RetroArch/commit/ec4b0f90896d9cca2b9eaa0df0e9127b3ca5445d

## Reviewers

@twinaphex, @alcaro, @bparker06
",True,0
Makefile: Fix debug builds.,Alcaro,285132250,2,354483782,0,"Release builds are impossible to debug properly, therefore they're the broken ones. Debug builds work fine, they're a bit slower but are not broken by any plausible definition.

Fix your buildbots instead. They're probably already setting a dozen variables, it's easy to add another one.",False,Bitter frustration
Makefile: Fix debug builds.,orbea,285132250,3,354484814,0,"No, I'm not going to fix every distro package that follows correct behavior of not setting `DEBUG` for release builds.

You should just make this script.
```
#!/bin/sh

./configure && make DEBUG=1
```",False,Bitter frustration
Makefile: Fix debug builds.,orbea,285132250,4,354485178,1,"Can you please untag my PR with that ridiculous bs? Changes will not be made because this is the correct behavior and you just broke it, seriously why can't you just accept that you are wrong?",False,Insulting
use the encryptionroot property to load keys,prometheanfire,298311894,1,298311894,0,"This simplifies the logic a bit and gets rid of the main loop that looks
for where the key is located.

Signed-off-by: Matthew Thode <mthode@mthode.org>


Related: https://github.com/zfsonlinux/zfs/pull/7189

### Types of changes
<!--- What types of changes does your code introduce? Put an `x` in all the boxes that apply: -->
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [x] Performance enhancement (non-breaking change which improves efficiency)
- [x] Code cleanup (non-breaking change which makes code smaller or more readable)
- [ ] Breaking change (fix or feature that would cause existing functionality to change)
- [ ] Documentation (a change to man pages or other documentation)

### Checklist:
<!--- Go over all the following points, and put an `x` in all the boxes that apply. -->
<!--- If you're unsure about any of these, don't hesitate to ask. We're here to help! -->
- [x] My code follows the ZFS on Linux code style requirements.
- [x] I have updated the documentation accordingly.
- [x] I have read the **CONTRIBUTING** document.
- [ ] I have added tests to cover my changes.
- [x] All new and existing tests passed.
- [x] All commit messages are properly formatted and contain `Signed-off-by`.
- [ ] Change has been approved by a ZFS on Linux member.
",True,0
use the encryptionroot property to load keys,prometheanfire,298311894,2,366723337,0,@tcaputi @kpande @bunder2015 your review here would be nice :D,False,0
use the encryptionroot property to load keys,tcaputi,298311894,3,366774624,0,"I'm still not an expert in bash or dracut, but the approach looks correct to me.",False,0
use the encryptionroot property to load keys,prometheanfire,298311894,4,366776125,0,"That would be changing one `if` out for another.  I'm not sure what would be gained or fixed doing so either (the logic here is straight forward, which I like).",False,0
use the encryptionroot property to load keys,codecov[bot],298311894,5,366798615,0,"# [Codecov](https://codecov.io/gh/zfsonlinux/zfs/pull/7194?src=pr&el=h1) Report
> Merging [#7194](https://codecov.io/gh/zfsonlinux/zfs/pull/7194?src=pr&el=desc) into [master](https://codecov.io/gh/zfsonlinux/zfs/commit/e921f6508b212c61fcedd0eeb2f9cf9da1abc4d1?src=pr&el=desc) will **decrease** coverage by `0.23%`.
> The diff coverage is `n/a`.

[![Impacted file tree graph](https://codecov.io/gh/zfsonlinux/zfs/pull/7194/graphs/tree.svg?width=650&height=150&src=pr&token=NGfxvvG2io)](https://codecov.io/gh/zfsonlinux/zfs/pull/7194?src=pr&el=tree)

```diff
@@            Coverage Diff             @@
##           master    #7194      +/-   ##
==========================================
- Coverage   76.39%   76.16%   -0.24%     
==========================================
  Files         327      327              
  Lines      103768   103768              
==========================================
- Hits        79278    79037     -241     
- Misses      24490    24731     +241
```

| Flag | Coverage Œî | |
|---|---|---|
| #kernel | `76.16% <√∏> (+0.22%)` | :arrow_up: |
| #user | `65.43% <√∏> (-0.46%)` | :arrow_down: |

------

[Continue to review full report at Codecov](https://codecov.io/gh/zfsonlinux/zfs/pull/7194?src=pr&el=continue).
> **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta)
> `Œî = absolute <relative> (impact)`, `√∏ = not affected`, `? = missing data`
> Powered by [Codecov](https://codecov.io/gh/zfsonlinux/zfs/pull/7194?src=pr&el=footer). Last update [e921f65...e5619df](https://codecov.io/gh/zfsonlinux/zfs/pull/7194?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).
",False,0
use the encryptionroot property to load keys,sempervictus,298311894,6,366845661,0,"@tcaputi: lol, your choice of input to the open source effectively guarantees you're going to be a bootloader and init system expert for the rest of your days.
@kpande: that sounds reasonable, but we need to be careful with all of this. Root context execution of user supplied inputs  seems like it would have pitfalls...",False,0
use the encryptionroot property to load keys,sempervictus,298311894,7,366846467,0,Complexity being roughly analogous to attack surface. This should be a KISS exercise to the degree possible.,False,0
use the encryptionroot property to load keys,prometheanfire,298311894,8,366850553,0,I have not tested the current iteration of the patch,False,0
use the encryptionroot property to load keys,prometheanfire,298311894,9,367075155,0,"so... uh, do I need to resubmit this now, since I can't reopen?",False,0
use the encryptionroot property to load keys,behlendorf,298311894,10,367120872,0,"@prometheanfire unfortunately, Github won't let me reopening this issue either.  But please open a new PR with your updated version.   I'd like to think we can continue discussing the relative merrits of the two proposed solutions and then move forward with one of them.",False,0
use the encryptionroot property to load keys,prometheanfire,298311894,11,367129849,0,"@behlendorf I'm actually fine with @kpande's solution, I think ours now differ by newlines only.  I just wanted to call out the behaviour.",False,0
use the encryptionroot property to load keys,behlendorf,298311894,12,367139373,0,I'm glad to hear it was simply a misunderstanding and there was no harm done.,False,0
use the encryptionroot property to load keys,lxguest,298311894,13,367141638,0,"is not misunderstanding, he randomly ban ppl from IRC, verbally abuse issue reportes, delete comments... i not know why he can moderate the project, he is very toxic in this community, this is not first time, like prometheanfire say is behaviour.",False,Bitter frustration
use the encryptionroot property to load keys,prometheanfire,298311894,14,367142083,0,This is the first time I've had any problem at least.,False,0
use the encryptionroot property to load keys,lxguest,298311894,15,367147796,1,"no better place becaus you ban me from IRC. respond to criticism is lies, everyone who do not think like yourself you consider stupid",False,Bitter frustration
Twig syntax highlighting broken,ericmorand,304422453,1,304422453,0,"Hi,

Jekyll syntax highlighting is broken with Twig. Consider the following code block containing a perfectly valid Twig syntax:

```
{% highlight twig %}
{% raw %}
{% set a = 'b' %}
{% endraw %}
{% endhighlight %}
```

It outputs the following HTML:

```
<code class=""language-twig"" data-lang=""twig"">
    <span class=""cp"">{%</span> 
    <span class=""k"">set</span> 
    <span class=""nv"">a</span> 
    <span class=""err"">=</span>
    <span class=""s1"">'b'</span> 
    <span class=""cp"">%}</span>
</code>
```

Notice the **err** class attributed to the equal sign.

## Steps to reproduce

* Follow the official quick-start guide: https://jekyllrb.com/docs/quickstart/
* Replace the content of the post created by the installation with this:

```
---
layout: post
title:  ""Welcome to Jekyll!""
categories: jekyll update
---

{% highlight twig %}
{% raw %}
{% set a = 'b' %}
{% endraw %}
{% endhighlight %}

```

",True,0
Twig syntax highlighting broken,ashmaroli,304422453,2,372357726,0,"How is the resulting output if you were to use triple-backticks instead?

    ```twig
    {% raw %}
      {% set a = 'b' %}
    {% endraw %}
    ```",False,0
Twig syntax highlighting broken,ericmorand,304422453,3,372361724,0,"@ashmaroli, same.",False,0
Twig syntax highlighting broken,ashmaroli,304422453,4,372362713,0,So you see the issue is not with Jekyll but rather with Rouge that `highlight` and the triple-backticks block uses to highlight code.,False,0
Twig syntax highlighting broken,ericmorand,304422453,5,372374996,0,"No, the problem also happens with the `{% highlight %}` syntax.",False,0
Twig syntax highlighting broken,ashmaroli,304422453,6,372378503,0,"Yes, that's because the `highlight` tag uses `Rouge` for syntax-highlighting by default
https://github.com/jekyll/jekyll/blob/86d86258a8bc912c906776d8f2f9a58b3d376519/lib/jekyll/tags/highlight.rb#L38-L46

If you want to use `pygments` instead of `rouge` as your site's highlighter, add the following to your `_config.yml`:
```yml
highlighter: pygments
```",False,0
Twig syntax highlighting broken,pathawks,304422453,7,372381451,0,This sounds like an issue with https://github.com/jneen/rouge rather than Jekyll. Jekyll has no knowledge of syntax of any language.,False,0
Twig syntax highlighting broken,ericmorand,304422453,8,372383292,0,"@pathawks, why did you close this? It's not fixed and I'm not the one explicitely using Rouge. The maintainers of the project are using a dependency that is buggy, they should take care of this. What do you want me to do? I don't even know what Rouge is!",False,Bitter frustration
Twig syntax highlighting broken,pathawks,304422453,9,372397547,0,"> What do you want me to do? I don't even know what Rouge is!

I‚Äôve provided a link to the repository so that you can open an issue there and explain the problem you are having.

I do not know what ‚ÄúTwig‚Äù is, so it would not make sense for me to be the one to explain what needs changing in Rogue.

There is nothing in Jekyll‚Äôs code that can be changed to fix this issue; the fix will have to come from Rogue.

Here is a link to Rogue‚Äôs Twig lexar: https://github.com/jneen/rouge/blob/master/lib/rouge/lexers/twig.rb",False,0
Twig syntax highlighting broken,ericmorand,304422453,10,372400893,1,"> There is nothing in Jekyll‚Äôs code that can be changed to fix this issue; the fix will have to come from Rogue.

That's not my point. You are the one using Rouge to implement a feature that you advertise explitely on your docs! That's your responsibility to take care of things that don't work as expected in the dependencies of your project. As a consumer of your product, I expect it to work as advertised:

https://jekyllrb.com/docs/templates/#code-snippet-highlighting

You are advertising syntax highlighting, you are supposed to deliver! And if you don't, you are supposed to take care of whatever is needed to have your product work as expected.


",False,Irony
gitkraken stuck in an infinite loop on startup,ghost,305030419,1,305030419,0,"When i startup my gitkraken, it is stuck in an infinite loop while it is opening the repository. I cannot do anything because the opening repo logo keeps popping up, and when the opening repo logo is gone, it comes back up a second later.
This occurs when i open any repository with gitkraken. (i use the latest version of gitkraken)
",True,0
gitkraken stuck in an infinite loop on startup,siprbaum,305030419,2,372956206,0,"Does it work, e.g. if you view the repo with `git log --all` or perhaps `gitk --all`?
If so, I suggest to report this to gitkraken, because it is probably a bug on their site then.",False,0
gitkraken stuck in an infinite loop on startup,pmiossec,305030419,3,372957212,0,"Yes. Not sure it's a problem of git-tfs. 

@GuylianWasHier Are you really sure to know what is git-tfs and that you post your issue in the good project? ",False,Mocking
gitkraken stuck in an infinite loop on startup,thenexxuz-zz,305030419,4,376245264,0,To the best of my knowledge GitKraken does not use this project at all. It looks like this would be used for command line git or other GUI's that utilize the system's installed git (which GitKraken does not use),False,0
gitkraken stuck in an infinite loop on startup,dennisclausen,305030419,5,381406699,0,Same for me. It keeps doing it not matter which repo im trying to open,False,0
gitkraken stuck in an infinite loop on startup,aboone2000,305030419,6,385287978,0,This is also happening for me also. When will you guys have a fix for this.,False,Impatience
gitkraken stuck in an infinite loop on startup,pmiossec,305030419,7,385288571,0,"@aboone2000 They will probably have a fix when you stop to post in the first forum you see and provide a reproduction case in their slack channel. 

And also if you ask them more kindly than what you just did.... ",False,Bitter frustration
gitkraken stuck in an infinite loop on startup,aboone2000,305030419,8,386471361,0,@pmiossec whats the problem. is there a fix or not. I am using this globally in my company and personal.,False,Impatience
gitkraken stuck in an infinite loop on startup,pmiossec,305030419,9,386515669,1,">whats the problem. is there a fix or not.

Perhaps you are a little long to understand...

I don't care!

**You are in a forum not related in any way to gitkraken.**

Perhaps you should improve your skills to find the support place of a software and not necessarily dumbly post in the first result provided by Google...

But it seems you are not the only one with this lack of skill if it could console yourself. ",False,Impatience
Please change the color of error output,jockm,312953330,1,312953330,0,"With my aging eyes that orange color is increasingly hard to read against a black background, and there is no way to change it.  

Yes I can copy and paste it into another editor, but that shouldn't be necessary just to see a simple message.

This is bad for accessibility and hard on your users.  Please change to something more readable ",True,Impatience
Please change the color of error output,tofrnr,312953330,2,384956581,0,"I agree, e.g. light green (lime) would be much more easy to recognize!",False,0
Please change the color of error output,lmihalkovic,312953330,3,386092501,0,@jockm any thoughts on the colors used in the last screenshot of #3383 would be welcome. the theme works with 1.9.0-beta (just not the highlights),False,0
Please change the color of error output,jockm,312953330,4,386096296,0,"I like the highlights, I think the comment color is fine, if a little subtle.  I think the more traditional soft green would be better.  

However the error color is just as bad as it was before

---

Anything with a heavy component of red or blue is harder to read.  The human eye has the fewest cones (the elements of our eyes that see color) for red, followed by blue, and the most for green.  

So that orange ‚Äî no matter the shade ‚Äî is taxing to read, even for people with good eyesight.  Factor in people with vision disabilities, or even just people getting older; and any shade of orange is going to feel downright hostel

I mean the the app is short on accessibility as it is, every bit helps.

There is exactly nothing wrong with making the error text white.  It is completely clear that it is the error text from context; but most text is black on white, or white on black for a reason.",False,0
Please change the color of error output,lmihalkovic,312953330,5,386207196,0,"@jockm thank you for the input. I [changed the colors](https://raw.githubusercontent.com/lmihalkovic/darkduino/master/sample.png)
",False,0
Please change the color of error output,jockm,312953330,6,386295248,1,"Here is the thing: the error text is still orange.  Aside from tradition there is no good reason for that, and clearly tradition doesn't matter considering the changes you made on the code area.

So please just make the error text white on black, or black on white.  The color coding on the file:line part is fine.",False,Impatience
"and, or doesn't work in DQL ",winnilein,313696087,1,313696087,0,"I migrate a mysql db to sqlAnywhere(sqla) to test my app developed with Doctrine2.

Acces to sqla works,
DQL like
 
$query = $em->createQuery('SELECT e FROM countries e WHERE e.conlng = :lng );

$query->setParameters(array(
    'lng' => 'DE'
            ));

$result = $query->getResult();

also runs funny.
 
$query = $em->createQuery('SELECT e FROM countries e WHERE e.connum = :num );

$query->setParameters(array(
    'num' => '4'
            ));

$result = $query->getResult();

also no problems!

## BUT!

$query = $em->createQuery('SELECT e FROM countries e WHERE e.conlng = :lng AND e.connum = :num');

$query->setParameters(array(
    'lng' => 'DE',
    'num' => '4',
            ));

$result = $query->getResult();

doesn't work!!

$result = empty!

with operator OR it's the same problem!

Any idea!
# #
",True,Bitter frustration
"and, or doesn't work in DQL ",Ocramius,313696087,2,380834595,0,What's the executed SQL query? Can you run that manually and verify the output? Also make sure the parameter types match.,False,0
"and, or doesn't work in DQL ",winnilein,313696087,3,381045479,0,"Interactive sql works!

![grafik](https://user-images.githubusercontent.com/38318899/38720441-91189f92-3ef6-11e8-9d20-b188b7e0eb70.png)

But if i change my test-pgm to this ..

![grafik](https://user-images.githubusercontent.com/38318899/38721092-197090d2-3ef9-11e8-9c87-2db46a9e2065.png)

look at the sql statement!

## SELECT e FROM countries e WHERE e.conlng = :lng  or e.connum = :num

and the result in the sceenshot.

It woorks with an OR operator??!

## I'm very irritated !

",False,Bitter frustration
"and, or doesn't work in DQL ",Ocramius,313696087,4,381098664,1," > `SELECT e FROM countries e WHERE e.conlng = :lng or e.connum = :num`

Is this the same SQL statement that the ORM runs?
Use a [`DebugStack`](https://github.com/doctrine/dbal/blob/0f23ed9ba28db2b392eeaaf5938ce804e52084b9/lib/Doctrine/DBAL/Logging/DebugStack.php) logger to see if the SQL statement is the same.

The one difference I noticed is that sometimes your parameters are `string`, sometimes `int`: try looking for differences there as well as in the executed SQL string.

 > I'm very irritated !

This is not a support hotline: if you need to get irritated over volunteers helping you out, you can instead hire somebody to help you out, and be (contractually) allowed to be irritated instead.",False,Bitter frustration
GDPR,micgro42,315863357,1,315863357,0,"Next month (May 2018), a new EU regulation will come into effect, concerning the privacy/ data protection of user data. This new regulation comes with very high potential sanctions: up to 20 Million EUR or 4% of a years revenue, depending on which is _higher_.

This also poses some questions on whether DokuWiki is GDPR-compliant (and what does that even mean?):

- [ ] If a user deletes their account, do we have to delete their username from changelogs/meta?
  - Question asked here: https://law.stackexchange.com/q/27795/17677
  - [ ] Would that potentially conflict with the license, if that license requires attribution?
- [ ] Do we have to delete IP-addresses from changelogs, maybe after some time?
  - [ ] Can we do that in a way that still lets us tell different anonymous users apart from each other?
- [ ] Do we need to show some message / do we need some legally worded user opt-in for some things? (Subscriptions?)
- [ ] Do we need a Privacy Statement for DokuWiki.org?
  - [ ] should we provide examples other users can adjust?
  - [ ] would it be desirable to extend that with technology (eg. automatically list what data is collected when, why and how long) and provide a way for plugins to hook into that?

Maybe some of you have expertise in this matter or work in a company with legal resources to answer such questions -- Input would be greatly appreciated üôè",True,0
GDPR,selfthinker,315863357,2,382905108,0,Do we have any donated money left to potentially pay a lawyer to help answer some questions?,False,0
GDPR,splitbrain,315863357,3,383005684,0,"Potentially yes. However since all this law *really* achieves is feeding lawyers, I would prefer to not contribute any more to that for ideological reasons ;-)

If anyone out there is already paying a lawyer to answer their GDPR questions, it would be nice if they could sneak in our questions though...",False,Entitlement
GDPR,cjohnsonuk,315863357,4,383101429,0,"As long as the logs only contain a user ID (ie a numerical reference to their user account) then you'd only have to hash their user details in the central account record (where the numerical user ID, user name and email address are associated) to remove the ""personally identifiable information"" (Pii).    If the user account is ""deleted"" by marking their account as deleted and replacing their Pii with a one way hash of the information in their account details then there is no longer anything that personally identifies them.  The posts could be detected as having a hashed user ID and shown as authored by ""[deleted user]"". If the user rejoined the service with the same details then the a check on the new account's one way hash would match with the deleted accounts hash and the an question asked of the user ""Do you want to associate your previous posts with your new account?"" 

But I checked the logs and they contain the _actual_ user name, not a numerical reference. So that would be a real chore to update the logs (but potentially still possible).

And then of course you have the issue where the user's Pii is included in the body / text of the posts...
eg in a todo item where the username is listed
",False,0
GDPR,mprins,315863357,5,383132933,0,"Even though thus is a EU regulation, the actual implementation is in national law so answers will vary across legislations. 
Starting with a privacy statement or agreement in which you explain what is collected and why and is accessible to whom and when it will be deleted is always a good start.",False,0
GDPR,Digitalin,315863357,6,383168648,0,"From some fresh readings, this regulation is a law that applies to all EU members with extraterritorial involvements (for outside EU working with Europeans),  probably with some variation by land, but not so much (e.g. sensitive data ). 
To get an quick overview, WordPress has a [roadmap](https://make.wordpress.org/core/2018/03/28/roadmap-tools-for-gdpr-compliance) to address GDPR and its first [implementations](https://core.trac.wordpress.org/query?status=!closed&keywords=~gdpr). 
It is quite complex stuff but seems logic and normal (see [Max Schrems](https://en.wikipedia.org/wiki/Max_Schrems ) ). Privacy and consent by design, right to erasure, personal data backup, encrypted data, ...etc,  just good sense. At this stage, I am not sure a lawyer is necessary but companies would need to use a DokuWiki core and plugins GDPR-compliant. 
",False,0
GDPR,splitbrain,315863357,7,383922750,0,I started a privacy policy at https://www.dokuwiki.org/privacy -- keeping it understandable (as requested by the GDPR) and complete is quite hard. Any hint on what's missing is welcome.,False,0
GDPR,cjohnsonuk,315863357,8,384096436,0,"> I started a privacy policy at https://www.dokuwiki.org/privacy -- keeping
> it understandable (as requested by the GDPR) and complete is quite hard.
> Any hint on what's missing is welcome.

I'm rewriting ours tomorrow.  I'll take a look and make some suggestions.

UPDATE : The one at the above link seems to have most of it covered well.  The additional sections that we have in ours are mostly to do with marketing which is most likely not relevant.",False,0
GDPR,Michaelsy,315863357,9,385056832,0,"[off-topic]: @splitbrain: I assume this is the page do you mean at the top of https://www.dokuwiki.org/privacy ?!
I already corrected the link there.",False,0
GDPR,Digitalin,315863357,10,388551842,0,"An excellent article in Bozho's tech blog: [GRPD - a practical guide for developers ](https://techblog.bozho.net/gdpr-practical-guide-developers/). Also, the Drupal GDPR Compliance Team gives a lot of links on their [dedicated page](https://www.drupal.org/project/drupal_gdpr_team)",False,0
GDPR,T100D,315863357,11,388653361,0,"@splitbrain About the privacy page, I think you have to ensure somehow the users data is save and that how it is kept save has to be described somehow internally for accountability.

Serverlogs, do they contain a user ticket number that is related to their account and visible or knowable by google analytics, if so you should mention that possibly.

GDPR is not done something done by lawyers, but rather accountants, they check if their customers are GDPR compliant. 

Good starting point: https://gdprchecklist.io
",False,0
GDPR,splitbrain,315863357,12,388761817,0,"Everyone, please refrain from posting more links to pages ""that explain everything"". If you want to help out, do one of the following:

* extend the privacy policy at https://www.dokuwiki.org/privacy
** feel free to ask specific questions for things you can not answer (eg. details of the server setup)
* post answers to the questions in the original post, with references to the exact text of the applicable laws",False,Bitter frustration
GDPR,daumling,315863357,13,390864029,0,"What about the cookie nag box? DW uses cookies, so DW would need such a box. I noticed that the bootstrap3 template offers to activate a cookie nag box, so why not integrate such a thing into the DW core?",False,0
GDPR,T100D,315863357,14,390962391,0,"@daumling Cookies which are required in order to fulfill the requests of the website visitor do not require explicit user consent. But any others ‚Äî including those used for general use statistics eg tracking ‚Äî do require it.

On our local dokuwiki as far as i can see the cookies are only functional to dokuwiki.",False,0
GDPR,daumling,315863357,15,390965152,0,"Not sure about that. Session cookies are considered personal data AFAIK, and consent is required. See e.g. this article: https://www.cookiebot.com/en/gdpr-cookies/ - it mentions session cookies.

There is a cookielaw plugin, but it is rudimentary.",False,0
GDPR,xrat,315863357,16,392180629,0,"AFAIK, https://www.dokuwiki.org/privacy is missing the required bits of [Information to be provided where personal data are collected](http://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX%3A32016R0679#d1e2254-1-1) GDPR ¬ß13, especially the mention of the data subject's rights.

Edit: Link to law replaced w/ official URL.",False,0
GDPR,splitbrain,315863357,17,392522327,0,"I updated the last part and renamed it to ""Your Rights"" that should make it more clear.

Regarding the ""where personal data are collected"" - is that referring to which country? That would be France (Hetzner's servers are located there). Not sure where to put that though.

@xrat can you make changes to the page where you think clarification is needed?",False,0
GDPR,Traumflug,315863357,18,392622963,0,"For your inspiration, I've edited Greebo (the installed release) to make the *DOKU_PREFS* cookie a session cookie. A session cookie means no permanent storage, so no user consent required.

The second commit removes recording of IP addresses from the logs. Quite some places need code removal, still the result works just fine. All new changelog entries no longer receive the IP address, so nothing can go wrong. Some retrocompatibility code for dealing with older records is also included.

As making a pull request on Github is a chore and Github refuses to accept patches, I made a Gist: https://gist.github.com/Traumflug/74fd0b4c8968fd0184e503d221b13310 with both patches.

With these patches applied the privacy statement reduces to about this (DokuWiki markup):

----
==== General Data Protection Regulation (GDPR) ====

We're neither interested in personal data, nor do we try to collect or use such data. In detail:

  * Pages at reprap-diy.com do not use trackers.
  * Visiting pages at reprap-diy.com stores up to three cookies in your browser to follow the session. These cookies get deleted when the session ends (when you close your browser).
  * Creating an account at reprap-diy.com stores your email address, content of the //Real name// field and an encrypted hash of your password.
  * Logging into an account and checking the //Remember me// checkbox stores another, permanent cookie (valid for one year) to keep you logged in. To remove this cookie, log out.
  * Each page edit stores the username of the user who did the edit. This information cannot get removed, but if the related account was removed, it also cannot be mapped to an email address or other personal data.
  * During page editing your IP address is used to lock the page against a competing edit. The address gets removed when the edit gets saved.
  * Some of the pages on reprap-diy.com may contain external videos. For YouTube we use the ""privacy enhanced"" youtube-nocookie.com domain that will not track your visit. Your IP address will be visible to the server providing the video, though. 
  * To view the data stored about you at reprap-diy.com, look at your  [[start?do=profile|user profile]].
  * To remove this data, go to your [[start?do=profile|user profile]] and delete your account.
----
Voil√°, no user consent required, problem solved.

The only issue which might remain is fighting spammers. No IP address, no entry into blacklists. But we all have secured account registration against spammers, right?

----
In case somebody doesn't believe that session cookies need no user consent, he may have a look at this pretty official page: http://ec.europa.eu/ipg/basics/legal/cookies/index_en.htm#section_2. It states:

> Cookies clearly exempt from consent according to the EU advisory body on data protection include:
>
> * user‚Äëinput cookies (session-id) such as first‚Äëparty cookies to keep track of the user's input when filling online forms, shopping carts, etc., for the duration of a session or persistent cookies limited to a few hours in some cases
> * authentication cookies, to identify the user once he has logged in, for the duration of a session
    user‚Äëcentric security cookies, used to detect authentication abuses, for a limited persistent duration
> * multimedia content player cookies, used to store technical data to play back video or audio content, for the duration of a session
> [...]
",False,Insulting
GDPR,selfthinker,315863357,19,392716963,0,"If I understand it correctly, I don't think we need to do anything about cookies for GDPR (apart from informing the user about them which is already handled by the privacy statement).
The only cookie which contains personally identifiable information is the `DW<hash>` cookie, and users can choose to delete that one at the end of each session by not ticking ""Remember me"". The `DOKU_PREFS` cookie is not used for personally identifiable information (although plugins could potentially abuse it).",False,0
GDPR,selfthinker,315863357,20,392722396,0,"Sorry for posting more links, but these are more relevant because this is how two big wiki projects handle GDPR:

* [GDPR in Wikimedia's issue tracker](https://phabricator.wikimedia.org/T194901) gives an overview but doesn't really say that much about what actually needs to be done and includes links to a lot of speculation. Like so many others, Wikipedia has changed their Privacy Policy. I'm not sure how they're (not) dealing with [public contributions](https://wikimediafoundation.org/wiki/Privacy_policy#your-public-contribs) is really compliant?
* [OpenStreetMap's GDPR Position Paper](https://wiki.openstreetmap.org/wiki/File:GDPR_Position_Paper.pdf) is more interesting as it seems quite thorough. Their process around [account removal](https://wiki.openstreetmap.org/wiki/Privacy_Policy#Account_Removal) is not as helpful for us, though, as they don't allow anonymous edits (and use a database which makes that task easier).
",False,0
GDPR,Traumflug,315863357,21,392724710,0,">  The DOKU_PREFS cookie is not used for personally identifiable information

The sheer existence of a cookie means personally identifiable information, because they come with and IP address / DNS entry attached. Content doesn't matter, much less encrypted content.

> Sorry for posting more links, but these are more relevant

D'oh. Those pages pointing to some volunteering efforts are more relevant than an official page. Ouch.

I certainly see this GDPR panic mode everywhere. People try extremely hard to stick to what they're used to, providing endless text blobs in the hope to walk around the problem somehow with lawyer fineprint. Instead of simply fixing the software.",False,Mocking
GDPR,selfthinker,315863357,22,392733145,0,"> The sheer existence of a cookie means personally identifiable information, because they come with and IP address / DNS entry attached.

I don't think this specific cookie comes with IP address and DNS entry attached. That cookie and its contents is not stored on the server but only in the browser.

> D'oh. Those pages pointing to some volunteering efforts are more relevant than an official page. Ouch.

No need to become personal, especially not dissing ""volunteering efforts"" in any Open Source project.

I meant it is much more relevant to us as in no-one else (apart from other version control software or services, like git or GitHub) deals with the one question which none of the official pages deal with: how to deal with user contributions that are intrinsic to the software.

> Instead of simply fixing the software.

If anyone of us would know what is needed to fix the software, we would do it. Can you point out what needs fixing? I don't think that is possible, most certainly not ""simple"".
I have the feeling no-one really understands any specifics about GDPR (and that includes the big guys like Google and Facebook). I think the majority of what's out there is misinformation.

I like how OpenStreetMap (who ""have received professional counsel"") say in their paper:

> Naturally estimating the impact of the GDPR introduction and consequences before it is
actually in force are fraught with the problem that we have to guess how the legislation will
be applied in practice and there is a danger of both over- and underreacting.
",False,Irony
GDPR,michitux,315863357,23,392736227,0,"Changing `DOKU_PREFS` into a session does not fix anything but breaks the usability imho. `DOKU_PREFS` is used e.g. for storing the size of the edit window and this should persist across sessions imho. As Anika says, cookies do not store the IP (or even DNS entry) of the user, the IP address is instead sent with every request.

Not storing IP addresses is also not the solution as there is a very valid reason to store them at least temporarily: detect and remove vandalism (by IP address you can identify the connection between several edits, possibly even several user accounts) and to be able to identify the author (at least in court) if the content posted was illegal and the site owner gets sued because of that.

What I think would be a good thing is to have some automatic way to remove IP addresses after some time, at least for changes where the user has been logged in (this could be a plugin of course). For anonymous edits I'm not sure if the IP address can be interpreted as an author identification that needs to be stored because of the license (but this of course depends on the selected license).

Concerning the removal of the user name: my personal (non-lawyer) interpretation is that due to the license of the content (creative commons license at least with attribution), DokuWiki has a legitimate interest to store this attribution as it otherwise cannot use the content and as the Wikimedia issue tracker says ""the right of erasure only exists when the processing is not necessary for some legitimate interest of the data controller"".",False,0
GDPR,splitbrain,315863357,24,392738048,0,"Thanks @michitux for pointing out the usability aspects. I was just about to do that.

@selfthinker thanks for the links on how other wikis handle it. I'll have a look later.

Regarding removing IP addresses after while, there is now the aptly named gdpr plugin which does exactly that. It also replaces user names in change logs for deleted users.

I will close this ticket now. We will probably not ever get definite answers to all the questions asked in the original issue. And it's an issue people love to discuss for the sake of discussing without getting any further.

For now we should simply focus on having a useful privacy policy for the 0.1 percent of users who care about that. So please, if you think the privacy policy needs adjustments just go ahead and do it.",False,Bitter frustration
GDPR,Traumflug,315863357,25,392738379,1,"> I don't think this specific cookie comes with IP address and DNS entry attached. That cookie and its contents is not stored on the server but only in the browser.

It's the very nature of any cookie to come with IP address or DNS records attached. All of them are stored in the browser, only. Still GDPR considers them to be personal data, which is why they have to become session cookies or ask for user content before being placed. Fairly simple basics.

> If anyone of us would know what is needed to fix the software, we would do it.

Code is provided above. Instead of looking at the code and commenting it, all the extensive comments sum up to ""Go away, we have to find a harder way"".

Very apparently, some people here *want* to stick their head in the sand. Instead of applying these patches and enjoying a GDPR compliant wiki. Enjoy it!

And I just see how you closed the issue to make extra sure nobody sees this solution. Extra compliment to that much stupidity!",False,Insulting
gatling stuck when project has other dependencies,kgignatyev-inspur,332965663,1,332965663,0,"
[gatling-stuck.zip](https://github.com/gatling/gatling/files/2107919/gatling-stuck.zip)

Basically when project includes 
<dependency>
            <groupId>io.kubernetes</groupId>
            <artifactId>client-java</artifactId>
            <version>1.0.0</version>
        </dependency>

gatling does not start execution of scenario. Repro case is attached

mvn  clean gatling:execute

and gatling is stuck doing nothing, last message
 
    14:34:15.070 [main] INFO  io.gatling.http.ahc.HttpEngine - Start warm up
    
    
now comment out kubernetes client in pom.xml and all will be well  
",True,0
gatling stuck when project has other dependencies,slandelle,332965663,2,397795838,0,"You've messed up the ""writers"" option in gatling.conf.
Works just fine for me.",False,0
gatling stuck when project has other dependencies,kgignatyev-inspur,332965663,3,398224013,0,"Hmm @slandelle , do not you think that simply stating that I 'messed up ""writers"" ' without explaining why is rude?

As far as I know I did not mess them, if I do not put writers in square braces and leave as in default config then Gattling would not even start with this exception:

/src/test/resources/gatling.conf: 118: gatling.data.writers has type STRING rather than LIST

java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at io.gatling.mojo.MainWithArgsInFile.runMain(MainWithArgsInFile.java:50)
	at io.gatling.mojo.MainWithArgsInFile.main(MainWithArgsInFile.java:33)
Caused by: com.typesafe.config.ConfigException$WrongType: gatling.conf @ file:/Users/kgignatyev/dev/reprocases/gatling-stuck/src/test/resources/gatling.conf: 118: gatling.data.writers has type STRING rather than LIST
	at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:159)
	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170)
	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:176)
	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:176)
	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184)
	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189)
	at com.typesafe.config.impl.SimpleConfig.getList(SimpleConfig.java:258)
	at com.typesafe.config.impl.SimpleConfig.getHomogeneousUnwrappedList(SimpleConfig.java:329)
	at com.typesafe.config.impl.SimpleConfig.getStringList(SimpleConfig.java:387)
	at io.gatling.core.config.GatlingConfiguration$.mapToGatlingConfig(GatlingConfiguration.scala:215)
	at io.gatling.core.config.GatlingConfiguration$.load(GatlingConfiguration.scala:98)
	at io.gatling.app.Gatling$.start(Gatling.scala:54)
	at io.gatling.app.Gatling$.fromArgs(Gatling.scala:45)
	at io.gatling.app.Gatling$.main(Gatling.scala:37)
	at io.gatling.app.Gatling.main(Gatling.scala)
	... 6 more

",False,Bitter frustration
gatling stuck when project has other dependencies,slandelle,332965663,4,398273426,1,"> Hmm @slandelle , do not you think that simply stating that I 'messed up ""writers"" ' without explaining why is rude?

No, I don't think spending some personal time investigating a problem you have with a software you got from me for free is ""rude"".

I've pointed out your mistake and you are perfectly capable of spotting the difference between the original configuration that was working and you've commented out and the one you've changed and that don't.

From the `gatling.conf` in the sample you've provided (L217):

```
    #writers = [""graphite"", ""console"", ""file""]
    writers = [""console, file""]
```",False,Mocking
storage OOM not proper handled by gradle,c3ph3us,339498894,1,339498894,0,"when we have storage OOM gradle doesn't sygnalize that, instead proper info we have a bullshit :)

`$ gradle clean`

>FAILURE: Build failed with an exception.
>
>* What went wrong:
>Unable to start the daemon process.
>This problem might be caused by incorrect configuration of the daemon.
>For example, an unrecognized jvm option is used.
>Please refer to the user guide chapter on the daemon at >https://docs.gradle.org/4.7/userguide/gradle_daemon.html
>Please read the following process output to find out more:`
>-----------------------
>* Try:
>Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log >output. Run with --scan to get full insights.

Unable to start the daemon process... **when explicity run with false flag in props file**

the next try maybe gradle didn't read the prop file proper .. **so explicity set no deamon arg**

`$ gradle --info --no-daemon clean`

>Initialized native services in: /opt/gradle/ceph3us/native
To honour the JVM settings for this build a new JVM will be forked. Please consider using the daemon: https://docs.gradle.org/4.7/userguide/gradle_daemon.html.
Starting process 'Gradle build daemon'. Working directory: /opt/gradle/ceph3us/daemon/4.7 Command: /opt/jdk1.8/bin/java -XX:+AggressiveOpts -XX:+UseG1GC -Xmn512m -XX:MaxMetaspaceSize=1g -XX:SurvivorRatio=40 -XX:+UseCompressedOops -XX:+UseCompressedClassPointers -XX:-OmitStackTraceInFastThrow -XX:SoftRefLRUPolicyMSPerMB=100 -XX:-HeapDumpOnOutOfMemoryError -Xms512m -Xmx3g -Dfile.encoding=UTF-8 -Duser.country=PL -Duser.language=pl -Duser.variant -cp /opt/gradle/lib/gradle-launcher-4.7.jar org.gradle.launcher.daemon.bootstrap.GradleDaemon 4.7
Successfully started process 'Gradle build daemon'
An attempt to start the daemon took 1.005 secs.

FAILURE: Build failed with an exception.

>* What went wrong:
Unable to start the daemon process.
This problem might be caused by incorrect configuration of the daemon.
For example, an unrecognized jvm option is used.
Please refer to the user guide chapter on the daemon at https://docs.gradle.org/4.7/userguide/gradle_daemon.html
Please read the following process output to find out more:
-----------------------


>* Try:
Run with --stacktrace option to get the stack trace. Run with --debug option to get more log output. Run with --scan to get full insights.

>* Get more help at https://help.gradle.org

still some daemon shit WTF ??? one more try brings same results of bulshit doeasnt reveal the TRUE CAUSE for BUILD FAILED 

'$ gradle --debug --no-daemon clean'

>{ unrelated sensitive data cut}
>16:50:14.639 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Changing state to: STARTING
16:50:14.640 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Waiting until process started: Gradle build daemon.
16:50:14.655 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Changing state to: STARTED
16:50:14.656 [INFO] [org.gradle.process.internal.DefaultExecHandle] Successfully started process 'Gradle build daemon'
16:50:14.656 [DEBUG] [org.gradle.launcher.daemon.client.DefaultDaemonStarter] Gradle daemon process is starting. Waiting for the daemon to detach...
16:50:14.657 [DEBUG] [org.gradle.process.internal.ExecHandleRunner] waiting until streams are handled...
16:50:14.659 [DEBUG] [org.gradle.launcher.daemon.bootstrap.DaemonOutputConsumer] Starting consuming the daemon process output.
16:50:15.611 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Changing state to: DETACHED
16:50:15.611 [DEBUG] [org.gradle.process.internal.DefaultExecHandle] Process 'Gradle build daemon' finished with exit value 0 (state: DETACHED)
16:50:15.611 [DEBUG] [org.gradle.launcher.daemon.client.DefaultDaemonStarter] Gradle daemon process is now detached.
16:50:15.613 [INFO] [org.gradle.launcher.daemon.client.DefaultDaemonStarter] An attempt to start the daemon took 0.982 secs.
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] FAILURE: Build failed with an exception.
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * What went wrong:
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Unable to start the daemon process.
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] This problem might be caused by incorrect configuration of the daemon.
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] For example, an unrecognized jvm option is used.
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Please refer to the user guide chapter on the daemon at https://docs.gradle.org/4.7/userguide/gradle_daemon.html
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Please read the following process output to find out more:
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] -----------------------
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Try:
16:50:15.620 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] Run with --stacktrace option to get the stack trace.  Run with --scan to get full insights.
16:50:15.621 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] 
16:50:15.621 [ERROR] [org.gradle.internal.buildevents.BuildExceptionReporter] * Get more help at https://help.gradle.org

**pleaes add OOM storage watcher during task execution that will throw some sort of StorageOOM exception**

**distracts from the fact that there was no memory left at the start of the gradle .. which should be signalized apriori any taskl start / evaluate**

tneet to consider the SPACE 
-- WHERE PROJECT IS EXECUTED (build output)   
-- WHEN GRADLE WRITES  (cache, etc) 

**those places should be checked during start gradle (LOW MEM WARNING) and monitored as mentioned above**


i have wasted 5 min befor i did to know the real cause.. ",True,Bitter frustration
storage OOM not proper handled by gradle,oehme,339498894,2,403610103,0,Closing due to abusive language. Please read our code of conduct and open an issue using appropriate language.,False,Bitter frustration
storage OOM not proper handled by gradle,c3ph3us,339498894,3,405948813,1,"@oehme  you are funny :) you can learn a bit about semantics, grammar and the meanings of words - essentialy about language and usage :) 
can you tell me who was here offended ?

**maybe better was to use here a euphemisms so that the dumb people didn't think they were being offended when in fact nobody was...**  if you fell so then it is your right to feel as you want to :) 

understanding is key to everything.. so **try to understand it does not hurt!**


cipa, kutas, jebnij, pierdolenie 

best 
ceph3us",False,Bitter frustration
Replace use of whitelist with allowlist and blacklist with denylist,dhh,352750519,1,352750519,0,"Per https://twitter.com/dhh/status/1032050325513940992, I'd like for Rails to set a good example and tone by using better terminology when we can. An easy fix would be to replace our use of whitelist with allowlist and blacklist with denylist.

We can even just use them as verbs directly, as we do with the former terms. So something is allowlisted or denylisted.

I took a quick look and it seems like this change is mostly about docs. We only have one piece of the code that I could find on a search that uses the term whitelist with `enforce_raw_sql_whitelist`. Need to consider whether we need an alias and a deprecation for that.",True,0
Replace use of whitelist with allowlist and blacklist with denylist,shalvah,352750519,2,414870475,0,"Good intentions, but I doubt there's any relation of the origin of the terms blacklist/whitelist to race. There are many idioms and phrases in the English language that make use of colours without any racial backstories.
I haven't met any black person (myself included) who was ever offended by the use of ""blacklist"". Frankly, a good number find it patronising to make this kind of change.",False,Bitter frustration
Replace use of whitelist with allowlist and blacklist with denylist,ghost,352750519,3,414873068,0,At least [one source from a search](https://www.etymonline.com/word/blacklist) suggests the word had its origins around union members.,False,0
Replace use of whitelist with allowlist and blacklist with denylist,dhh,352750519,4,414873738,0,"Regardless of origin, allow/deny are simply clearer terms that does not require tracing the history of black/white as representations of that meaning. We can simply use the meaning directly.",False,0
Replace use of whitelist with allowlist and blacklist with denylist,glaszig,352750519,5,414875618,1,"1. [etymology is quite important](
https://www.quora.com/Is-the-term-blacklist-racist?share=1). in the end, we might consider plain words ‚Äûblack‚Äú and ‚Äûwhite‚Äú racist and enter the realms of newspeak which i figure you especially, @dhh, are familiar with.

2. ‚Äúallow/deny are simply clearer terms‚Äù ‚Äî now that‚Äôs an actual, technically useful argument.

3. can we please stop jumping onto political bandwagons? i am here for the sanity.",False,Bitter frustration
Attempts to add timeout support for lua unit commands,tomjn,406907882,1,406907882,0,"e.g. something like this:

```lua
    Spring.GiveOrderToUnit(unitID,
      CMD.INSERT,
      {-1,CMD.ATTACK,''CMD.OPT_SHIFT'',''unitID2''},
      {""timeout"" = 500 }
    );
```

Related to https://springrts.com/mantis/view.php?id=6128",True,Impatience
Attempts to add timeout support for lua unit commands,rtri,406907882,2,460854542,0,"That would work, but the (badly named) command ""options"" have special meaning. Timeout is a separate parameter.",False,Irony
Attempts to add timeout support for lua unit commands,tomjn,406907882,3,460855281,0,"@rtri I appreciate the comment but you closed it before I could fix the PR!
No need for the trigger happy close, unless you're trying to put off
contributions?

Perhaps next time you should use the built in review functionality and
request changes? E.g. switching to an additional parameter because options
have special meaning?

On Wed, 6 Feb 2019 at 00:07, rtri <notifications@github.com> wrote:

> Closed #427 <https://github.com/spring/spring/pull/427>.
>
> ‚Äî
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/spring/spring/pull/427#event-2120001408>, or mute the
> thread
> <https://github.com/notifications/unsubscribe-auth/AADl598QGC_BSGkh38MqMyeyXMN4RDV2ks5vKhy8gaJpZM4ajxVn>
> .
>
",False,Bitter frustration
Attempts to add timeout support for lua unit commands,rtri,406907882,4,460862065,0,"You seem to have missed be999101, which I started before this PR arrived.",False,0
Attempts to add timeout support for lua unit commands,tomjn,406907882,5,461088078,1,"@rtri **that's no excuse for rudeness**, I opened the PR to try and be helpful.

Next time:

 - If you already have a PR, link to it
 - Leave a review requesting changes

For all you know I may have come back and refactored it shortly after. If it had been a few days then sure, close it, as is common practice here, but instead you were hostile. Stop that.",False,Bitter frustration
daylight saving breaks `+minutes()`,Demetrio92,421471669,1,421471669,0,"Adding minutes to a date around daylight saving creates NAs

```R
tsp_initial = ymd_hms('2019-03-09 07:22:03') %>% force_tz('US/Pacific')
tsp_initial + minutes(1160)
## NA
```

Bug or Feature? 
It definitely broke my until-now-working code.

----------------

## FIX:

```R
tsp_initial = ymd_hms('2019-03-09 07:22:03') %>% force_tz('US/Pacific')
tsp_initial + dminutes(1160)
## ""2019-03-10 03:42:03 PDT""
```

---------

## Explanation

`period` and `duration` act differently when adding a timespan to a date. Consider a simpler example: on the 30th of January what's the date on the calendar in a month from now? 

`period` will tell you it's 30th of February and as this is not possible will produce an NA
```r
ymd_hms(""2021-01-30 11:00:00"") + months(1)  # NA
```

`duration` will add some vague 30 days and return a different date depending on the year. 
```r
ymd_hms(""2021-01-30 11:00:00"") + dmonths(1)  # ""2021-03-01 21:30:00 UTC""
ymd_hms(""2020-01-30 11:00:00"") + dmonths(1)  # ""2020-02-29 21:30:00 UTC""
```

RTFM,  I guess

https://lubridate.tidyverse.org/reference/duration.html
https://lubridate.tidyverse.org/reference/period.html


",True,Bitter frustration
daylight saving breaks `+minutes()`,Demetrio92,421471669,2,473256531,0,"Compared to Postgres behaviour. Just for an example of how this _could_ be handeled. 

```SQL
my-db=> select (
    select (
        select '2019-03-09 07:22:03'::timestamp without time zone at time zone 'US/Pacific'
    ) + interval '1160 mins'
) at time zone 'US/Pacific'
;
      timezone       
---------------------
 2019-03-10 03:42:03
(1 row)


my-db=> select (
    select (
        select '2019-03-09 07:22:03'::timestamp without time zone at time zone 'US/Pacific'
    ) + interval '1160 mins'  - interval '1 hour'
) at time zone 'US/Pacific'
;
      timezone       
---------------------
 2019-03-10 01:42:03

my-db=> select (
    select (
        select '2019-03-09 07:22:03'::timestamp without time zone at time zone 'US/Pacific'
    ) + interval '1160 mins'  + interval '1 hour'
) at time zone 'US/Pacific'
;
      timezone       
---------------------
 2019-03-10 04:42:03
```",False,0
daylight saving breaks `+minutes()`,deanm0000,421471669,3,474593226,0,"The same thing happens with hours().  It's a new bug because I had code that worked last year that was broken this year.  As a workaround I changed my code to something like 

`with_tz(with_tz(mydate, 'GMT')+hours(1),'America/Los_Angeles')`",False,0
daylight saving breaks `+minutes()`,akleinhesselink,421471669,4,482892390,0,"I'm finding a similar issue when you try and parse a time that is incompatible with daylight savings time (i.e. 2:00 AM until 3:00 AM doesn't exist when we ""spring ahead"" in the states).  You get an NA and a failed to parse warning.   Technically this makes sense but maybe this should be an Error rather than a warning, and maybe a more informative message.  

 Surprisingly this doesn't break the ""strptime"" function. 

```r 
library(lubridate)

test_date <- '03/10/19 02:00 AM'
mdy_hm( test_date, tz = 'America/New_York')
#[1] NA
#Warning message:
# 1 failed to parse. 
```
Using strptime 

```r 

d2 <- strptime(test_date, format = '%m/%d/%y %I:%M %p', tz = 'America/New_York')
print( d2 )
# [1] ""2019-03-10 02:00:00""

attributes(d2)
# $names
# [1] ""sec""    ""min""    ""hour""   ""mday""   ""mon""    ""year""   ""wday""   ""yday""   ""isdst"" 
# [10] ""zone""   ""gmtoff""
# $class
# [1] ""POSIXlt"" ""POSIXt"" 
# $tzone
# [1] ""America/New_York"" ""EST""              ""EDT""     

``` 

```r 

sessionInfo()
R version 3.5.3 (2019-03-11)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS Mojave 10.14.4

Matrix products: default
BLAS: /System/Library/Frameworks/Accelerate.framework/Versions/A/Frameworks/vecLib.framework/Versions/A/libBLAS.dylib
LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] shiny_1.3.0     reprex_0.2.1    lubridate_1.7.4

loaded via a namespace (and not attached):
 [1] Rcpp_1.0.1        compiler_3.5.3    later_0.8.0       remotes_2.0.3    
 [5] prettyunits_1.0.2 tools_3.5.3       pkgload_1.0.2     digest_0.6.18    
 [9] pkgbuild_1.0.3    jsonlite_1.6      evaluate_0.13     memoise_1.1.0    
[13] rlang_0.3.4       cli_1.1.0         rstudioapi_0.10   yaml_2.2.0       
[17] xfun_0.6          withr_2.1.2       stringr_1.4.0     knitr_1.22       
[21] desc_1.2.0        fs_1.2.7          devtools_2.0.2    rprojroot_1.3-2  
[25] glue_1.3.1        R6_2.4.0          processx_3.3.0    rmarkdown_1.12   
[29] sessioninfo_1.1.1 callr_3.2.0       clipr_0.5.0       magrittr_1.5     
[33] whisker_0.3-2     usethis_1.5.0     backports_1.1.3   ps_1.3.0         
[37] promises_1.0.1    htmltools_0.3.6   rsconnect_0.8.13  assertthat_0.2.1 
[41] mime_0.6          xtable_1.8-3      httpuv_1.5.1      stringi_1.4.3    
[45] miniUI_0.1.1.1    crayon_1.3.4 
``` 



",False,0
daylight saving breaks `+minutes()`,vspinu,421471669,5,482939819,0,2:AM is borderline. It probably makes sense to parse it actually. I would say this is a bug.,False,0
daylight saving breaks `+minutes()`,DavisVaughan,421471669,6,528960994,0,"I just hit this too with `<datetime> - hours(1)`. It was pretty unfortunate, it wrecked a pretty long series spanning multiple years. Crossing over the DST boundary is definitely what did it.

``` r
library(lubridate, warn.conflicts = FALSE)

bad_time <- with_tz(as_datetime(1300000380) , ""America/New_York"")

bad_time
#> [1] ""2011-03-13 03:13:00 EDT""

bad_time - hours(1)
#> [1] NA

in_est <- with_tz(as_datetime(1300000380 - 781) , ""America/New_York"")
in_est
#> [1] ""2011-03-13 01:59:59 EST""

dst(bad_time)
#> [1] TRUE
dst(in_est)
#> [1] FALSE
```

<sup>Created on 2019-09-06 by the [reprex package](https://reprex.tidyverse.org) (v0.2.1)</sup>",False,0
daylight saving breaks `+minutes()`,DavisVaughan,421471669,7,528998731,0,"Okay, so I think that there is a bug that has been there since essentially the beginning of the package.

The summary is that I think that right here, there should be another if branch that says if we are ONLY updating using hours/min/sec or lower, then it should use `cl_new.pre` if the new civil seconds amount is above the old civil seconds amount (i.e. we went forward), otherwise use `cl_new.post` (we went backwards).
https://github.com/tidyverse/lubridate/blob/cd8267976e51bfab9bd43f9eb30f2749d45e4ff9/src/update.cpp#L168

I've read the docs in `?Period-class`, and I know you want this reversible property:

```
date + period - period = date
```

I would argue that if it the arithmetic only involves hours/minutes/seconds then this is reversible. For my example above, I think that this is perfectly reasonable behavior (lubridate doesn't do this right now):

``` r
library(lubridate, warn.conflicts = FALSE)

border <- with_tz(as_datetime(1300000380 - 781) , ""America/New_York"")
border
#> [1] ""2011-03-13 01:59:59 EST""

border + seconds(1)
#> [1] ""2011-03-13 03:00:00 EDT""

border + seconds(1) - seconds(1)
#> [1] ""2011-03-13 01:59:59 EST""
```

<sup>Created on 2019-09-06 by the [reprex package](https://reprex.tidyverse.org) (v0.2.1)</sup>

With days, you can't enforce the reversible behavior, so this behavior would result in an `NA`, not the problematic result shown here (again, this is my custom version)

``` r
library(lubridate, warn.conflicts = FALSE)

border <- with_tz(as_datetime(1300000380 - 781) , ""America/New_York"")
border
#> [1] ""2011-03-13 01:59:59 EST""

border_minus_a_day <- border - days(1)
border_minus_a_day
#> [1] ""2011-03-12 01:59:59 EST""

one_day_one_second <- days(1) + seconds(1)
one_day_one_second
#> [1] ""1d 0H 0M 1S""

# This SHOULD result in an NA
border_minus_a_day + one_day_one_second
#> [1] ""2011-03-13 03:00:00 EDT""

# Because this is not reversible
border_minus_a_day + one_day_one_second - one_day_one_second
#> [1] ""2011-03-12 02:59:59 EST""
```

<sup>Created on 2019-09-06 by the [reprex package](https://reprex.tidyverse.org) (v0.2.1)</sup>

Update: Hmm, this wouldn't be reversible either. I'm not sure I have the rule quite right yet.

``` r
library(lubridate, warn.conflicts = FALSE)

border <- with_tz(as_datetime(1300000380 - 781) , ""America/New_York"")
border
#> [1] ""2011-03-13 01:59:59 EST""

border_minus_a_day <- border - days(1)
border_minus_a_day
#> [1] ""2011-03-12 01:59:59 EST""

one_day_in_hours_one_second <- hours(24) + seconds(1)

border_minus_a_day + one_day_in_hours_one_second
#> [1] ""2011-03-13 03:00:00 EDT""

border_minus_a_day + one_day_in_hours_one_second - one_day_in_hours_one_second
#> [1] ""2011-03-12 02:59:59 EST""
```

<sup>Created on 2019-09-06 by the [reprex package](https://reprex.tidyverse.org) (v0.2.1)</sup>

Preexisting rationale for this behavior in Java:

- https://www.javacodex.com/Date-and-Time/Testing-Daylight-Saving-By-Adding-One-Second",False,0
daylight saving breaks `+minutes()`,DavisVaughan,421471669,8,529114335,0,"Hmm okay I think the rule could be if we land in the spring forward daylight savings time gap and `(period size) <= (dst gap size)` then it is a reversible operation and we should get a real date and not an `NA`. 99% of the time this means if `(period size) <= 1 hour`, but there are the _rare_ cases where a dst gap is not 1 hour.

(Note that this would imply that the original issue would still be an `NA` because `minutes(1160)` is larger than 1 hour, and would not be reversible)

I would try to implement this, but I can't figure out how to get the size of the dst gap using cctz. The newest version of google's cctz has a method for `<time_zone>.next_transition()` and `.previous_transition()` which might could be used to get the borders of the gap. Then the difference would be the size. https://github.com/google/cctz/blob/b4935eef53820cf1643355bb15e013b4167a2867/include/cctz/time_zone.h#L188

``` r
library(lubridate, warn.conflicts = FALSE)

one_hour_before_border <- force_tz(as_datetime(""2011-03-13 01:00:00"") , ""America/New_York"")
one_hour_before_border
#> [1] ""2011-03-13 01:00:00 EST""

# this is what i would expect...
one_hour_before_border + hours(1)
#> [1] ""2011-03-13 03:00:00 EDT""

# because this is reversible! good!
one_hour_before_border + hours(1) - hours(1)
#> [1] ""2011-03-13 01:00:00 EST""

one_hour_one_second_before_border <- one_hour_before_border - seconds(1)
one_hour_one_second_before_border
#> [1] ""2011-03-13 00:59:59 EST""

one_hour_and_one_second <- hours(1) + seconds(1)

# my tweaked version currently gives this, but
# this should actually give NA because...
one_hour_one_second_before_border + one_hour_and_one_second
#> [1] ""2011-03-13 03:00:00 EDT""

# ...this is not reversible!
one_hour_one_second_before_border + one_hour_and_one_second - one_hour_and_one_second
#> [1] ""2011-03-13 01:59:59 EST""
```

<sup>Created on 2019-09-07 by the [reprex package](https://reprex.tidyverse.org) (v0.2.1)</sup>

Here is a strange case where there was a 20 minute DST gap. Found from the [date library examples](https://github.com/HowardHinnant/date/wiki/Examples-and-Recipes#tz_search)

``` r
library(lubridate, warn.conflicts = FALSE)

one_second_before_africa_border <- with_tz(as_datetime(""1920-08-31 23:59:59""), ""Africa/Accra"")
one_second_before_africa_border
#> [1] ""1920-08-31 23:59:59 GMT""

# this is what i would expect...
one_second_before_africa_border + seconds(1)
#> [1] ""1920-09-01 00:20:00 +0020""

# because this is reversible! good!
one_second_before_africa_border + seconds(1) - seconds(1)
#> [1] ""1920-08-31 23:59:59 GMT""

twenty_minutes_one_second_before_africa_border <- one_second_before_africa_border - minutes(20)
twenty_minutes_one_second_before_africa_border
#> [1] ""1920-08-31 23:39:59 GMT""

twenty_minutes_one_second <- minutes(20) + seconds(1)

# my tweaked version currently gives this, but
# this should actually give NA because...
twenty_minutes_one_second_before_africa_border + twenty_minutes_one_second
#> [1] ""1920-09-01 00:20:00 +0020""

# ...this is not reversible!
twenty_minutes_one_second_before_africa_border + twenty_minutes_one_second - twenty_minutes_one_second
#> [1] ""1920-08-31 23:59:59 GMT""
```

<sup>Created on 2019-09-07 by the [reprex package](https://reprex.tidyverse.org) (v0.2.1)</sup>",False,0
daylight saving breaks `+minutes()`,Demetrio92,421471669,9,530394116,0,"> Okay, so I think that there is a bug that has been there since essentially the beginning of the package.
> 
> The summary is that I think that right here, there should be another if branch that says if we are ONLY updating using hours/min/sec or lower, then it should use `cl_new.pre` if the new civil seconds amount is above the old civil seconds amount (i.e. we went forward), otherwise use `cl_new.post` (we went backwards).
> 
> https://github.com/tidyverse/lubridate/blob/cd8267976e51bfab9bd43f9eb30f2749d45e4ff9/src/update.cpp#L168
> 
> I've read the docs in `?Period-class`, and I know you want this reversible property:
> 
> ```
> date + period - period = date
> ```
> 
> I would argue that if it the arithmetic only involves hours/minutes/seconds then this is reversible. For my example above, I think that this is perfectly reasonable behavior (lubridate doesn't do this right now):
> 
> ```r
> library(lubridate, warn.conflicts = FALSE)
> 
> border <- with_tz(as_datetime(1300000380 - 781) , ""America/New_York"")
> border
> #> [1] ""2011-03-13 01:59:59 EST""
> 
> border + seconds(1)
> #> [1] ""2011-03-13 03:00:00 EDT""
> 
> border + seconds(1) - seconds(1)
> #> [1] ""2011-03-13 01:59:59 EST""
> ```
> 
> Created on 2019-09-06 by the [reprex package](https://reprex.tidyverse.org) (v0.2.1)
> 
> With days, you can't enforce the reversible behavior, so this behavior would result in an `NA`, not the problematic result shown here (again, this is my custom version)
> 
> ```r
> library(lubridate, warn.conflicts = FALSE)
> 
> border <- with_tz(as_datetime(1300000380 - 781) , ""America/New_York"")
> border
> #> [1] ""2011-03-13 01:59:59 EST""
> 
> border_minus_a_day <- border - days(1)
> border_minus_a_day
> #> [1] ""2011-03-12 01:59:59 EST""
> 
> one_day_one_second <- days(1) + seconds(1)
> one_day_one_second
> #> [1] ""1d 0H 0M 1S""
> 
> # This SHOULD result in an NA
> border_minus_a_day + one_day_one_second
> #> [1] ""2011-03-13 03:00:00 EDT""
> 
> # Because this is not reversible
> border_minus_a_day + one_day_one_second - one_day_one_second
> #> [1] ""2011-03-12 02:59:59 EST""
> ```
> 
> Created on 2019-09-06 by the [reprex package](https://reprex.tidyverse.org) (v0.2.1)
> 
> Update: Hmm, this wouldn't be reversible either. I'm not sure I have the rule quite right yet.
> 
> ```r
> library(lubridate, warn.conflicts = FALSE)
> 
> border <- with_tz(as_datetime(1300000380 - 781) , ""America/New_York"")
> border
> #> [1] ""2011-03-13 01:59:59 EST""
> 
> border_minus_a_day <- border - days(1)
> border_minus_a_day
> #> [1] ""2011-03-12 01:59:59 EST""
> 
> one_day_in_hours_one_second <- hours(24) + seconds(1)
> 
> border_minus_a_day + one_day_in_hours_one_second
> #> [1] ""2011-03-13 03:00:00 EDT""
> 
> border_minus_a_day + one_day_in_hours_one_second - one_day_in_hours_one_second
> #> [1] ""2011-03-12 02:59:59 EST""
> ```
> 
> Created on 2019-09-06 by the [reprex package](https://reprex.tidyverse.org) (v0.2.1)
> 
> Preexisting rationale for this behavior in Java:
> 
>     * https://www.javacodex.com/Date-and-Time/Testing-Daylight-Saving-By-Adding-One-Second

seems like a time for a PR!",False,0
daylight saving breaks `+minutes()`,vspinu,421471669,10,536286765,0,"This is a bug and it has to do with the fact that arithmetics with periods are not well defined. 

> I've read the docs in ?Period-class, and I know you want this reversible property:

That rule is unfortunate. There is no solid reason to follow it and it caused a lot of trouble with the package. There is a new lower level package `timechange` which has a clear defined semantics and allows a full control over how things are rolled both on month change and DST.

```R
> timechange::time_add(time, minutes = 1160)
[1] ""2019-03-10 03:42:03 PDT""
```

I wanted to roll lubridate on top of timechange for at least a year now, but just don't have time. Hopefully by the end of this year. 
",False,0
daylight saving breaks `+minutes()`,hadley,421471669,11,555764867,0,"Minimal reprex:

``` r
library(lubridate, warn.conflicts = FALSE)

x <- ymd_hms('2019-03-09 07:22:03', tz = 'US/Pacific')
x + minutes(1160)
#> [1] NA
timechange::time_add(x, minutes = 1160)
#> [1] ""2019-03-10 03:42:03 PDT""
```

<sup>Created on 2019-11-19 by the [reprex package](https://reprex.tidyverse.org) (v0.3.0)</sup>",False,0
daylight saving breaks `+minutes()`,blakiseskream,421471669,12,596241454,0,"Adding to this - my pipeline broke last night, this time adding `days(1)` to 2:00 AM on March 7th. I'm not sure if this should output 2:00 AM on March 8th (which ""springs ahead"" to 3:00 AM on March 8th) or 3:00 AM on March 8th.

Funny enough, it only seems to affect the `2:00 AM` hour. 

Again reprex:

``` r
library(lubridate)

# 1:59:59 AM
start_date <- ymd_hms('2020-03-07 01:59:59', tz = 'America/Los_Angeles')
start_date + days(1)
#> [1] ""2020-03-08 01:59:59 PST""

# 2:00:00 AM
start_date <- ymd_hms('2020-03-07 02:00:00', tz = 'America/Los_Angeles')
start_date + days(1)
#> [1] NA

# 2:59:59 AM
start_date <- ymd_hms('2020-03-07 02:59:59', tz = 'America/Los_Angeles')
start_date + days(1)
#> [1] NA

# 3:00:01 AM
start_date <- ymd_hms('2020-03-07 03:00:01', tz = 'America/Los_Angeles')
start_date + days(1)
#> [1] ""2020-03-08 03:00:01 PDT""
```

<sup>Created on 2020-03-08 by the [reprex package](https://reprex.tidyverse.org) (v0.3.0)</sup>

Upon reflection here, I'll just not base things on the `2:00 AM` hour
",False,0
daylight saving breaks `+minutes()`,vspinu,421471669,13,596247576,0,"The logic here is exactly as with `ymd(""2009-01-29"") + months(1)` which results in `NA`. I am seriously considering changing this unfortunate default to a more desirable behavior, and leaving the current as an opt-in. ",False,0
daylight saving breaks `+minutes()`,robertwwalker,421471669,14,813102778,0,"Just wanted to reup this.  @pitakakariki pointed out, perhaps, a more troublesome case.  NA seems like a fine result because it flags troubles.  The other direction problem generates an answer that is plainly incorrect.

![image](https://user-images.githubusercontent.com/22829624/113522079-b31fe180-9552-11eb-9aa9-8e974986624c.png)
",False,0
daylight saving breaks `+minutes()`,DavisVaughan,421471669,15,813387353,0,"For anyone doing arithmetic with _hours, minutes, or seconds_ with lubridate, you likely want to use Durations objects (like `dhours()`) rather than Period objects (like `hours()`). For arithmetic with more granular units of time like days, months or years, you probably want to stick with Period objects.

So, if using lubridate, my advice for @Demetrio92 and @robertwwalker is to switch to `dminutes()` and `dhours()` as that seems to be what you are expecting. That will match your intuition if you were expecting a result equal to ""sitting in a chair for 1160 minutes, then looking at the clock"" (assuming that your clock automatically adjusts itself for DST).

You could also try using [clock](https://clock.r-lib.org/). The POSIXct API in clock defaults to the behavior I described in the first paragraph, so it might feel more intuitive. It also errors when landing on a nonexistent time in a DST gap, or on an ambiguous time in a DST fallback, rather than returning `NA`.

@Demetrio92's original example:

```r
library(clock)
library(lubridate)
library(magrittr)

# This example has to do with DST Gaps (nonexistent times)
x <- as.POSIXct(""2019-03-09 07:22:03"", tz = ""US/Pacific"")
x
#> [1] ""2019-03-09 07:22:03 PST""

# This is lubridate Period addition:
# - converts to naive (2019-03-09 07:22:03, with no assumed time zone)
# - adds 1160 minutes (2019-03-09 07:22:03 -> 2019-03-10 02:42:03)
# - converts back to US/Pacific, but this is now in a DST gap from
#   01:59:59 -> 03:00:00, so lubridate returns NA
x + minutes(1160)
#> [1] NA

# clock - With POSIXct, hours, minutes, and seconds add like lubridate Durations
add_minutes(x, 1160)
#> [1] ""2019-03-10 03:42:03 PDT""

# so clock matches this result of lubridate Duration arithmetic:
x + dminutes(1160)
#> [1] ""2019-03-10 03:42:03 PDT""

# To get the lubridate Period behavior in clock if you want it
nt <- x %>%
  as_naive_time() %>%
  add_minutes(1160)

# naive time with no implied time zone - note that this would be in a 
# DST gap if we tried to apply a US/Pacific zone to it
nt
#> <time_point<naive><second>[1]>
#> [1] ""2019-03-10 02:42:03""

# unlike lubridate, clock treats this as an error rather than returning NA
as.POSIXct(nt, tz = date_zone(x))
#> Error: Nonexistent time due to daylight saving time at location 1. Resolve nonexistent time issues by specifying the `nonexistent` argument.

# control how to handle this nonexistent time with `nonexistent`,
# use `""NA""` to match lubridate behavior
as.POSIXct(nt, tz = date_zone(x), nonexistent = ""roll-forward"")
#> [1] ""2019-03-10 03:00:00 PDT""
as.POSIXct(nt, tz = date_zone(x), nonexistent = ""roll-backward"")
#> [1] ""2019-03-10 01:59:59 PST""
as.POSIXct(nt, tz = date_zone(x), nonexistent = ""NA"")
#> [1] NA
```

@robertwwalker's example:

```r
library(clock)
library(lubridate)
library(magrittr)

# This example has to do with DST Fallbacks (ambiguous times)
x <- as.POSIXct(""2021-04-04 02:15:00"", tz = ""Pacific/Auckland"")
x
#> [1] ""2021-04-04 02:15:00 NZDT""

# This is lubridate Period addition:
# - converts to naive (2021-04-04 02:15:00, with no assumed time zone)
# - adds one hour (02 -> 03)
# - converts back to Pacific/Auckland with no issues, but the result is
#   potentially not intuitive
x + hours(1)
#> [1] ""2021-04-04 03:15:00 NZST""

# clock - With POSIXct, hours, minutes, and seconds add like lubridate Durations
add_hours(x, 1)
#> [1] ""2021-04-04 02:15:00 NZST""

# so clock matches this result of lubridate Duration arithmetic:
x + dhours(1)
#> [1] ""2021-04-04 02:15:00 NZST""

# To get lubridate Period behavior in clock if you want it
x %>%
  as_naive_time() %>% # this is the key
  add_hours(1) %>%
  as.POSIXct(tz = date_zone(x))
#> [1] ""2021-04-04 03:15:00 NZST""
```",False,0
daylight saving breaks `+minutes()`,Demetrio92,421471669,16,813461646,0,"@DavisVaughan this is awesome! I'm glad there is a reliable fix. 

However: `dminutes` should not act different from `minutes`. This is confusing at best. 

Better: both act the same with a parameter `nonexistent` changing the behavior. 

Long run: deprecate usage of `minutes` as it involves a lot of autocasting under the hood and will inevitably lead to various issues.


-------

Imagine two programmers wrote together a lot of R code on one project. One of them used `minutes` the other one `dminutes`. Some parts of the code produce NAs while other parts don't. Happy debugging. ",False,0
daylight saving breaks `+minutes()`,DavisVaughan,421471669,17,813464536,0,"> However: dminutes should not act different from minutes. This is confusing at best.

I disagree. The whole reason there are two functions for adding minutes is that they work differently. I would encourage you to read the docs for [Durations](https://lubridate.tidyverse.org/reference/duration.html) and [Periods](https://lubridate.tidyverse.org/reference/period.html) again, as they are quite different.",False,0
daylight saving breaks `+minutes()`,Demetrio92,421471669,18,813474969,0,"> > However: dminutes should not act different from minutes. This is confusing at best.
> 
> I disagree. The whole reason there are two functions for adding minutes is that they work differently. I would encourage you to read the docs for [Durations](https://lubridate.tidyverse.org/reference/duration.html) and [Periods](https://lubridate.tidyverse.org/reference/period.html) again, as they are quite different.

Reading `period` docs
> Within a Period object, time units do not have a fixed length (except for seconds) until they are added to a date-time.  ... When math is performed with a period object, each unit is applied separately. How the length of a period is distributed among its units is non-trivial. For example, when leap seconds occur 1 minute is longer than 60 seconds.

> Periods track the change in the ""clock time"" between two date-times. They are measured in common time related units: years, months, days, hours, minutes, and seconds. Each unit except for seconds must be expressed in integer values.

I do not  see why this description contradicts my expected output. 

Specifically it says stuff like 
>or example, when leap seconds occur 1 minute is longer than 60 seconds. 

My impression here would be that periods are more robust than durations. 

-----

I generally agree, the issue is more deep than just two functions that supposed to do the same. 

Still, worst case, rename `minutes` into `pminutes`. 

----

I am glad `period` now contains a warning. At least people who carefully study the docs will be aware. 
> Note: Arithmetic with periods can result in undefined behavior when non-existent dates are involved (such as February 29th in non-leap years). Please see Period for more details and %m+% and add_with_rollback() for alternative operations.

Coming from postgres I just did `+minutes()` and it worked as expected without any issues until it didn't. I wasn't even aware there were two ways of doing this in `lubridate`
",False,0
daylight saving breaks `+minutes()`,Demetrio92,421471669,19,813475352,0,"TL;DR 

`ymd(""2009-01-29"") + months(1)` this should probably go to the top of the demos in both docs. Highlights the difference very well. ",False,0
daylight saving breaks `+minutes()`,robertwwalker,421471669,20,813489326,0,@DavisVaughan Thank you very much.,False,0
daylight saving breaks `+minutes()`,pitakakariki,421471669,21,813664174,0,"Maybe worth noting that this behaviour breaks the reversibility rule:

```
x <- as.POSIXct(""2021-04-04 02:15:00"", tz = ""Pacific/Auckland"")
x
#  [1] ""2021-04-04 02:15:00 NZDT""

x + hours(1)
#  [1] ""2021-04-04 03:15:00 NZST""

x + hours(1) - hours(1)
#  [1] ""2021-04-04 02:15:00 NZST""
```",False,0
daylight saving breaks `+minutes()`,pitakakariki,421471669,22,814552874,0,"I've been thinking (too much probably) about the period-duration distinction.

Imagine the (completely implausible I hope) scenario where the time-lords decide that they're going to do a leap second at the same time as a daylight savings change.

Here's how I'd expect `hours` and `dhours` to act:

    ""2021-04-04 02:15:00 NZDT"" + hours(1) == ""2021-04-04 02:15:00 NZST""

    ""2021-04-04 02:15:00 NZDT"" + dhours(1) == ""2021-04-04 02:14:59 NZST""

On the other hand when it comes to `days` I have no idea what I should even expect. It doesn't appear to be reversible currently though:

```
> as.POSIXct(""2021-04-04 02:15:00"", tz = ""Pacific/Auckland"")
[1] ""2021-04-04 02:15:00 NZDT""
> 
> as.POSIXct(""2021-04-04 02:15:00"", tz = ""Pacific/Auckland"") + days(1)
[1] ""2021-04-05 02:15:00 NZST""
> 
> as.POSIXct(""2021-04-04 02:15:00"", tz = ""Pacific/Auckland"") + days(1) - days(1)
[1] ""2021-04-04 02:15:00 NZST""
```

I'd also like to add that all of this is really annoyingly complex and I'm really glad there are people who are not me making it easier to deal with. So a big thank you to everyone involved in lubridate development.",False,0
daylight saving breaks `+minutes()`,DavisVaughan,421471669,23,814880472,0,"> Here's how I'd expect hours and dhours to act:

Hmm, @pitakakariki from this example it looks to me like you might misunderstand how Period and Duration are supposed to be working. Let me try explaining. I'll use types from clock, since they allow me to explicitly show the intermediate steps that are happening under the hood.

``` r
library(clock)

x <- date_time_parse(""2021-04-04 02:15:00"", ""Pacific/Auckland"", ambiguous = ""earliest"")
x
#> [1] ""2021-04-04 02:15:00 NZDT""

# Adding a lubridate ""Period"" is like:
# - Dropping the original time zone completely
# - Adding the unit of time
# - Adding the original time zone back (if possible)

# Adding a lubridate ""Duration"" is like:
# - Converting the original time zone to UTC (where DST never affects you)
# - Adding the unit of time
# - Adding the original time zone back (which is always possible)

# We can show these explicitly with clock's naive-time and sys-time types, which
# mimic dropping the original time zone and converting to UTC respectively.
```

``` r
## Period example:

# Notice this has no implied zone attached
nt <- as_naive_time(x)
nt
#> <time_point<naive><second>[1]>
#> [1] ""2021-04-04 02:15:00""

# Now we add the time. Since there is no implied time zone, this doesn't
# have to do with the DST fallback whatsoever.
nt_plus_hour <- nt + duration_hours(1)
nt_plus_hour
#> <time_point<naive><second>[1]>
#> [1] ""2021-04-04 03:15:00""

# Now we convert back to Pacific/Auckland. Since the 3 o'clock hour is past
# the DST fallback, there is no ambiguity in the conversion, but it may or
# may not be what you want.
as.POSIXct(nt_plus_hour, ""Pacific/Auckland"")
#> [1] ""2021-04-04 03:15:00 NZST""

x + lubridate::hours(1)
#> [1] ""2021-04-04 03:15:00 NZST""
```

``` r
## Duration example:

# This is in UTC time, notice how the hour has shifted
st <- as_sys_time(x)
st
#> <time_point<sys><second>[1]>
#> [1] ""2021-04-03 13:15:00""

# Now we add the time in UTC. DST can never affect this.
st_plus_hour <- st + duration_hours(1)
st_plus_hour
#> <time_point<sys><second>[1]>
#> [1] ""2021-04-03 14:15:00""

# Now convert back to Pacific/Auckland. This just shifts by the appropriate
# UTC offset to get the Pacific/Auckland time.
as.POSIXct(st_plus_hour, ""Pacific/Auckland"")
#> [1] ""2021-04-04 02:15:00 NZST""

x + lubridate::dhours(1)
#> [1] ""2021-04-04 02:15:00 NZST""
```

<sup>Created on 2021-04-07 by the [reprex package](https://reprex.tidyverse.org) (v1.0.0)</sup>",False,0
daylight saving breaks `+minutes()`,pitakakariki,421471669,24,815393608,0,"Thanks @DavisVaughan but I think we might be talking at cross purposes. Your comment helps explain the intuition behind the implementation, but my expectations come from the reversibility principal in the documentation.

If adding `hours(1)` to 2:15am DT and 2:15am ST both give us 3:15am ST, then there's no way to subtract `hours(1)` from 3:15am ST that's consistent with reversibility.",False,0
daylight saving breaks `+minutes()`,DavisVaughan,421471669,25,815813891,0,"As Vitalie mentioned here https://github.com/tidyverse/lubridate/issues/759#issuecomment-536286765, the reversibility rule is probably not the best idea for date time arithmetic, even if it sounds like a good idea (I agree with him now). So I wouldn't base too many assumptions on it.",False,0
daylight saving breaks `+minutes()`,pitakakariki,421471669,26,815983690,1,"@DavisVaughan this is an issue tracker, not a help forum. An inconsistency between behaviour and documentation is a bug, which is why I'm reporting these examples. Your insistence that this is somehow a ""misunderstanding"" on my part is starting to feel quite rude.
",False,Impatience
Video width and height are switched when a vertically-oriented game is loaded,Patrickdroid,430055555,1,430055555,0,"## Description

With vertically oriented games, Retroarch switches the emulator-provided width and height. For example, if the width/height is supposed to be 320x240, RA will switch this to 240x320. This only occurs with vertically oriented games. I tested this in both MAME and FBA using the Windows 64 bit version of RA. 

### Expected behavior

The game to be displayed using the correct resolution (width/height). 

### Actual behavior

The height and width is switched when a vertically oriented game is loaded. 

### Steps to reproduce the bug

1. load any vertically oriented game in either FBA or MAME using Windows 64 bit version of RA (example: DonPachi)
2. under video settings, enable integer scale. 
3. set ""custom aspect ratio width/height"" to 1x.
4. look at ""custom aspect ratio width/height"" to see that it says 240 for width and 320 for height. This is the reverse of what it should be; it should be 240 for height and 320 for width. 

### Bisect Results

I first noticed this a few days ago. 

### Version/Commit
You can find this information under Information/System Information

- RetroArch: 1.7.6
Git version: 9750719074

### Environment information

- OS: Windows 10 
- Compiler: NA

![image](https://user-images.githubusercontent.com/12267058/55672818-c2cd9800-585c-11e9-8ba2-0fd5024aee7c.png)

![image](https://user-images.githubusercontent.com/12267058/55672822-d7aa2b80-585c-11e9-8747-b31df5c07da0.png)

",True,0
Video width and height are switched when a vertically-oriented game is loaded,ghost,430055555,2,480553748,0,some arcades have the monitor rotated 90 degrees. The  can the emulator can either rotate the image to the displays orientation or display it as intended and you rotated your display 90 degrees,False,0
Video width and height are switched when a vertically-oriented game is loaded,Patrickdroid,430055555,3,480587874,0,"I understand that, but the emulator-provided resolution should not be changed from the correct resolution, regardless of orientation. If the game is 320x240 it should be output as 320x240, not 240x320. The user can flip it or rotate it or resize it or whatever, but the output resolution from the emulator should remain the same (eg, 320x240 should be output as 320x240 before the user does stuff to it). Otherwise you get scaling artifacts unless you just happen to adjust the y axis to a multiple of both 240 and 320 (eg, 960). And if you‚Äôre playing a game that uses a weird resolution like 19XX (384x240), then it causes big problems with overscan or underscan when using integer scaling. As of right now, video aspect ratio for vertical games must be configured manually (custom ratio width/height, non-integer) along with custom ratio x/y position, because the resolution/ratio for vertical games isn‚Äôt being detected correctly. ",False,0
Video width and height are switched when a vertically-oriented game is loaded,ghost,430055555,4,480592063,0,"the thing is the topic say the width and height are switched they simply arent it just a rotated monitor. It draws the image on the monitor as 4:3 then you flip the monitor physically this changes it to 3:4 on real hardware as far as the hardware is concerned its a normal 4:3 device but your rendering the image 90 degrees rotated.

In mame2003 or plus just use tate mode this stop the rotation and you need to flip your monitor like a real one. look at sf2 this is on a 4:3 screen look at the resolution of the game it use crt timing when rendering to look the way it does in a crt if you used the literal resolution for sf2 the gfx would be overly stretched. 

",False,0
Video width and height are switched when a vertically-oriented game is loaded,ghost,430055555,5,480592631,0,"it looks like your talking about capcom games watch this to understand whats going on and it is indeed 4:3

https://www.youtube.com/watch?v=LHfPA4n0TRo&feature=youtu.be",False,0
Video width and height are switched when a vertically-oriented game is loaded,Patrickdroid,430055555,6,480595131,0,"I‚Äôm not referring exclusively to capcom games. You can see this behavior in the example screenshots I posted above, which are from Donpachi. 

I understand very well that certain games are meant to be displayed vertically. This doesn‚Äôt change anything that I‚Äôve said. 

With a vertically-oriented 320x240 game, width should say 320 and height should say 240, and the displayed image should be sideways. On a 4:3 CRT monitor the image would be 320x240 and sideways. You then rotate the monitor 90 degrees. This doesn‚Äôt change the image to 240x320. It is still 320x240 but now it is rotated 90 degrees so it is no longer sideways.

Altering the internal resolution of the game is incorrect, and will always result in scaling artifacts unless you happen to use a resolution that is a multiple of both 320 and 240. 

",False,Bitter frustration
Video width and height are switched when a vertically-oriented game is loaded,Patrickdroid,430055555,7,480595698,0,If you display the arcade pcb to a CRT monitor in normal orientation it will be a 320x240 image that is sideways. You rotate the CRT 90 degrees. The resolution of the game remains unchanged.,False,0
Video width and height are switched when a vertically-oriented game is loaded,ghost,430055555,8,480596063,0,yes resolution of you display remains unchanged. I dont have a crt to test whats going on there I would assume you should be using core provided information and tate mode on for mame2003 and plus dont know if the others support tate mode,False,0
Video width and height are switched when a vertically-oriented game is loaded,Patrickdroid,430055555,9,480596550,1,You just aren‚Äôt getting what I‚Äôm saying. I‚Äôm going to have to post some screenshots later to help illustrate what‚Äôs going on currently and what should be going on instead.,False,Bitter frustration
v3.0.19 fails to build with libressl,Zirias,432631890,1,432631890,0,"FreeRADIUS 3.0.19 doesn't build with libressl 2.8.3 any more, it shows the following build warnings:

    src/main/tls.c:3286:33: warning: incompatible pointer types passing 'SSL_SESSION 
     *(SSL *, unsigned char *, int, int *)' (aka 'struct ssl_session_st *(struct ssl_st *, unsigned char *, int, int *)') to parameter of type 'SSL_SESSION *(*)(struct ssl_st *, const unsigned char *, int, int *)' (aka 'struct ssl_session_st *(*)(struct ssl_st *, const unsigned char *, int, int *)') [-Wincompatible-pointer-types]
                            SSL_CTX_sess_set_get_cb(ctx, cbtls_get_session);
                                                         ^~~~~~~~~~~~~~~~~         
    /usr/local/include/openssl/ssl.h:730:20: note: passing argument to parameter 'get_session_cb' here
        SSL_SESSION *(*get_session_cb)(struct ssl_st *ssl,
                       ^
    src/main/tls.c:3383:3: warning: implicit declaration of function 'SSL_CTX_set_num_tickets' is invalid in C99 [-Wimplicit-function-declaration]
                    SSL_CTX_set_num_tickets(ctx, 1);
                    ^
    src/main/tls.c:3396:3: warning: implicit declaration of function 'SSL_CTX_set_num_tickets' is invalid in C99 [-Wimplicit-function-declaration]
                    SSL_CTX_set_num_tickets(ctx, 0);
                    ^

This leads to this linker error:

    LINK build/bin/radiusd
    /usr/bin/ld: error: undefined symbol: SSL_CTX_set_num_tickets
    >>> referenced by tls.c
    >>>               build/objs/src/main/tls.o:(tls_init_ctx)

See also [FreeBSD Bug 237216](https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=237216)

The following simple patch fixes the build:

    --- src/main/tls.c.orig        2019-04-12 06:47:56 UTC
    +++ src/main/tls.c
    @@ -1579,7 +1579,7 @@ done:
           return 0;
     }
     
    -#if OPENSSL_VERSION_NUMBER < 0x10100000L || defined(LIBRESSL_VERSION_NUMBER)
    +#if OPENSSL_VERSION_NUMBER < 0x10100000L
     static SSL_SESSION *cbtls_get_session(SSL *ssl, unsigned char *data, int len, int *copy)
     #else
     static SSL_SESSION *cbtls_get_session(SSL *ssl, const unsigned char *data, int len, int *copy)
    @@ -3379,14 +3379,14 @@ post_ca:
                    */
                   SSL_CTX_sess_set_cache_size(ctx, conf->session_cache_size);
     
    -#if OPENSSL_VERSION_NUMBER >= 0x10101000L
    +#if OPENSSL_VERSION_NUMBER >= 0x10101000L && !defined(LIBRESSL_VERSION_NUMBER)
                   SSL_CTX_set_num_tickets(ctx, 1);
     #endif
     
           } else {
                   SSL_CTX_set_session_cache_mode(ctx, SSL_SESS_CACHE_OFF);
     
    -#if OPENSSL_VERSION_NUMBER >= 0x10101000L
    +#if OPENSSL_VERSION_NUMBER >= 0x10101000L && !defined(LIBRESSL_VERSION_NUMBER)
                   /*
                    *      This controls the number of stateful or stateless tickets
                    *      generated with TLS 1.3.  In OpenSSL 1.1.1 it's also
",True,0
v3.0.19 fails to build with libressl,arr2036,432631890,2,482639056,0,Please submit patches that specify specific libressl version numbers. libressl 2.9.0 appears to add API compatibility with OpenSSL 1.1.0 so just checking for the presence or absence of LIBRESSL_VERSION_NUMBER is not an accurate method of determining what functionality libressl supports.,False,0
v3.0.19 fails to build with libressl,Zirias,432631890,3,482648741,0,"It isn't that simple. The two checks I added are about OpenSSL 1.1.1, there is no libressl version yet that's compatible with that. So there's no way to do these checks for specific version numbers.

Leaves me with the one I removed; this could indeed be replaced with a specific version -- I assume libressl did that API change in version 2.8.0, but I'm not sure about it.",False,0
v3.0.19 fails to build with libressl,arr2036,432631890,4,482780586,0,"Then add a note against the preprocessor checks stating that OpenSSL 1.1.1 wasn't supported as of version 2.9.0.  Be advised if you want to keep these checks in v3.0.x, you should also do the same for the existing uses of LIBRESSL_VERSION_NUMBER, otherwise, libressl support in v3.0.x will go the way it just did in master branch (9652affe38f41ba2484e013cf9d2c0bcb8c80d67).",False,0
v3.0.19 fails to build with libressl,Zirias,432631890,5,482785417,0,"So you took this issue as an opportunity for dropping libressl support? IMHO, using libressl version numbers isn't going to work out well, because libressl follows the OpenSSL API and defines the OPENSSL_VERSION_NUMBER macros accordingly -- incompatibilities still happen, and there might be OpenSSL API changes that aren't supported by libressl at all yet (as those for 1.1.1). What's typically done is adding just a few checks for LIBRESSL_VERSION_NUMBER being defined, so the *current stable* version of libressl is supported. This is normally enough for anyone building from source and for projects like OpenBSD or void linux that use libressl as system-wide replacement.

But of course, if you decide to drop libressl support, this renders this issue invalid.",False,0
v3.0.19 fails to build with libressl,arr2036,432631890,6,482790016,0,"I don't see your point.  The current stable version of libressl is not fixed, it's going to change, and when it does, we will need to re-evaluate which APIs we use when linked with libressl. Without version numbers and comments we have absolutely no idea against which version of libressl this evaluation was last performed against, and no idea when we can strip out old compatibility code. 

It's not that we don't want to support libressl, it's that we resent the poor quality of patches which have been submitted so far to support it.  Or, to be more explicit, continuing to support Theo de Raadt's little ego trip is fine, but it should not come at the expense of additional effort for our development team.",False,Bitter frustration
v3.0.19 fails to build with libressl,alandekok,432631890,7,482790056,1,"> So you took this issue as an opportunity for dropping libressl support? 

Don't be an asshole.

This is open source software.  You're welcome to contribute.  If you're too lazy to contribute a decent patch, then don't complain that we're too lazy to jump to your every whim.",False,Vulgarity
Rename 'kick' as 'remove from chat',ara4n,436303024,1,436303024,0,as 'kick' is needlessly aggressive,True,0
Rename 'kick' as 'remove from chat',BurnyBoi,436303024,2,487068460,0,"It hasn't been considered aggressive for the decades it's been an IRC command, so why would it be now? Put a nice message in the reason field if you don't want to hurt their feelings. It's not like you're actually physically kicking them...",False,Bitter frustration
Rename 'kick' as 'remove from chat',jryans,436303024,3,487124637,0,"We have the option to be less aggressive, so let's go ahead and do that. There's no need to comply with the idiosyncratic norms of IRC culture in the labelling of UI features.",False,0
Rename 'kick' as 'remove from chat',BurnyBoi,436303024,4,487154478,0,"My main point was that it's not aggressive at all. Being ""kicked out"" of somewhere is a common term that is mutually understood to mean you were removed. Removing someone from a room against their will is going to potentially upset them regardless of what you call it. 

I don't really care what the UI says, but please keep the /command as ""/kick"" for the sake of familiarity and efficiency. ",False,0
Rename 'kick' as 'remove from chat',VindictiveViolator,436303024,5,487168565,1,"Um, excuse me, sweetie, this is 2019Ôªø...

'ban' as in 'banish' is also unnecessarily aggressive and totally inappropriate if you consider African American history. It should be renamed to 'Adorn Chastity Device' instead.",False,Irony
BUG: padding must not be negative,Tectract,483355054,1,483355054,0,"https://github.com/kaminari/kaminari/blob/16c0d83c69d6e498c6fe11777d6757b3cf39222d/kaminari-core/lib/kaminari/models/page_scope_methods.rb#L29

I was using negative padding, and I need it. Please change this to be a warning at most, not a halting error.

",True,0
BUG: padding must not be negative,Tectract,483355054,2,523414694,0,"Confirmed, my app works again after version-locking down to 0.17.0...

Here is the offending commit: 
https://github.com/kaminari/kaminari/commit/21baf16c0401dea1b6477a457aca5936237f4a56#diff-d90e018990d8862b642722642018c5af

This seems to have been done with no rational given (?)...

The usage case here is when I have 3 articles listed on the front page, but then I have links generated via link_to_next_page, and I want it to load 6 at a time, starting at the 4th article on the first click. So then I have a page size of 6 and and offset of NEGATIVE 3!!!


",False,0
BUG: padding must not be negative,yuki24,483355054,3,523504400,0,"The whole context could be found here: https://github.com/kaminari/kaminari/issues/839. Have you looked at it? Also, this is not a bug but a feature request. It'd be greatly appreciated if you could send a PR.",False,0
BUG: padding must not be negative,Tectract,483355054,4,523612951,0,"That's all fine and dandy, that it can theoretically return no objects in some situations, but there's no reason you should have broken my completely working app. I mean, isn't that a perfectly fine ""fail silently"" or maybe a ""warn"" situation? I don't see the need for a halting error to be raised here. I'm version-locked to 0.17.0 because of this, now...

To me, it is a bug. The feature of being able to use negative padding has been broken.

 raise ArgumentError, ""padding must not be negative"" if num < 0

should be changed to:

 puts ""warning: pagination padding is negative""

or something like that. 

",False,Bitter frustration
BUG: padding must not be negative,yuki24,483355054,5,523680336,0,"Regardless of this ticket being a feature request or a bug report, any contribution or PR is greatly appreciated. However, just removing it does sound like a one-sided opinion given that [all the three DBs don't support negative `OFFSET` properly](https://github.com/kaminari/kaminari/issues/839#issuecomment-268211318) and we probably can not accept it. We would like a negative padding feature that consistently works with different databases and paginatable arrays.

Keep in mind that PostgreSQL and MySQL do not accept negative `OFFSET` values and SQLite just ignores it. We would also have to ensure compatibility with the built-in paginatable array and `kaminari-mongoid` if possible. The intension of raising an error is to prevent people from running into inconsistent behaviour (see [this comment for more details](https://github.com/kaminari/kaminari/issues/839#issuecomment-268211318)).",False,0
BUG: padding must not be negative,Tectract,483355054,6,523734517,1,"This doesn't really make sense to me. I have a perfectly working app and this change breaks it. It's a BUG. I'm not concerned with the fact that sometimes using negative padding can result in no rows being returned, in fact I would say that is the EXPECTED behavior, and that is perfectly normal when you use it *improperly*. I don't see why a warning would not suffice here.

I don't understand why you INSIST that my app must HALT AND STOP WORKING COMPLETELY because I used a negative number you didn't like. 

Good developers should give their users MORE options, instead of TAKING AWAY options for usability because they went slightly outside how the developer anticipated the software being used.

Why would I submit a PR to FIX your BROKEN pagination tool, after you've already said you won't accept it, for some reason which makes no sense to me.
",False,Impatience
[RFC] Add autoloader root directory config,marekdedic,510675125,1,510675125,0,"## Current behaviour
`composer.json`:

```json
""config"": {},
""autoload"": {
    ""psr-4"": {
        ""MyApp\\"": ""src/php/MyApp/""
    }
},
```

Gives the following `vendor/composer/autoload_psr4.php`:
```php
<?php

// autoload_psr4.php @generated by Composer

$vendorDir = dirname(dirname(__FILE__));
$baseDir = dirname($vendorDir);

return array(
    'MyApp\\' => array($baseDir . '/src/php/MyApp')
);
```

## Proposed behaviour

`composer.json`:

```json
""config"": {
    ""autoloader-root-dir"": ""src/php/""
},
""autoload"": {
    ""psr-4"": {
        ""MyApp\\"": ""MyApp/""
    }
},
```

Would give the following `vendor/composer/autoload_psr4.php`:
```php
<?php

// autoload_psr4.php @generated by Composer

$vendorDir = dirname(dirname(__FILE__));
$baseDir = dirname($vendorDir);

return array(
    'MyApp\\' => array($baseDir . '/MyApp')
);
```
I.e. the generated file would look *as if* `composer.json` were in the `src/php/` directory. Only descending the folder tree would be allowed, so you cannot do something like `""autoloader-root-dir"": ""../somewhereElse/""`. The name `autoloader-root-dir` is tentative.

## Motivation

### Built applications

In modern web development, codebases quite often use multiple languages, some of them compiled (typically something akin to TypeScript compiled to JavaScript). This means that the final application needs to be built. To cleanly separate the source files from the build artifacts, one would want to place all the source files into one directory (e. g. `src`) and have the built app in another one (e. g. `dist`). However, the `composer.json` file would typically be in the root of the repository - that is, *not* in the `src` folder. Under the current behaviour, there is no way to do this, not have the final php files somewhere like `dist/src/` and have a working autoloader. This RFC would enable that by setting `""autoloader-root-dir"": ""src/""`.

### Separating configuration files from the application itself

When an app is deployed, sometimes it is undesirable to deploy configuration files such as `.travis-ci.yml` or similar. With this RFC, one could put the actual application in a dedicated directory (e. g. `app/`), keep all the configuration files (including `composer.json`) in the root of the repo and still have a working autoloader by setting `""vendor-dir"": ""app/vendor""` and `""autoloader-root-dir"": ""app/""`.

This issue is a continuation of the recent debate in #5198. I would be willing to take a try at a PR, if the feature is deemed appropriate for inclusion in composer.",True,0
[RFC] Add autoloader root directory config,stof,510675125,2,544980563,0,"For PHP 5.6+, the relevant file for the autoloader is not `autoload_psr4.php` but `autoload_static.php` which makes such that no variables are ever used in the config array so that the array of paths can stay in the OPCache shared memory.
And IIRC, composer uses the shortest relative path possible for that. So with your own code in `app/MyApp` and the vendors in `app/vendor`, `autoload_static.php` may already have the expected `../MyApp` path (to be confirmed). So the second case my already be solved.",False,0
[RFC] Add autoloader root directory config,marekdedic,510675125,3,544996950,0,"Thanks, I didn't know that.

For me the main use case is the first one, however the second one was mentioned by some people in the linked issue. If it turns out to be solved already, that's great.",False,0
[RFC] Add autoloader root directory config,Seldaek,510675125,4,545872309,0,"What if you set vendor-dir to `src/vendor`? then the relative path should be ../MyApp, and if you then copy src/vendor to dist/vendor it should still work.

I'd rather avoid adding a new parameter for such a custom use case if it can be achieved with existing infrastructure.",False,0
[RFC] Add autoloader root directory config,Seldaek,510675125,5,545873149,0,"Hmm yeah no sorry re-reading the description I guess this isn't gonna work. I am not sure I fully understand what you are trying to achieve though, more details would be good to really see what the point is and whether it's something we want to support if it isn't yet.",False,0
[RFC] Add autoloader root directory config,alcohol,510675125,6,547304941,0,"I find the arguments rather weak and lacking. In ""modern"" web development, one deploys build artifacts, not entire git repositories. So the structure of your git repository is irrelevant. This immediately solves both ""issues"" (separation of source and distribution files, and config files from build services and such).",False,Insulting
[RFC] Add autoloader root directory config,marekdedic,510675125,7,576660736,0,"> And IIRC, composer uses the shortest relative path possible for that. So with your own code in `app/MyApp` and the vendors in `app/vendor`, `autoload_static.php` may already have the expected `../MyApp` path (to be confirmed). So the second case my already be solved.

I've tried it and can confirm that that works. That leaves the first issue which I would like to solve

I have a git repo of the following:
```
‚îú‚îÄ‚îÄ composer.php
‚îú‚îÄ‚îÄ src
‚îÇ   ‚îú‚îÄ‚îÄ php
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ index.php
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ MyApp
‚îÇ   ‚îÇ         ‚îî‚îÄ‚îÄ MyClass.php
‚îÇ   ‚îî‚îÄ‚îÄ js
‚îÇ         ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ vendor
      ‚îî‚îÄ‚îÄ ...
```

I would like to create (with any build system, I don't really care) a `dist` directory which I could upload to a webhosting (it being a build artifact as @alcohol pointed out). I imagined it being of the sort:

```
‚îú‚îÄ‚îÄ index.php
‚îú‚îÄ‚îÄ MyApp
‚îÇ   ‚îî‚îÄ‚îÄ MyClass.php
‚îú‚îÄ‚îÄ js
      ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ vendor
      ‚îî‚îÄ‚îÄ ...
```

But, by now, the path from `vendor/composer/autoload_static.php` to `MyApp/MyClass.php` has changed. So the autoloading would no longer work. But with the proposed new config parameter, I could set 
```json
""config"": {
    ""autoloader-root-dir"": ""src/php/""
}
```
and then autoloading would work in the build artifact directory.

Why not just use `vendor-dir` then? Because that would require me putting the `vendor` dir in `src/php`. But the `vendor` dir **is not** source code for me - therefore, I wouldn't want it in the `src` directory.

Clearer now? I think the exact organization of the `dist` directory could be different a bit, but the overall idea would stay the same. I'd like to hear your thoughts on this.",False,0
[RFC] Add autoloader root directory config,Seldaek,510675125,8,576689294,0,"What you need to configure here is your webhosting's webroot to point to `src/php/` so it can find index.php at the root, but vendors are *not* present within the webroot. This is a best practice as having vendors in the webroot mean people can easily access your dependencies which might reveal vulnerable software etc.",False,0
[RFC] Add autoloader root directory config,marekdedic,510675125,9,577366624,0,"> This is a best practice as having vendors in the webroot mean people can easily access your dependencies which might reveal vulnerable software etc.

Thanks for the advice, I did not know that.

However, unfortunately, this is not a good solution on its own - I can't set the webroot to `src/php` as the files in `src` are sources - not build artifacts. And the web/app needs to run from the build artifacts, not the sources... Any other ideas? Solving this without needing to add anything would of course be preferable...

",False,0
[RFC] Add autoloader root directory config,Seldaek,510675125,10,578046989,0,"Then put the build artifacts wherever you need, and make sure that's the web root, and that the web root contains an index.php which includes whatever path is needed so it finds vendor dir etc, even tho that should be in a separate directory.

For example [Symfony's standard distro](https://github.com/symfony/symfony-standard) has a web dir with app.php being the index file/front controller, you can see what's in the web dir, it should be only things that should be accessible by user browsers. And the web root should be web/",False,0
[RFC] Add autoloader root directory config,RaymondBakker,510675125,11,586761997,0,It baffles me this hasn't just been added yet. Just give people the flexibility instead of point towards web host configs.,False,Impatience
[RFC] Add autoloader root directory config,alcohol,510675125,12,587342272,1,"Yes, well, luckily we don't have to make decisions based on what baffles some individuals and what not.",False,Mocking
BUG: MeshPhongMaterial flicker with flatShading: false,kpetrow,518811026,1,518811026,0,"Updated from v92 to v110 and have flicker on the edges.  I use PlaneBufferGeometry with null z values to represent unknown elevations(holes).  The sparse grid shows a flicker where the nulls' edges are.  Was not seen with flatShading: true or v92:

![image](https://user-images.githubusercontent.com/18248938/68339808-812af700-00a2-11ea-8f01-ec761d009580.png)

I know you are going to say NaN are not supported in geometries, but any chance you could fix this or point me in proper direction?


##### Three.js version

- [ ] Dev
- [x ] r110
- [ ] ...

##### Browser

- [x] All of them
- [ ] Chrome
- [ ] Firefox
- [ ] Internet Explorer

##### OS

- [ ] All of them
- [x] Windows
- [ ] macOS
- [ ] Linux
- [ ] Android
- [ ] iOS

##### Hardware Requirements (graphics card, VR Device, ...)
",True,0
BUG: MeshPhongMaterial flicker with flatShading: false,kpetrow,518811026,2,550519724,0,"Its something that occurred between 103 and 104.  103 works, 104 flickers.... Looking at change log",False,0
BUG: MeshPhongMaterial flicker with flatShading: false,Mugen87,518811026,3,551033188,0,I'm afraid a screenshot is not sufficient to debug this issue. Please always provide live examples when reporting such problems.,False,0
BUG: MeshPhongMaterial flicker with flatShading: false,kpetrow,518811026,4,551102563,0,"thank you for your help;  Flicker is seen when not moving.

https://jsfiddle.net/b4xzmqcr/

",False,0
BUG: MeshPhongMaterial flicker with flatShading: false,kpetrow,518811026,5,551115996,0,"The issue gets worse when multiple 3js windows are open and active(open the fiddle in 2 windows).  Also noticeably when ""lightShot"" screen capture was opened.  Also noticed when whats app message received.  I have a dev team of 4 guys finding inconsistencies of producing on different graphics cards with different results, but all see a flicker at different times",False,0
BUG: MeshPhongMaterial flicker with flatShading: false,Mugen87,518811026,6,551129187,0,"Sorry, but I can't see any flickering on my system. Tested with Chrome 78.0.3904.97, FF 70.0.1 and macOS 0.14.6. Are you on Windows?",False,0
BUG: MeshPhongMaterial flicker with flatShading: false,kpetrow,518811026,7,551142905,0,"video: Multiple instances of the window seem to be the issue, even with duplicate fiddles it appears.

[here](https://drive.google.com/open?id=1zgscsAoXE1DNdWyIW1qORhXgatCRhKL1)

windows version 1809 OS Build 17763.805
NVIDIA GeForce RTX 2080 SUPER

I have also tested on msi lapiop with NVIDIA GeForce GTX 980M and the problem is probably worse.

All other devs have windows machines, but we could check on a mac if that helps
",False,0
BUG: MeshPhongMaterial flicker with flatShading: false,arpu,518811026,8,551155765,0,"looks like a Problem with the nvideo driver?  i tested this on two Systems, with amd driver (linux) without any problem 


",False,0
BUG: MeshPhongMaterial flicker with flatShading: false,kpetrow,518811026,9,551164574,0,"I haven't gone through the diff of 103-104, but ideas on where to look that would make this happen?

",False,0
BUG: MeshPhongMaterial flicker with flatShading: false,sciecode,518811026,10,551177321,0,"Was able to replicate on Windows with NVidia GPU, but not on Mac with intel integrated GPU. 

Couldn't find anything on the change logs from `r103 -> r104` that would explain that. 

But, then again, you are making use of unsupported behavior, as you said it yourself.",False,0
BUG: MeshPhongMaterial flicker with flatShading: false,kpetrow,518811026,11,551184591,0,Seems very odd that separate instances of three are causing it.  Even though unsupported seems like it could be a canary for a scoping issue?,False,0
BUG: MeshPhongMaterial flicker with flatShading: false,makc,518811026,12,551397312,0,"why not just map NaNs to some big but finite values to move the triangles out of the view. that would mean removing the index attribute as well, however (rough idea: https://jsfiddle.net/uxwqyjvm/ )",False,0
BUG: MeshPhongMaterial flicker with flatShading: false,Mugen87,518811026,13,551619400,0,"In any event, I don't see a `three.js` bug here. I suppose this happens also with pure WebGL and with any other 3D engine so I think it's more correct to close the issue and move the dicussion to stackoverflow or the forum.",False,0
BUG: MeshPhongMaterial flicker with flatShading: false,kpetrow,518811026,14,551764216,0,"@makc We are constantly updating the surface with new known position.  Might be worth testing but would take a considerable amount of work.  If anything this would double the overhead(two geometries needed) of multi-million point grid geometries.  Do you have any docs for ExplodeModifier?  Also we have to have bufferGeometry.  These are very large surface models.

@Mugen87 I do not think that is true because the issue is not seen in r103.  There was a change in the code base that introduced this bug.  If it never worked, then i would agree it is a  webGL/GPU issue, but it was working up to r103 and we have it in production at r92 for a pretty long time(since r92 release)",False,0
BUG: MeshPhongMaterial flicker with flatShading: false,kpetrow,518811026,15,551851903,0,"So there were two shader changes in the revision in the build.  
EDIT

single change then adding that change in so this is messing it up.
sorry my github skills suck , we are svn:

it is here in light_pars_begin lines 1-40:
```

export default /* glsl */`
uniform vec3 ambientLightColor;
uniform vec3 lightProbe[ 9 ];
// get the irradiance (radiance convolved with cosine lobe) at the point 'normal' on the unit sphere
// source: https://graphics.stanford.edu/papers/envmap/envmap.pdf
vec3 shGetIrradianceAt( in vec3 normal, in vec3 shCoefficients[ 9 ] ) {
	// normal is assumed to have unit length
	float x = normal.x, y = normal.y, z = normal.z;
	// band 0
	vec3 result = shCoefficients[ 0 ] * 0.886227;
	// band 1
	result += shCoefficients[ 1 ] * 2.0 * 0.511664 * y;
	result += shCoefficients[ 2 ] * 2.0 * 0.511664 * z;
	result += shCoefficients[ 3 ] * 2.0 * 0.511664 * x;
	// band 2
	result += shCoefficients[ 4 ] * 2.0 * 0.429043 * x * y;
	result += shCoefficients[ 5 ] * 2.0 * 0.429043 * y * z;
	result += shCoefficients[ 6 ] * ( 0.743125 * z * z - 0.247708 );
	result += shCoefficients[ 7 ] * 2.0 * 0.429043 * x * z;
	result += shCoefficients[ 8 ] * 0.429043 * ( x * x - y * y );
	return result;
}
vec3 getLightProbeIrradiance( const in vec3 lightProbe[ 9 ], const in GeometricContext geometry ) {
	vec3 worldNormal = inverseTransformDirection( geometry.normal, viewMatrix );
	vec3 irradiance = shGetIrradianceAt( worldNormal, lightProbe );
	return irradiance;
}
```






",False,0
BUG: MeshPhongMaterial flicker with flatShading: false,kpetrow,518811026,16,551859368,0,"looks like probe light is causing this issue @WestLangley  @sciecode :

Added support for light probes. [#16223](https://github.com/mrdoob/three.js/pull/16223)

Edit as i walk through for future refrence:
1.  Light probe is part of ambient light so removing ambient light fixes the issue. 
2. light probe is a measure of ambient light not a child.  So no ambient light no light probe?
3. Hemisphere lighting does not show flicker
4. Hemisphere white light is super flickery
5.  Looks like the more white the light the worse the flicker, equally bad on hemisphere and ambient.",False,0
BUG: MeshPhongMaterial flicker with flatShading: false,kpetrow,518811026,17,551883083,0,"updated fiddle with super flicker:
https://jsfiddle.net/sxjemofg/1/

turn off ambient light for no flicker",False,0
BUG: MeshPhongMaterial flicker with flatShading: false,kpetrow,518811026,18,551904425,0,"RESOLVED

fixed it.  Is this worth committing?  Someone who knows get and three should commit this üëç @WestLangley 
@Mugen87 


```
vec3 shGetIrradianceAt( in vec3 normal, in vec3 shCoefficients[ 9 ] ) {
	// normal is assumed to have unit length
	float x = normal.x, y = normal.y, z = normal.z;
	// band 0
	vec3 result = shCoefficients[ 0 ] * 0.886227;

      //FIX
      // in webgl 2.0 this could be replaced with  isnan(val)
      if(
              (! ( x < 0.0 || x > 0.0 || x == 0.0 )) || 
              (! ( y < 0.0 || y > 0.0 || y == 0.0 )) || 
              (! ( z < 0.0 || z > 0.0 || z == 0.0 ))
        ){ return result; };
      // response from dev who helped fix this:
      // ""it is a bug because they should always account for NaN 
      // whether it is intended or not ;)""
      //END FIX

	// band 1
	result += shCoefficients[ 1 ] * 2.0 * 0.511664 * y;
	result += shCoefficients[ 2 ] * 2.0 * 0.511664 * z;
	result += shCoefficients[ 3 ] * 2.0 * 0.511664 * x;
	// band 2
	result += shCoefficients[ 4 ] * 2.0 * 0.429043 * x * y;
	result += shCoefficients[ 5 ] * 2.0 * 0.429043 * y * z;
	result += shCoefficients[ 6 ] * ( 0.743125 * z * z - 0.247708 );
	result += shCoefficients[ 7 ] * 2.0 * 0.429043 * x * z;
	result += shCoefficients[ 8 ] * 0.429043 * ( x * x - y * y );
	return result;
}
```",False,0
BUG: MeshPhongMaterial flicker with flatShading: false,WestLangley,518811026,19,551911419,0,"> I use PlaneBufferGeometry with null z values to represent unknown elevations(holes). 

You have NaN's in your position data.

You also have NaN's in your vertex normals. Hence their length is undefined.

three.js does not support NaNs in data pushed to the GPU.",False,0
BUG: MeshPhongMaterial flicker with flatShading: false,kpetrow,518811026,20,551912932,0,"one should always account for NaN whether it is intended or not.  And its a simple fix.  We need this to work for our production and it doesnt harm anything having a check in.   Could we please add, is there any harm?  @WestLangley ",False,0
BUG: MeshPhongMaterial flicker with flatShading: false,WestLangley,518811026,21,551917382,0,">one should always account for NaN whether it is intended or not.

No, YOU need to account for it in your app and pass valid data.

",False,Impatience
BUG: MeshPhongMaterial flicker with flatShading: false,Mugen87,518811026,22,551921717,0,"Yes, this is something that needs to be fixed on app level.

> three.js does not support NaNs in data pushed to the GPU.

NaN geometry data also corrupt operations on the CPU like intersection tests or the computation of bounding volumes. So again, this is no library bug.",False,0
BUG: MeshPhongMaterial flicker with flatShading: false,kpetrow,518811026,23,551925337,0,"We have a valid use case for putting NaN's in our data set that as far as I can tell can not be done in any other way, efficiently.  In any real world application dealing with NaN's is just a fact of life .

@Mugen87 Bounding Volumes was actually my last post.  It all stems from not handling NaN's properly.  Just a simple test for isNaN would resolve most these.  Why is there such push back and not a desire to resolve this issue?

Are there any suggestions for dealing with real world data that has NaN's?  We have bandied around alpha mapping, but then we increase overhead with the map and need to access the alpha map and the vertex positions.  

Also what if the numbers are very large and overflows?  Will that cause a NaN that might need to be dealt with.",False,0
BUG: MeshPhongMaterial flicker with flatShading: false,Mugen87,518811026,24,551929007,0,"> Why is there such push back and not a desire to resolve this issue?

`NaN` values is an app level problem from our point of view since the user is in some sense the data producer. Geometric data like position, normal or color data are meant to be numerical. `NaN` data are no numerical data.

Handling `NaN` values is use case specific since proper default values depend on what the application actually does.

This is not only an issue in graphics. When you process huge data in context of KDD and machine learning, data cleanup is one important preparation step before you execute the actual algorithm (which is not responsible for handling missing, undefined or corrupt values).",False,0
BUG: MeshPhongMaterial flicker with flatShading: false,kpetrow,518811026,25,551934130,1,"so this is a ""no"" on resolving?  Is there a reason not to other than ""we don't support this""?  Its a one line PR.

I guess we will be patching every update from here out.",False,Impatience
Feature Request: Optionally Raise Some or All Vanilla Engine Limits,HollowedEmpire,528942466,1,528942466,0,"First off, I love Chocolate Doom and how accurate and true it is to the original executables - right down to bugs.

Though one particular aspect that I feel may be good to be able to alter is the static limits of the engine, such as the visplane limits. Preferably this could be done as an extra command line argument or configuration setting. When not specified, the normal limit values can be used as a default.

These limits in the engine have no real affect on how the game is actually _played_, so I feel providing an optional way to adjust them would still be true to the _vanilla experience_ and not go against the philosophy of the project. It's akin to how all the networking changes _aren't_ 'true to the original' Doom, or how the Vanilla Save Game Limit or Vanilla Demo Limit are already optional changes you can make. All these changes are to give the _vanilla experience_ a greater range of access, just as they are all equally not 'true' and are not 'pure' to the original.

To make the code change as minimal as possible, it could be as simple as specifying numbers for the limits (i.e. visplane array size). One could simply set them to high values in order to run more intensive maps. Another possible and more elegant solution could be letting Chocolate Doom read the map data and set the array limits dynamically at map load time to match what is present.

I'm sure I'm not the only one who would love to use Chocolate Doom for SiGiL and other maps that are otherwise only incompatible due to these hard coded limits. It's quite annoying juggling multiple ports solely because of this single aspect alone, especially that it doesn't affect the _vanilla experience_ in any way - or at least much less so than the changes already put in and present in Chocolate Doom.",True,0
Feature Request: Optionally Raise Some or All Vanilla Engine Limits,chungy,528942466,2,558845638,0,"This is beyond the scope of Chocolate Doom, but you may be pleased to know that [Crispy Doom](https://www.chocolate-doom.org/wiki/index.php/Crispy_Doom) already accomplishes the desire to remove vanilla limits.",False,0
Feature Request: Optionally Raise Some or All Vanilla Engine Limits,HollowedEmpire,528942466,3,559145447,0,"Thanks, though I already have tried Crispy. Not a fan unfortunately, there's too much changed with it and may as well use any given handful of ports at that point.

I'm curious though why it would be considered beyond the scope of the project. At face value, this seems to be pretty arbitrary considering the things already changed and different in Chocolate from the original game. Could I ask for you to shed a little more light on where the line is drawn at? It'd give at least some sort of resolve and explain the mindset. 

Edit: Thanks in advance!",False,0
Feature Request: Optionally Raise Some or All Vanilla Engine Limits,HollowedEmpire,528942466,4,559161975,0,"Also I'd like to point out one of the core tiers from PHILOSOPHY.md: ""The surrounding aspects of the game that aren‚Äôt part of the central gameplay experience can be extended as long as there‚Äôs a good reason and Vanilla behavior is respected."" 

What I'm proposing is being able to load countless maps and (more or less) an official Episode 5 for Doom, all of which are fully compatible with Vanilla Doom outside the limits. That seems to fall _precisely_ under that tier, does it not? :\  This is why I'm confused by you stating it is 'beyond the scope'.",False,Bitter frustration
Feature Request: Optionally Raise Some or All Vanilla Engine Limits,Kroc,528942466,5,559164985,0,"It's important that Choco-doom glitches and crashes _exactly_ the same way that the original DOS executable does; otherwise it doesn't solve the problem of allowing people to test against vanilla _behaviour_. If choco-doom doesn't behave the same as doom.exe did in 1993 then it's not ""vanilla"" compatibility people are testing, it's ""chocolate"" compatibility! :)",False,0
Feature Request: Optionally Raise Some or All Vanilla Engine Limits,HollowedEmpire,528942466,6,559183271,0,"But wouldn't the optional extended formats offered by Chocolate go against what you just said? For example the Vanilla save game limit and Vanilla demo limit removal options. Those are not 1993 compatible and thus would go against the behavior.

And additionally this is why I said it should be an optional parameter to up the limits. So if you didn't specify to extend the limits, it would have the same limits and _would_ crash in the exact same manner as the 1993 version, and thus make the compatibility point moot.
",False,Impatience
Feature Request: Optionally Raise Some or All Vanilla Engine Limits,fabiangreffrath,528942466,7,559211176,1,"> Thanks, though I already have tried Crispy. Not a fan unfortunately, there's too much changed with it

Yes? What exactly?

> I'm curious though why it would be considered beyond the scope of the project. 

Think about Chocolate Doom as a research project to replicate Vanilla Doom's behavior in the most precise, convenient and portable manner. It was never meant to play Sigil or any other limit breaking map or episode. And this won't change. You're asking at the wrong place. Also, I don't see why this would still be unclear after reading the philosophy document? ",False,Bitter frustration
Do not deprecate useResultCache,moxival,530232265,1,530232265,0,"### Do not deprecate the `AbstractQuery::useResultCache`

<!-- Fill in the relevant information below to help triage your issue. -->

|    Q        |   A
|------------ | ------
| New Feature | no
| RFC         | no
| BC Break    | no

#### Summary

Please do not deprecate the `AbstractQuery::useResultCache`. 
Without this method the ""chainability"" gets broken:
`
$em->createQuery()->setParameters()->useResultCache($debug)->getResult()
`

Without the method we have no way of doing the above and honestly I do not understand the need to remove it.
",True,0
Do not deprecate useResultCache,holtkamp,530232265,2,559723347,0,"Encountered the same situation where the ""application context"" determines whether caching is enabled.

```php
$query->useResultCache($this->applicationContext->useCaching());
```

I do like the explicit enable and disable cache methods, but why deprecate the ""useResultCache()"" methods? They can all exist right?",False,0
Do not deprecate useResultCache,ilianiv,530232265,3,559772460,0,"Same here ‚òπÔ∏è  I have to refactor all my `useResultCache()` calls with an ugly `if`s:

```
üëå  Nice and smooth in ORM 2:

$res = $entityManager
  ->createQuery($dql)
  ->useResultCache($cache_enabled, $ttl)
  ->getResult();

----

ü§Æ Ugly in ORM 3:

$q = $entityManager->createQuery($dql);
if ($cache_enabled) {
  $q->enableResultCache($ttl);
}
$res = $q->getResult();
```",False,0
Do not deprecate useResultCache,greg0ire,530232265,4,559859909,0,"The deprecation was introduced in #7701 , cc @someniatko .",False,0
Do not deprecate useResultCache,someniatko,530232265,5,559885504,0,"@Ocramius it looks like people are complaining, milord
Should we revert the change?",False,0
Do not deprecate useResultCache,Ocramius,530232265,6,559885740,0,"Not really: conditionally using result cache is secondary, mostly for ORM wrappers than for end consumers, so I'd say that the new API is indeed preferable.

In practice, in development environments, replace the cache with a volatile version, rather than passing a `$debug` flag down by multiple layers.",False,0
Do not deprecate useResultCache,greg0ire,530232265,7,559979136,0,"> replace the cache with a volatile version

That's clever! @moxival @holtkamp @ilianiv are you satisifed with this? 

```
üëå  Nice and smooth in ORM 2:

$res = $entityManager
  ->createQuery($dql)
  ->useResultCache($cache_enabled, $ttl)
  ->getResult();

----

‚ú®üëå‚ú®  Nicer and smoother in ORM 2:
// globally replace the cache with a volatile cache
// stop passing $cache_enabled through a bazillion layers

$res = $entityManager
  ->createQuery($dql)
  ->enableResultCache($ttl)
  ->getResult();
```",False,0
Do not deprecate useResultCache,moxival,530232265,8,560015478,0,"***EDIT***  _Lets make my comment less b**chy_

No I am not satisfied by this, when i submitted this issue I did not ask for a lecture in ""volatile cache"" I asked for you guys to leave a perfectly good method (that in no way will mess the ORM code).

If you can not leave the method by all means make us fill our code with ifs and elses for much more smoothness and volatile stuff üò∏ ",False,Vulgarity
Do not deprecate useResultCache,greg0ire,530232265,9,560016351,0,"Is this your way of telling us you don't understand what ""volatile cache"" means? If yes, there are nicer ways to do so. If not, then explain what it means to you, because when I read your last sentence , I'm under the impression that you don't‚Ä¶ why would this cause lots of ifs and elses?

regarding your EDIT: at least you noticed you were not being very nice :P

",False,Irony
Do not deprecate useResultCache,ilianiv,530232265,10,560020351,0,"@Ocramius @greg0ire Lets say you want to implement an api that should handle GET parameter ‚Äúnocache‚Äù that will make it hit the DB bypassing the cache. In this case replacing the cache is not so good. 

I think it‚Äôll cost you nothing to add a proxy method useResultCache() that will call one of the two new methods and everyone will be happy.",False,Entitlement
Do not deprecate useResultCache,greg0ire,530232265,11,560020788,0,"I think it would be simpler to have a cache decorator that is aware of the request stack (assuming Symfony context) and forwards calls to either a normal cache or a null cache (such as one of those: https://github.com/ThaDafinser/psr6-null-cache/blob/master/src/Adapter/ , but for doctrine/cache, we have not yet migrated to PSRs) depending on the value of this parameter. That way it works for the whole API just like you asked, and you did not modify n repositories with n calls to `useResultCache()` for that.",False,0
Do not deprecate useResultCache,moxival,530232265,12,560022587,1,"Dear @greg0ire this issue is not about me knowing what ""volatile cache"" is or isn't, I may know what volatile cache is (although in all honesty violate cache is just real fancy term) 'coz I was in the field for the last 15 years or I may have no idea coz I am 15 y.o. and I am just starting up with PHP  . May i remind you **milord** that PHP does not constrain itself to Symfony nor decorators (even less to decorators that are aware of stuff). 

If it is too challenging for you to keep the `useResultCache` please just say so _my liege_ we would totally understand. 

I also understand that ORM 3 sounds exhilarating and you feel like you should make all your loyal subjects miserable and have them change every single bit of code but maybe, just maybe you would let us (the stupid ones that have no clue what ""volatile cache"" is or how to manipulate their request stack with decorators while trying to implement PSR- 35672 at no avail) this little twinkle star of hope and leave the `useResultCache` alone because there is literally **0** (_zero_) effort involved on your part in doing so üò∫ (_please note the emoji - that means my comment is not mean but witty and hilarious filed with warm feelings and melting caches_).",False,Insulting
Incorrect C code generation when trying to echo a Lock variable,ghost,534583789,1,534583789,0,"Bad C code is generated with the following Nim code:

### Example
```nim
import locks

var l: Lock
echo l
```

### Current Output
```
CC: ltest.nim
Error: execution of an external compiler program 'gcc -c  -w  -I/home/tay/.choosenim/toolchains/nim-1.0.4/lib -I/home/tay -o /home/tay/.cache/nim/ltest_d/stdlib_dollars.nim.c.o /home/tay/.cache/nim/ltest_d/stdlib_dollars.nim.c' failed with exit code: 1

/home/tay/.cache/nim/ltest_d/stdlib_dollars.nim.c: In function ‚Äòdollar___3Dq1hcGH7a2iNlnh4hnPbg‚Äô:
/home/tay/.cache/nim/ltest_d/stdlib_dollars.nim.c:219:50: error: ‚Äòpthread_mutex_t‚Äô {aka ‚Äòunion <anonymous>‚Äô} has no member named ‚Äòabi‚Äô
  219 |  addQuoted__N56rmvBL9a3xqnKq09cKdb3A((&result), x.abi);
      |                                                  ^
```

### Expected Output
Compiler either compiles properly or errors before C code gen

### Possible Solution

* It looks like there might be an issue with the implementation of `$` for the Lock type

### Additional Information

* Fedora 31: `Linux localhost.localdomain 5.3.11-300.fc31.x86_64 #1 SMP Tue Nov 12 19:08:07 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux`

```
$ nim -v
Nim Compiler Version 1.0.4 [Linux: amd64]
Compiled at 2019-11-27
Copyright (c) 2006-2019 by Andreas Rumpf

git hash: c8998c498f5e2a0874846eb31309e1d1630faca6
active boot switches: -d:release
```
",True,0
Incorrect C code generation when trying to echo a Lock variable,Yardanico,534583789,2,616117085,0,Why close? The issue is still there,False,0
Incorrect C code generation when trying to echo a Lock variable,ghost,534583789,3,616764562,0,"It's been several months and there was no feedback, I figured an issue wasn't necessary.",False,0
Incorrect C code generation when trying to echo a Lock variable,Yardanico,534583789,4,616768581,0,"@3n-k1 that's not how it works in open-source in general - it just means that nobody worked on the issue yet, but as long as the bug is still there it should be open",False,Bitter frustration
Incorrect C code generation when trying to echo a Lock variable,ghost,534583789,5,616902966,0,"I understand how FOSS works. Now, I would prefer if you didn't mention me again as I would prefer not to receive notifications for this.",False,Bitter frustration
Incorrect C code generation when trying to echo a Lock variable,ghost,534583789,6,652191027,1,"It's been over half a year, and I get a bad taste in my mouth every time I see this. I'm closing this, and I will be blocking anyone who responds or re-opens; I really don't care if that makes me an ass.

Y'all have a minimal example and all the information necessary for this, feel free to open a ***new*** issue with a copy-paste of the initial issue I posted here.

This is me officially leaving the Nim community. So long, and thanks for all the fish.",False,Insulting
Windows gVim :Gwrite symlinked files,mg979,549848681,1,549848681,0,"In Windows gVim (not `win32unix`), `:Gwrite` doesn't work on files that are in symlinked directories (but `:Gstatus` works). It says that the file is _outside repository_.

In `win32unix` and `nvim` it works instead (it changes the buffer name to the real path).

For gVim (`win32`), I could fix it by changing https://github.com/tpope/vim-fugitive/blob/6bc345f6f16aee0dcc361f2f3bf80e4964f461af/autoload/fugitive.vim#L792 with
```vim
    return FugitiveVimPath(empty(url) ? a:url : has('win32') ? resolve(url) : url)
```

",True,0
Windows gVim :Gwrite symlinked files,tpope,549848681,2,574482902,0,"Where is the symlink, where is it pointing, and where is the repository?",False,0
Windows gVim :Gwrite symlinked files,mg979,549848681,3,574494489,0,"![Imgur](https://i.imgur.com/2DkGNXk.jpg)

Real path: 

    D:\Gianmaria\Dropbox (Personale)\Linux\vimrc\plugin\buffer_history.vim

and repository in

    D:\Gianmaria\Dropbox (Personale)\Linux\vimrc\plugin",False,0
Windows gVim :Gwrite symlinked files,tpope,549848681,4,574509430,0,You said the symlink was a directory. I ask where the symlink is and you give me a file. I also asked where it's pointing and you did not answer. I can't help you if you won't answer my questions.,False,Impatience
Windows gVim :Gwrite symlinked files,mg979,549848681,5,574898411,0,"Sorry, it was late and I was sleepy. The symlinked directory is the same as the repo, I have:

```
~\vimfiles                                   (not symlinked, real path)
~\vimfiles\plugin                            (symlink to D:\Gianmaria\Dropbox (Personale)\Linux\vimrc\plugin, that is also a repo)
                                             (repo is also readable here -> ~\vimfiles\plugin\.git)
~\vimfiles\plugin\buffer_history.vim         (:Gwrite here gives problems)

:Gwrite in the real file path (D:\Gianmaria\Dropbox (Personale)\Linux\vimrc\plugin\buffer_history.vim) works.
```

I understand that fighting with Windows paths is very annoying, just saying that fugitive isn't perfect in gVim on Windows, I had other issues as well, I didn't want to bother and I can also understand if you don't consider Windows a top priority. I just had an issue and I reported it, feel free to ingore it if you don't think it's worth wasting time on it.",False,0
Windows gVim :Gwrite symlinked files,tpope,549848681,6,574930020,0,"On initialization, Fugitive checks if the current file is a symlink, and if so, calls `cd .`. On UNIX, and presumably `win32unix` since it's working there for you, this forces the current buffer to have its name resolved.

https://github.com/tpope/vim-fugitive/blob/08601f221a336d13a606b33714e5a1e8457b07f7/plugin/fugitive.vim#L212-L215

If you can find a way to force the buffer name to resolve on win32, we can do that as well. I would start with `exe 'cd' fnameescape(getcwd())`.

Throwing `resolve()` elsewhere is a game of whack-a-mole, plus it adds IO to things that are supposed to be fast. It's not a tenable solution.",False,0
Windows gVim :Gwrite symlinked files,mg979,549848681,7,574931687,0,"Ok I'll try to do that, thanks.",False,0
Windows gVim :Gwrite symlinked files,mg979,549848681,8,574955957,0,"What I could figure out is that in Windows something work differently with git.
I did some testing and I think that the problem lies in Git for Windows, that works differently, in that it doesn't realizes that the two files belong to the same repository. The command gives _the same error_ also if run from the terminal (cmd.exe).

I updated git, just in case, but the error persists. I think only a `resolve()` call in the `:Gwrite` command for `win32` can fix this.

These are the `cmd` generated in https://github.com/tpope/vim-fugitive/blob/c83355d5c52002f94d08267f1d14ca6d1a2763e9/autoload/fugitive.vim#L429
  
```
WLS Debian (neovim), also here $HOME/.vim/plugin is a symlink to the Dropbox directory,
 :Gwrite works and the file path is changed to its resolved path

['-C', '/mnt/d/Gianmaria/Dropbox (Personale)/Linux/vimrc/plugin',
 '--literal-pathspecs', 'add', '--',
 '/home/gianmaria/.vim/plugin/buffer_history.vim']
```  

```
gVim Windows, also here the starting paths are different, but the command doesn't work

['-C', 'D:/Gianmaria/Dropbox (Personale)/Linux/vimrc/plugin',
 '--literal-pathspecs', 'add', '--',
 'C:/Users/Gianmaria/vimfiles/plugin/buffer_history.vim']
```

I also tested the same commands in the terminal (WSL and cmd.exe)

```
In WSL terminal, this works

$ git -C /mnt/d/Gianmaria/Dropbox\ \(Personale\)/Linux/vimrc/plugin --literal-pathspecs add -- /home/gianmaria/.vim/plugin/buffer_history.vim

In cmd.exe, this doesn't work (same error as with fugitive):

C:\Users\Gianmaria\vimfiles\plugin>git -C ""d:\Gianmaria\Dropbox (Personale)\Linux\vimrc\plugin"" --literal-pathspecs add -- c:\Users\Gianmaria\vimfiles\plugin\buffer_history.vim
```

",False,0
Windows gVim :Gwrite symlinked files,tpope,549848681,9,574963987,0,"This is news to me, but I don't believe it changes anything. Fugitive only passes an absolute path if it can't figure out the appropriate relative path, because older Git versions didn't support absolute paths at all. The core problem remains: Fugitive expects your buffer name to not be a symlink.",False,0
Windows gVim :Gwrite symlinked files,mg979,549848681,10,574964757,0,"This would fix it:
```
  let file = s:winshell() ? resolve(file) : file
```
just before

https://github.com/tpope/vim-fugitive/blob/c83355d5c52002f94d08267f1d14ca6d1a2763e9/autoload/fugitive.vim#L4605-L4610
",False,0
Windows gVim :Gwrite symlinked files,mg979,549848681,11,574965099,0,"Since the problem is with Git for Windows (imho), I can't think of anything else.

I made a mapping to resolve current file, and it works with it:

    nnoremap \f\ :exe 'file' resolve(@%)<cr>",False,0
Windows gVim :Gwrite symlinked files,tpope,549848681,12,574978313,1,"The problem is not with Git for Windows. You pointed out the real problem in your initial post (emphasis my own).

> In `win32unix` and `nvim` it works instead (**it changes the buffer name to the real path**).

All of Fugitive is built around the assumption that the buffer name is the the real path. At first I ignored this, but then bugs cropped up when people called `:cd` and the buffer name changed from a symlink path to a real path mid flight, so my solution was to force a `:cd .` early, thus preventing symlink paths from ever entering the equation. What you've told me is that this `:cd .` doesn't work. Two solutions spring to mind:

1. Find a way to make it work (where ""make it work"" is ""resolve symlink in the buffer name"").
2. If there is no way to make it work, then the original bug is irrelevant. Stop calling `resolve()` on the Git dir and work tree and everything should be back to matched up.

The following solutions do not spring to mind:

1. Change the semantics of one public function by adding `resolve()` to it, and verify that the one command that is confirmed broken is fixed.

Please stop doubling down on the bad idea that I told you is bad, and try the thing I asked you to try:

> I would start with exe 'cd' fnameescape(getcwd()).

---

Ironically, after I started writing this you edited in a decent idea.

> I made a mapping to resolve current file, and it works with it:
> 
> ```
> nnoremap \f\ :exe 'file' resolve(@%)<cr>
> ```

See also https://github.com/tpope/vim-fugitive/pull/814#issuecomment-446767081 where I attempt to provide a more robust way to do this automatically. This is the sort of thing I would consider doing, but getting to work right requires calling `edit` to reload the file, and I worry that editing the file from `BufReadPost` will cause non-obvious bugs. I would encourage you to try it, at least until the issue is fixed.",False,Mocking
cannot require file in lib folder,SampsonCrowley,556620312,1,556620312,0,"### Steps to reproduce
<!-- (Guidelines for creating a bug report are [available
here](https://edgeguides.rubyonrails.org/contributing_to_ruby_on_rails.html#creating-a-bug-report)) -->

Follow the Docs: https://guides.rubyonrails.org/testing.html#using-separate-files

```ruby
# lib/test/authenticated_test_helper.rb
module AuthenticatedTestHelper
  def sign_in(email, password)
    post account_login_url(email: email, password: password)
  end

  def admin_sign_in(email, password)
    post admin_session_url(email: email, password: password)
  end
end
```

```ruby
# test/controllers/admin/samples_controller_test.rb
require 'test_helper'
require 'test/authenticated_test_helper' # << throws: `cannot load such file -- lib/test/authenticated_test_helper

module Admin
  module SamplesControllerTest < ActionDispatch::IntegrationTest
    ...
    ...
  end
end
```

### Expected behavior
<!-- Tell us what should happen -->
It should find the file since it's defined in EXACTLY the location that's recommended

### Actual behavior
<!-- Tell us what happens instead -->
```bash
30: from -e:1:in `<main>'
        29: from -e:1:in `require'
        28: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:54:in `load'
        27: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:54:in `load'
        26: from /home/samps/sites/cannabislims/bin/rails:9:in `<main>'
        25: from /home/samps/.gem/ruby/2.6.5/gems/zeitwerk-2.2.2/lib/zeitwerk/kernel.rb:23:in `require'
        24: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:30:in `require'
        23: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:21:in `require_with_bootsnap_lfi'
        22: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/loaded_features_index.rb:92:in `register'
        21: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:22:in `block in require_with_bootsnap_lfi'
        20: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:22:in `require'
        19: from /home/samps/.gem/ruby/2.6.5/gems/railties-6.0.2.1/lib/rails/commands.rb:18:in `<main>'
        18: from /home/samps/.gem/ruby/2.6.5/gems/railties-6.0.2.1/lib/rails/command.rb:46:in `invoke'
        17: from /home/samps/.gem/ruby/2.6.5/gems/railties-6.0.2.1/lib/rails/command/base.rb:69:in `perform'
        16: from /home/samps/.gem/ruby/2.6.5/gems/thor-1.0.1/lib/thor.rb:392:in `dispatch'
        15: from /home/samps/.gem/ruby/2.6.5/gems/thor-1.0.1/lib/thor/invocation.rb:127:in `invoke_command'
        14: from /home/samps/.gem/ruby/2.6.5/gems/thor-1.0.1/lib/thor/command.rb:27:in `run'
        13: from /home/samps/.gem/ruby/2.6.5/gems/railties-6.0.2.1/lib/rails/commands/test/test_command.rb:33:in `perform'
        12: from /home/samps/.gem/ruby/2.6.5/gems/railties-6.0.2.1/lib/rails/test_unit/runner.rb:39:in `run'
        11: from /home/samps/.gem/ruby/2.6.5/gems/railties-6.0.2.1/lib/rails/test_unit/runner.rb:50:in `load_tests'
        10: from /home/samps/.gem/ruby/2.6.5/gems/railties-6.0.2.1/lib/rails/test_unit/runner.rb:50:in `each'
         9: from /home/samps/.gem/ruby/2.6.5/gems/railties-6.0.2.1/lib/rails/test_unit/runner.rb:50:in `block in load_tests'
         8: from /home/samps/.gem/ruby/2.6.5/gems/zeitwerk-2.2.2/lib/zeitwerk/kernel.rb:23:in `require'
         7: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:30:in `require'
         6: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:21:in `require_with_bootsnap_lfi'
         5: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/loaded_features_index.rb:92:in `register'
         4: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:22:in `block in require_with_bootsnap_lfi'
         3: from /home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:22:in `require'
         2: from /home/samps/sites/cannabislims/test/controllers/admin/samples_controller_tests/authenticated_test.rb:2:in `<main>'
         1: from /home/samps/.gem/ruby/2.6.5/gems/zeitwerk-2.2.2/lib/zeitwerk/kernel.rb:23:in `require'
/home/samps/.gem/ruby/2.6.5/gems/bootsnap-1.4.5/lib/bootsnap/load_path_cache/core_ext/kernel_require.rb:33:in `require': cannot load such file -- lib/test/authenticated_test_helper (LoadError)
```
### System configuration
**Rails version**: 6.0.2.1

**Ruby version**: 2.6.5p114 (2019-10-01 revision 67812) [x86_64-linux]
",True,0
cannot require file in lib folder,SampsonCrowley,556620312,2,579580920,0,"If i change to `require 'lib/test/authenticated_test_helper'` nothing changes
but changing to `require Rails.root.join('lib', 'test', 'authenticated_test_helper')` suddenly it works",False,0
cannot require file in lib folder,eileencodes,556620312,3,579754101,0,"Your test helpers should live in `test/` not in `lib/`. `lib` is not autoloaded, which is why `require 'lib/test/authenticated_test_helper'` doesn't work.

You are right though the docs are confusing. I believe they wrote a typo and meant write to store your helpers in `test/lib`, I usually put them in `test/test_helper/`. 

I have fixed the documentation in e7514128a6a5ca11385abfa7f699cc4fd9ceefd1 and 9082609a33531cf671c62231712d9a21a7facef6. The guides will be updated on the next release of Rails.

---

As an aside, you have more than one issue open on this repo that feels a bit like you're yelling at us. I get that you're new to Rails and are frustrated, but most of the folks here are volunteers and no one is purposefully making your life hard. ‚ù§Ô∏è  We appreciate bug reports, and will fix things as soon as we can, but we're all human and we make mistakes.",False,0
cannot require file in lib folder,SampsonCrowley,556620312,4,579894300,0,@eileencodes what's the point of the lib folder if I can't require files from it? that has always been the standard folder to place code that doesn't belong in `app`,False,Bitter frustration
cannot require file in lib folder,SampsonCrowley,556620312,5,579895030,1,"@eileencodes I don't see how opening a separate issue when I find one is yelling at you. I have been using rails since rails 3. I don't see anything in either issue where I'm blaming anyone or getting angry, but i am starting to get sick of library maintainers taking bug reports as personal attacks",False,Bitter frustration
qb: Improve the menu checks.,orbea,565358712,1,565358712,0,"## Description

This greater ensures the `HAVE_MENU_WIDGETS`, `HAVE_RGUI`, `HAVE_MATERIALUI`, `HAVE_OZONE` and `HAVE_XMB` will never be defined when `HAVE_MENU` is not.

## Related Issues

This allows only checking if the specific feature is enabled and not that `HAVE_MENU` is also defined with processor code and some clean up in `Makefile.common`.

## Related Pull Requests

I noticed this was not correctly fixed as I previously thought in commit https://github.com/libretro/RetroArch/commit/2be5ffe4e999757ba89a87b0012d1441ec78af77.",True,0
qb: Improve the menu checks.,orbea,565358712,2,590073199,0,"@twinaphex  Please stop censoring users, its very unprofessional.",False,Bitter frustration
qb: Improve the menu checks.,orbea,565358712,3,590074184,1,"> My reason for not immediately merging this is for another reason - I want to make menu widgets independent of the menu code altogether. This includes also the menu animation code. That is why I don't want to immediately merge this PR overnight.

Also since you obviously do not understand what I even was submitting let me point out this PR would of only made that task easier in the future where you would of needed to remove one line instead of dozens as is the current state in the code, but good job throwing out the baby with the bathwater. :)",False,Mocking
Where are the docs?,DylanYoung,567185616,1,567185616,0,"What options are supported by brewfile?  I can't find anything after extensive googling.  Even the main README mentions `unless` without describing it....

Can you point me to docs on the syntax of your DSL?",True,0
Where are the docs?,jacobbednarz,567185616,2,587928136,0,"The DSL is defined at https://github.com/Homebrew/homebrew-bundle/blob/677f39d62a8aefdcbc091f193af1f7ddbee0052d/lib/bundle/dsl.rb

The individual components (`cask`, `brew`) are documented in their own repositories as bundle should be passing through any arguments or parameters as is.",False,0
Where are the docs?,DylanYoung,567185616,3,587974208,0,"Those aren't docs :(

But it's something at least. Thanks.

I think you should leave this issue open. ",False,Bitter frustration
Where are the docs?,MikeMcQuaid,567185616,4,588150899,0,"> Where are the docs?

In the README and `brew bundle --help`.

> Even the main README mentions `unless` without describing it....

`unless` is a Ruby keyword. `Brewfile` is a Ruby DSL.

> I think you should leave this issue open.

No, sorry, we don't leave issues open unless there's actionable work in them. As-is: there is not.",False,0
Where are the docs?,DylanYoung,567185616,5,588269263,0,"Yes, so where's the description of the DSL.  I can write a DSL in python too, that doesn't mean it's going to have the same syntax, nor that I would expect my users to learn all of python in order to learn my dsl. 

This is highly actionable.  There are zero docs, by your own admission.  If you'd like to keep it that way, I guess that's up to you. ",False,Bitter frustration
Where are the docs?,DylanYoung,567185616,6,588271046,0,"Here is the output of `brew bundle --help` in case you were curious:

```
Usage: brew bundle subcommand

Bundler for non-Ruby dependencies from Homebrew, Homebrew Cask and the Mac App
Store.

        --file=                      Read the Brewfile from this file. Use
                                     --file=- to pipe to stdin/stdout.
        --global                     Read the Brewfile from ~/.Brewfile.

brew bundle [install] [-v|--verbose] [--no-upgrade]
[--file=path|--global]

Install or upgrade all dependencies in a Brewfile.

    -v, --verbose                    Print the output from commands as they are
                                     run.
        --no-upgrade                 Don't run brew upgrade on outdated
                                     dependencies. Note they may still be
                                     upgraded by brew install if needed.

brew bundle dump [--force] [--describe] [--no-restart]
[--file=path|--global]

Write all installed casks/formulae/taps into a Brewfile.

        --force                      Overwrite an existing Brewfile.
        --describe                   Include a description comment above each
                                     line, unless the dependency does not have a
                                     description.
        --no-restart                 Do not add restart_service to formula
                                     lines.

brew bundle cleanup [--force] [--zap] [--file=path|--global]

Uninstall all dependencies not listed in a Brewfile.

        --force                      Actually perform the cleanup operations.
        --zap                        Remove casks using the zap command
                                     instead of uninstall.

brew bundle check [--verbose] [--no-upgrade] [--file=path|--global]

Check if all dependencies are installed in a Brewfile.

    -v, --verbose                    Print and check for all missing
                                     dependencies.
        --no-upgrade                 Ignore outdated dependencies.

brew bundle exec command

Run an external command in an isolated build environment.

brew bundle list [--all|--brews|--casks|--taps|--mas]
[--file=path|--global]

List all dependencies present in a Brewfile. By default, only Homebrew
dependencies are listed.

        --all                        List all dependencies.
        --brews                      List Homebrew dependencies.
        --casks                      List Homebrew Cask dependencies.
        --taps                       List tap dependencies.
        --mas                        List Mac App Store dependencies.
```

No mention of the DSL, not even the subcommands allowed.   I'm a big fan of brew-bundle, but it's not super useful for complex cases without docs.  It's especially confusing because there are a number of outdated docs on the net for old iterations of brew bundle.",False,0
Where are the docs?,DylanYoung,567185616,7,588272330,0,But I guess I'll start learning about Ruby DSLs since documentation doesn't seem to be a priority for you *sigh*.,False,Bitter frustration
Where are the docs?,DylanYoung,567185616,8,588278422,0,"Perhaps you can answer this at least.

Is the following section of the man pages applicable to brew-bundle?

i.e. Can I define a custom external command and call it in a Brewfile?
```
CUSTOM EXTERNAL COMMANDS
       Homebrew, like git(1), supports external commands. These are executable
       scripts  that  reside  somewhere  in  the  PATH,  named brew-cmdname or
       brew-cmdname.rb, which can be invoked like brew  cmdname.  This  allows
       you to create your own commands without modifying Homebrew's internals.

       Instructions for creating your own commands can be found in  the  docs:
       https://docs.brew.sh/External-Commands
```",False,Bitter frustration
Where are the docs?,MikeMcQuaid,567185616,9,588278985,0,Please reread our Code of Conduct and adjust your communication accordingly: https://github.com/Homebrew/.github/blob/master/CODE_OF_CONDUCT.md#code-of-conduct,False,Impatience
Where are the docs?,DylanYoung,567185616,10,588280321,0,How have I violated that code of conduct?  I'm sorry if I have.,False,0
Where are the docs?,DylanYoung,567185616,11,588281668,0,"I came here with an issue that you've acknowledged exists.  I've done nothing but try to work with you to try to resolve that problem, respectfully and openly.

I will leave now.  Good luck with whatever's bugging you.  ",False,Impatience
Where are the docs?,MikeMcQuaid,567185616,12,588297726,1,"> I'm sorry if I have.

I appreciate the apology.

> I've done nothing but try to work with you to try to resolve that problem, respectfully and openly.

I have not found your communication to be respectful. For example:

> Good luck with whatever's bugging you.

Yes, I agree leaving is probably for the best, sorry.",False,Irony
Solution for 16631 : IndexFormatter() doesn't round to the nearest integer,Wlodarski,574196668,1,574196668,0,"## PR Summary

Solution for issue #16631  ",True,0
Solution for 16631 : IndexFormatter() doesn't round to the nearest integer,Wlodarski,574196668,2,600339991,0,Two weeks without any manifestation of interest. Closing.,False,0
Solution for 16631 : IndexFormatter() doesn't round to the nearest integer,tacaswell,574196668,3,600366626,0,"The tests are still not passing and none of the issues brought up in #16631 (the need to document the API changes, an attempt to move to the simpler fix, or a clear argument as to why IndexFormatter should not be deprecated).  From my point of view we have been waiting on you.",False,Impatience
Solution for 16631 : IndexFormatter() doesn't round to the nearest integer,WeatherGod,574196668,4,600374797,0,"It would also have helped if the issue title and description had more
information. I certainly could not tell what this PR was for just by
looking at this among all of the other PRs and issues.

On Tue, Mar 17, 2020 at 8:37 PM Thomas A Caswell <notifications@github.com>
wrote:

> The tests are still not passing and none of the issues brought up in
> #16631 <https://github.com/matplotlib/matplotlib/issues/16631> (the need
> to document the API changes, an attempt to move to the simpler fix, or a
> clear argument as to why IndexFormatter should not be deprecated). From my
> point of view we have been waiting on you.
>
> ‚Äî
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/matplotlib/matplotlib/pull/16634#issuecomment-600366626>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AACHF6DM2ANDPG462BOB3BLRIAJUTANCNFSM4K73BSUA>
> .
>
",False,Impatience
Solution for 16631 : IndexFormatter() doesn't round to the nearest integer,Wlodarski,574196668,5,600409859,1,"> The tests are still not passing

I will repeat for the last time : THE TEST REQUIRES THE BUG TO BE PRESENT.

The bug manifests itself by outputting ``Label 0`` for -1 < indexes < 0 instead of outputting an empty string ``""""``. 

The test is passed when the bug is present, that is when``Label 0`` is wrongly outputted. 
The test fails when the bug is NOT present, that is when ``IndexFormatter``  returns an empty string, ``""""``, AS IT SHOULD.

The solution --- which consists only of rounding the index value, ``round(i)``  instead of truncating the index value, ``int(i + 0.5)`` --- returns an empty string, ``""""``.  The solution fails the test because the test is WRONG. The test is by itself A BUG.

> From my point of view we have been waiting on you.

I CANNOT CORRECT THE TEST. IT'S YOUR RESPONSIBILITY AND PREROGATIVE.

> the need to document the API changes

If you adopt the solution, THERE NO NEED TO CHANGE THE DOCUMENTATION NOR THE API

> why IndexFormatter should not be deprecated)

Because, WITH THE SOLUTION instead a deprecation :

1. IndexFormatter works. It simply works, as intended, without any bug.
2. ``IndexFormatter(x_labels)`` is far simpler than proposed convoluted alternatives like ``FuncFormatter(lambda x, _: dict(zip(range(len(x_labels)), x_labels)).get(x, """"))`` to achieve the same exact end result.
3. The solution doesn't break back compatibility in the middle of minor revision. The users don't have to change their programs. They don't have to find an alternative. 
4. It follows the precepts of semantic versioning instead of adopting post hoc rationalisations (like ""others deprecated functionalities in the middle of minor revision so lets pretend it's okay for us to do the same"")
5. You don't have to spend time and energy changing the documentation. You don't have to teach the users alternative methods that are necessarily harder to implement and explain.
6. Apparently, the DEPRECATION is based, at least partially, on the false assumption that a decade of zero complaints from the users somehow means the users are unhappy with IndexFormatter and therefore don't use it. 
7. THE DEPRECATION misses entirely the point of avoiding the vicious cycle of complexity. Increasing complexity to achieving the same exact functionality is a huge waste of your time. It only make you busier and busier, writing more convoluted code and spending more time managing harder to debug bugs.

I have wasted enough of my time trying to convince you. So be it.",False,Bitter frustration
2d Projection created by multmatrix can not be extruded.,VendicarKahn,598170002,1,598170002,0,"Doing a manual projection with a user defined matrix [[1,0,0,0],[0,1,0,0],[0,0,0,0],[0,0,0,1]]
results in a 2d projection onto the xy plane.

Requesting a linear extrude of this 2d object residing on the xy plane results in an error extruding a 3d object when no 3d object exists.

",True,0
2d Projection created by multmatrix can not be extruded.,nophead,598170002,2,612379550,0,"Un-surprising as you haven't made a 2D object, you have just squashed a 3D one, which will no longer be manifold due to lots of self intersections.",False,0
2d Projection created by multmatrix can not be extruded.,t-paul,598170002,3,612398754,0,"As @nophead already said, a matrix tranformation does not convert 3D to 2D. For that there's a special module [`projection()`](https://en.wikibooks.org/wiki/OpenSCAD_User_Manual/Using_the_2D_Subsystem#3D_to_2D_Projection).",False,0
2d Projection created by multmatrix can not be extruded.,VendicarKahn,598170002,4,612652622,0,"So you are saying that there are two kinds of 2d objects in openscad.¬†The first has zero dimensions along one axis and the second has zero directions along one axis and has special magical properties.¬†Unacceptable.¬†11.04.2020, 07:29, ""Torsten Paul"" <notifications@github.com>:
As @nophead already said, a matrix tranformation does not convert 3D to 2D. For that there's a special module projection().

‚ÄîYou are receiving this because you authored the thread.Reply to this email directly, view it on GitHub, or unsubscribe.",False,Mocking
2d Projection created by multmatrix can not be extruded.,nophead,598170002,5,612653448,0,"No there are only 2D objects with XY coordinates and 3D objects with X, Y, Z coordinates.

If a 3D object has all its Z values 0 it is an invalid 3D object as 3D objects represent 3D solids and must have a finite thickness.",False,0
2d Projection created by multmatrix can not be extruded.,t-paul,598170002,6,612654324,0,"OpenSCAD is mesh based. If you squish a (default aligned) cube with 6 polygons to zero height, you still end up with a degenerated 3d object having 4 zero area polygons and 2 with the same coordinates but different normal vector.",False,0
2d Projection created by multmatrix can not be extruded.,VendicarKahn,598170002,7,612655134,0,"Manifolds are only defined in 3 or more dimensions. ¬†So, ya. ¬†Using a matrix to project a 3d object onto a 2d surface necessarily means not having a manifold.¬†The projection function also does not return a manifold since it returns a 2d object.¬†There is no such thing as a 2-manifold.¬†""Lots of self intersections"" -¬† That is your problem, not mine.¬†If you provide a multmatrix function you should support any matrix, not just an unspecified¬†subset of them.¬†¬†11.04.2020, 05:40, ""Chris"" <notifications@github.com>:
Un-surprising as you haven't made a 2D object, you have just squashed a 3D one, which will no longer be manifold due to lots of self intersections.

‚ÄîYou are receiving this because you authored the thread.Reply to this email directly, view it on GitHub, or unsubscribe.",False,0
2d Projection created by multmatrix can not be extruded.,VendicarKahn,598170002,8,612655412,0,"¬†If you provide a multmatrix function then you should support all matricies, not just an unspecified subset of them.¬†What other matrix operations don't you support.¬†¬†12.04.2020, 14:05, ""Torsten Paul"" <notifications@github.com>:
OpenSCAD is mesh based. If you squish a (default aligned) cube with 6 polygons to zero height, you still end up with a degenerated 3d object having 4 zero area polygons and 2 with the same coordinates but different normal vector.

‚ÄîYou are receiving this because you authored the thread.Reply to this email directly, view it on GitHub, or unsubscribe.",False,0
2d Projection created by multmatrix can not be extruded.,VendicarKahn,598170002,9,612655837,1,"Maybe you should actually write that down somewhere. ¬†You know, like in the documentation. ¬†Especially in the Multmatrix section of the documentation to make it clear that this program is incapable of managing ¬† all matrix operations.¬†LOL¬†It's half baked.¬†¬†12.04.2020, 13:58, ""Chris"" <notifications@github.com>:
No there are only 2D objects with XY coordinates and 3D objects with X, Y, Z coordinates.
If a 3D object has all its Z values 0 it is an invalid 3D object as 3D objects represent 3D solids and must have a finite thickness.

‚ÄîYou are receiving this because you authored the thread.Reply to this email directly, view it on GitHub, or unsubscribe.",False,Irony
"Are u crazy? How did u make so unusable bloatwared software? Its possible to be simple, wtf do u do?",noiseshaade,627087857,1,627087857,0,"Whats wrong with retroarch? Everything is wrong! Its terrible, hujnya ebanaya karoche!

btw its my pack. No all borders/icons/whatever , simple shader which improves graphic out of box
https://github.com/noiseshaade/RetroArch-vape

No stupid descriptions, interface fits in screen
![image](https://user-images.githubusercontent.com/61594968/83236221-27cf0600-a161-11ea-822d-83149d470924.png)

Out of box, retroarch is extremely complex even for advanced users. Who thought that F1 is great idea for menu? Why not escape like in usual games/emulators?

The only advantage of ra vs standalone emulators is that it support shader and TECHNICALLY it could be better, but on practice its 100mb of shader folder. Whats wrong with syncronization, why snes has sound crackling out of box? U link syncronization with monitors sync rate but it seems like it changes.  Why you cant do like previous old emulators did and somehow link emulation with stable generator? Its not very bad if u sync game with time, not display

Why sound should lag in 2020?

Why you linked sound control with DB and pression +- returns random volume?

For example, just scaled image with bilinear filter looks extremely clean like this
![image](https://user-images.githubusercontent.com/61594968/83236335-5e0c8580-a161-11ea-952e-9696308c42dc.png)

Color and feelings are not natural

Its possible to fix with some YIQ, blur, black/white point correction, noise for texturization, it gives this effect
![image](https://user-images.githubusercontent.com/61594968/83236413-7ed4db00-a161-11ea-9163-7d076ec77254.png)
Mario looks like
![image](https://user-images.githubusercontent.com/61594968/83236448-88f6d980-a161-11ea-8039-190ef1c86d9c.png)
Not like extra clear
![image](https://user-images.githubusercontent.com/61594968/83236780-13d7d400-a162-11ea-9532-d47b1e38261b.png)

Its also problem that LCD provides unnatural, too juicy color. May be this mario looks more natural, i reduced saturation

![image](https://user-images.githubusercontent.com/61594968/83236857-38cc4700-a162-11ea-9715-e1fd050c8f87.png)


U provide users extremely complex shader pack with AAAAAAA effects for tik tok

Why, if games created for playing, not playing in emulators? I promise that you software makes people VERY angry

A lot of hotkeys assigned on alphabet part of keyboard, why? Stock keyboard buttons sucks. Is it bad if u want just play from keyboard?

as for me, i use

WASD for arrows
QE - start/select
IO-/\O
KL-[]X

JU-L1L2
;P-R1R2

Why does its so necessary to load ffmeg library and crash if u remove this? It needs a lot of memory and huge size of emulator part. Are u really sure that so much users want to record the game or stream on twich?

May be there is some simple fork of retroarch for normal people? Why u cant make simple and ready for use, not so terrible?
",True,Bitter frustration
"Are u crazy? How did u make so unusable bloatwared software? Its possible to be simple, wtf do u do?",noiseshaade,627087857,2,635840655,0,"These CRT shaders how does they works? They are very complex but at least out of box they provide ugly dark scanlines, no noise, no blur, thats all

![image](https://user-images.githubusercontent.com/61594968/83237679-4b934b80-a163-11ea-826d-bbab8fabdbe3.png)

Look and ducktales gradients, do u really wanna say that crt method is 'narural'?

![image](https://user-images.githubusercontent.com/61594968/83237790-767d9f80-a163-11ea-96e2-07a4a0f72afe.png)

",False,Bitter frustration
"Are u crazy? How did u make so unusable bloatwared software? Its possible to be simple, wtf do u do?",noiseshaade,627087857,3,635841408,1,"Your project exist about 10 years and there is still not usable interface out of box, even no guide which explain how make this shit normal, explain what do u need to restore graphic etc",False,Insulting
Release notes still missing for 1.0.4,impredicative,628465400,1,628465400,0,"What's up with the release notes still missing for 1.0.4? It has been four days since the release. This is unprofessional.

https://github.com/pandas-dev/pandas/releases/tag/v1.0.4 links to https://pandas.pydata.org/docs/whatsnew/v1.0.4.html but the whatsnew page has been missing. Shouldn't it be made available no later than two hours after a release? People look for it and can't find it. A release shouldn't be published if its notes are not ready to go.

@simonjayhawkins you seem responsible.",True,Bitter frustration
Release notes still missing for 1.0.4,TomAugspurger,628465400,2,636881444,0,"@impredicative check your tone please: things like ""This is unprofessional"" and ""you seem responsible"" are not kind.

See https://github.com/pandas-dev/pandas/issues/33300#issuecomment-635932128.",False,Bitter frustration
Release notes still missing for 1.0.4,simonjayhawkins,628465400,3,636883137,0,try https://pandas.pydata.org/pandas-docs/version/1.0.4/whatsnew/v1.0.4.html in the meantime.,False,0
Release notes still missing for 1.0.4,impredicative,628465400,4,636918517,1,"@TomAugspurger I am stating the facts, and you don't like the facts. Do you know what else is not kind? People wasting their time clicking on a link that doesn't work, waiting days, and it still doesn't work. It's unkind and it's a waste of time. This is a routine problem with Pandas releases. Also, why did you close this issue considering it's not resolved yet? Who is your manager? I want to report your dysfunction to them.",False,Insulting
"Replace the refinery with the ""ERCC"" refinery in the standard game",Yara-smurf,631364219,1,631364219,0,"## Issue Summary
As it has become the standard in RAGL and proven its value, it might be a good idea to replace it everywhere for better balancing.

What is ERCC? 
ERCC provides more exits for harvesters (north exit) which makes harvesting from all sides more balanced. Furthermore, the outline of the refinery got changed to square to alleviate map-inconveniences. 

TODO: 
More detailed guide by widow about ERCC.",True,0
"Replace the refinery with the ""ERCC"" refinery in the standard game",PunkPun,631364219,2,639423838,0,"It this has a footprint
=+x
+=+
x==

while the current one has this
\_X\_
xxx
X==
\===

it looks like
![Screen Shot 2020-06-05 at 14 18 10](https://user-images.githubusercontent.com/37534529/83871007-ebd70a80-a737-11ea-9e09-6e61d8529322.png)

while the OG looks like
![Screen Shot 2020-06-05 at 14 24 34](https://user-images.githubusercontent.com/37534529/83871195-4f613800-a738-11ea-8ee9-de5d4111230b.png)


",False,0
"Replace the refinery with the ""ERCC"" refinery in the standard game",PunkPun,631364219,3,639424170,0,ERCC currently suffers from #18232,False,Bitter frustration
"Replace the refinery with the ""ERCC"" refinery in the standard game",pchote,631364219,4,639424561,0,Please also include screenshots that show how the harvester clips through solid walls and the roof for a fair comparison.,False,0
"Replace the refinery with the ""ERCC"" refinery in the standard game",PunkPun,631364219,5,639427273,0,"It works most of the time, but if you do these exact paths it clips

![ERCC4](https://user-images.githubusercontent.com/37534529/83871616-08c00d80-a739-11ea-9668-d47c2d36851c.gif)
![ERCC1](https://user-images.githubusercontent.com/37534529/83871625-0b226780-a739-11ea-8af6-99c820c4a31d.gif)
![ERCC3](https://user-images.githubusercontent.com/37534529/83871628-0bbafe00-a739-11ea-9f66-a492a9b27f56.gif)
![ERCC2](https://user-images.githubusercontent.com/37534529/83871630-0c539480-a739-11ea-853d-66bd42b7185b.gif)

One of the problems is that the harvester art is bigger than once cell
![Harv](https://user-images.githubusercontent.com/37534529/83871836-75d3a300-a739-11ea-93f7-94b332bd69a0.png)
that also causes problems with the current ref
![REF1](https://user-images.githubusercontent.com/37534529/83871862-88e67300-a739-11ea-9828-d546cd89e71d.png)
![REF3](https://user-images.githubusercontent.com/37534529/83871884-94399e80-a739-11ea-9c23-dc0f8aef6a83.png)
but these don't exist with ERCC
",False,0
"Replace the refinery with the ""ERCC"" refinery in the standard game",netnazgul,631364219,6,639430488,0,"> pchote: it would be much better if it used a new design that was not a space-warping bodge of the original refinery
> pchote: get rid of the overhang completely
> pchote: it is never going to be accepted as a replacement for the original refinery, but if you want to be added as a distinct thing that could be optionally enabled then it needs to make sense as its own thing",False,Bitter frustration
"Replace the refinery with the ""ERCC"" refinery in the standard game",pchote,631364219,7,639458526,1,"One thing that I find the most offensive about this extended trainwreck of a discussion is that it basically boils down to a few competitive players, who already have access to ERCC via map-mods, insisting that everybody else is playing the game wrong and should have the option to use the classic refineries taken away from them. This is not cool, IMO.",False,Bitter frustration
"Rename default branch from ""master"" to ""main""",trivikr,638202113,1,638202113,0,"**Is your feature request related to a problem? Please describe.**
Lot of software developers are discussing on twitter to rename default branches for their projects from ""master"" to ""main"" or equivalent https://twitter.com/search?q=master%20branch&src=typed_query

The primary reason being master-slave an oppressive metaphor.

**Describe the solution you'd like**
Node.js core follows the trend to change the industry standard, and renames default branch from ""master"" to ""main"" or similar

**Describe alternatives you've considered**
Sticking with existing master branch name for the default

EDIT: Updated ""renaming master to main"" to ""renaming default branch name from 'master' to 'main'""",True,0
"Rename default branch from ""master"" to ""main""",devsnek,638202113,2,643656288,0,This is not a technically difficult task (https://www.hanselman.com/blog/EasilyRenameYourGitDefaultBranchFromMasterToMain.aspx) but it might break some things. I definitely think we should try to change it.,False,0
"Rename default branch from ""master"" to ""main""",ronag,638202113,3,643661780,0,Does this mean we would have to change the cluster API (which includes master in its vocabulary) as well in a semver-major?,False,0
"Rename default branch from ""master"" to ""main""",Gallardo994,638202113,4,643663090,0,"This is getting silly already. No way it makes any sense.

UPD: How is this offtopic? Please stop bringing politics into development world. That's just mindblowingly silly.",False,Mocking
"Rename default branch from ""master"" to ""main""",ghost,638202113,5,643663108,1,"That is hilarious. From defacing nodejs.org with blm propaganda, to renaming master branch to avoid similarity with master-slave metaphor. What is the next requirement OpenJS will ask for? To remove the **test** directory to avoid similarity with **test**icles? Please keep technical aspects of the software free of politics and globalistic propaganda.",False,Mocking
Its most awful software,noiseshaade,643360992,1,643360992,0,"I met some 12yo guy and he played in Megaman, it was very hard for him. Not only megaman, even Duck Tales
And... Out of box your software is totally unusable, especially for kids

These AI, wtf is this? Why even cheap old chinese consoles with shitty hardware can just launch games and play without fucking with settings, but your software can not?

You have tons of shaders, but no shader preview. With paint.net u can preview changes, in your AAA software u cant. Very slow navigation, it does not remember position
Extremely huge interface
My config is this
![image](https://user-images.githubusercontent.com/61594968/85333850-4e563780-b4a8-11ea-97f7-b730b2c5f262.png)
Your is this
![image](https://user-images.githubusercontent.com/61594968/85333927-70e85080-b4a8-11ea-86c7-f9e45bac2f7f.png)
Why you describe wtf does 'audio' means? Can u make it fit in screen

Ozone has no scrollbar, no mouse support, if u try to scroll, actually its equal to up/down button

People dont wait that software does not work with mouse

U cant explain even default configuration, what button u need to press for some effect
![image](https://user-images.githubusercontent.com/61594968/85334229-e0f6d680-b4a8-11ea-8ee7-44cdc1c33a8b.png)
U can draw something like normal games if gamepad is not connected like 'L - ok', 'O - cancel'
How to reset settings with start button u dont descibe too

Why do u draw licence? It takes much space
![image](https://user-images.githubusercontent.com/61594968/85334410-3501bb00-b4a9-11ea-962d-8b9ed949ca50.png)
Why not organize cores like

NES\
Snes\

U just give people huge list of all cores. What difference between then - there is no description. But u can see that lisense is GPL v3, very important information haha

Your software with 'release' has >13000 of files, size is bigger than 600mb and out of box it even does not have any core

Alphabet keys are assigned on emulator functions. If u wanna make keyboard gamepad like
wasd iokl it will conflikt, and no highlight. WTF, why F1 calls menu? Do u know that many people use laptop, they can assign something on F keys by default

And they have to press Fn+F1 to call menu, who wait that?

Your bloatwared AAAAAA software is extremely glitchy, unusable for NORMAL people

Even your buttons numeration
![image](https://user-images.githubusercontent.com/61594968/85335010-48615600-b4aa-11ea-83fc-caddb6418469.png)
Like BY...some keys...AX is wtf

On 1st place u put very strange settings like bind timeout, duty cycle

![image](https://user-images.githubusercontent.com/61594968/85335123-747cd700-b4aa-11ea-8d85-c3affaf220d2.png)

If user wanna change settings, on 1st place u need to have Port1, just tune control

I dont know how after 10 years of development u still have so glitchy and unusable software",True,Insulting
Its most awful software,noiseshaade,643360992,2,647770931,0,"And ra eats cpu even if emulation is on pause. nestopia or epsxe do not do this, it continues to render and heat laptop even if it in fullscreen and u switch from ra to windows ",False,0
Its most awful software,noiseshaade,643360992,3,647772059,0,"For normal people on windows u can make small version without these bloatwared things, simple structure of folders, no tons of files in root, some cores are included out of box",False,Mocking
Its most awful software,noiseshaade,643360992,4,647773390,0,"As for me, to make interface usable, i just reduce scale to 0.6x and disable sublabels
If I need some description, i can press select button, btw font size in this is very small
![image](https://user-images.githubusercontent.com/61594968/85336082-105b1280-b4ac-11ea-9a05-117d092eacb4.png)


",False,0
Its most awful software,noiseshaade,643360992,5,647775337,0,"For normal view as for me i need some calibration, its possible to make it very simple

From this
![image](https://user-images.githubusercontent.com/61594968/85336237-49938280-b4ac-11ea-9db4-30048a378ff7.png)
To this
![image](https://user-images.githubusercontent.com/61594968/85336254-53b58100-b4ac-11ea-8f56-0c2f357b90fb.png)
YIQ boost, levels correction, some blur (coz CRT TV does not provide clean lines), some noise for texturization

Its not correct to just draw black lines or crt subpixels, coz when u played from tv, it was no huge subpixels from typical distance

These crt shaders have too strong settings and its a parody",False,0
Its most awful software,hizzlekizzle,643360992,6,647775729,0,"I appreciate you taking the time to assemble these suggestions and pics. I deleted the pic of your friend, as we don't want any pictures of people (esp anyone 12 or under) for various international liabilities.

While I/we don't agree with all of your suggestions (or with how you've expressed them lol), some of them are valid and helpful, so thanks for that. As long as you want to keep your suggestions in this thread, I won't close/lock it.",False,0
Its most awful software,noiseshaade,643360992,7,647776372,0,vashe pofig,False,0
Its most awful software,noiseshaade,643360992,8,647777508,1,"But u should know that for normal people, not crazy linuxoids, especially for kids, your software is nightmare in everything what u did. Any normal guy i thing can tune classic emulator, but retroarch looks like u hate people and kidding them",False,Insulting
LLDB is not able to find default source location,smallB007,670691047,1,670691047,0,"
https://github.com/rust-lang/rust/issues/33674<< Still happening in 2020!!!",True,Impatience
LLDB is not able to find default source location,jonas-schievink,670691047,2,667530128,0,"No reason to open a duplicate. You will need to provide more info if you want to see this fixed. 

I also suggest toning down your language a bit, most people working on Rust do this in their free time, and it isn't exactly motivating to have people complain like that.",False,Bitter frustration
LLDB is not able to find default source location,smallB007,670691047,3,667668829,1,"""most people working on Rust do this in their free time""
And again that crap: ""I'm doing it for free so don't expect any quality from it.""
I don't care if you fix it or not. I tell you what. Leave it and don't fix it. It is not my soft... I don't care...",False,Bitter frustration
Run CI on push to any branch and schedule all workflows hourly,mvorisek,723058396,1,723058396,0,"<!-- Fill in the relevant information below to help triage your pull request. -->

|      Q       |   A
|------------- | -----------
| Type         | improvement
| BC Break     | no
| Fixed issues | no

#### Summary

- triggering on push to any branch is important to allow testing before PR is made
- test default branch hourly is important to provided actual build state for the default branch",True,0
Run CI on push to any branch and schedule all workflows hourly,greg0ire,723058396,2,709974535,0,"> triggering on push to any branch is important to allow testing before PR is made

People can and should test locally, then they can make a draft PR IMO",False,0
Run CI on push to any branch and schedule all workflows hourly,mvorisek,723058396,3,709977176,0,"> 
> 
> > triggering on push to any branch is important to allow testing before PR is made
> 
> People can and should test locally, then they can make a draft PR IMO

yes, this is what it does - if you are in other repo, CI results are logged there, not here...",False,0
Run CI on push to any branch and schedule all workflows hourly,mvorisek,723058396,4,709980153,0,"@greg0ire this is also what Symfony does.

doctrine/orm does trigger on PR only (not on push) - but I will submit there a PR shortly ;-)",False,0
Run CI on push to any branch and schedule all workflows hourly,greg0ire,723058396,5,709988539,0,"> locally

I mean on their computer, not on their fork.",False,0
Run CI on push to any branch and schedule all workflows hourly,mvorisek,723058396,6,709989299,0,"I know ;-), but why not in GH which is the standard for almost every other project now :)",False,0
Run CI on push to any branch and schedule all workflows hourly,morozov,723058396,7,719748334,0,Closing as I don't see a value in applying this change.,False,0
Run CI on push to any branch and schedule all workflows hourly,mvorisek,723058396,8,719928932,0,@morozov I am very disappointed as allowing CI to run on push in very and industry standard practice across broad spectrum of open-source projects. Please see https://github.com/doctrine/dbal/pull/4349#discussion_r506721571,False,Bitter frustration
Run CI on push to any branch and schedule all workflows hourly,mvorisek,723058396,9,719929136,0,"> People can and should test locally, then they can make a draft PR IMO

@greg0ire please explain **SHOULD**, draft PR is definitelly not a good solution for anything that should not be merged",False,Bitter frustration
Run CI on push to any branch and schedule all workflows hourly,greg0ire,723058396,10,719939733,0,I mean people should attempt to run the test they can run locally before opening a PR.,False,0
Run CI on push to any branch and schedule all workflows hourly,mvorisek,723058396,11,719941235,0,"> 
> 
> I mean people should attempt to run the test they can run locally before opening a PR.

Yes! Many of them just create a branch, push, and want to see the CI result. This is perfect for quick and small fixes when local setup migh be very time consuming even for one DB.

So this PR seems perfectly legit to me.",False,0
Run CI on push to any branch and schedule all workflows hourly,greg0ire,723058396,12,719942629,0,"> Many of them just create a branch, push, and want to see the CI result.

Isn't that what draft PRs are for?",False,0
Run CI on push to any branch and schedule all workflows hourly,mvorisek,723058396,13,719943030,0,"> draft PR is definitelly not a good solution for anything that should not be merged

obviously no :)",False,0
Run CI on push to any branch and schedule all workflows hourly,greg0ire,723058396,14,719945614,1,Says who? You?,False,Mocking
Bundler and RubyGems install directory mismatch,felipec,727127817,1,727127817,0,"Many people don't install to the default directory, but to the user directory (--user-install), in fact, it's the default on many distributions.

Unfortunately this causes a mismatch between bundler and gem, since bundler doesn't read `gemrc` configuration files.

This problem has been explored in many issues: rubygems/bundler#710, rubygems/bundler#2565, rubygems/bundler#2667, chef/chef-dk#148. No satisfactory resolution from the development team.

I took the time to develop a simply and easy solution myself that should absolutely work for everyone, an environment variable: GEM_USER_INSTALL, which just works ‚Ñ¢. And thus I am creating yet another issue ticket.",True,0
Bundler and RubyGems install directory mismatch,hsbt,727127817,2,714299222,0,"> Many people don't install to the default directory, but to the user directory (--user-install), in fact, it's the default on many distributions.

I don't think so. The users who are using the default package provided many of Linux distributions are minority. also see https://rails-hosting.com/2020/#ruby-rails-version-updates

I'm Ruby programmer over the 20+ years. I and my company also didn't use it. We use the custom runtime build by like ruby-build.

> No satisfactory resolution from the development team.

This comment is a little offensive.",False,Bitter frustration
Bundler and RubyGems install directory mismatch,felipec,727127817,3,714312772,0,"@hsbt 

> I don't think so. The users who are using the default package provided many of Linux distributions are minority.

Really? Is that why 64% of Arch Linux users have [installed ruby gems](https://pkgstats.archlinux.de/packages#query=rubygems)? Is that also why Debian packages rubygems directly into the `ruby` package?

> https://rails-hosting.com/2020/#ruby-rails-version-updates

Rails developers != Ruby users.

> This comment is a little offensive.

You think reality is offensive?",False,Bitter frustration
Bundler and RubyGems install directory mismatch,deivid-rodriguez,727127817,4,714333788,0,"Whatever the popularity of Linux distributions is, I agree we could do better here. What about honouring `--user-install` if bundler is installing to system gems (i.e., no custom `BUNDLE_PATH` configuration is given). I think it's reasonable to respect whatever rubygems considers as the preferred location to install system gems.",False,0
Bundler and RubyGems install directory mismatch,felipec,727127817,5,714344167,0,"@deivid-rodriguez I also created a patch for that, but it's so ugly it seems like a hack. Basically we would need to parse `Gem.configuration` and find the `--user-install` string. I did it only for `gem`, but the same has to be checked for `install`.

```diff
--- a/bundler/lib/bundler/rubygems_integration.rb
+++ b/bundler/lib/bundler/rubygems_integration.rb
@@ -163,6 +163,8 @@ def sources
     end
 
     def gem_dir
+      user_install = Gem.configuration[:gem]&.split(' ')&.include?('--user-install')
+      return Gem.user_dir if user_install
       Gem.dir
     end
 

```",False,0
Bundler and RubyGems install directory mismatch,deivid-rodriguez,727127817,6,714372702,0,"Right, it _does_ feel like a hack, since these `gemrc` options are meant as permanent flags to the `gem` CLI commands. Not sure what the best solution is, need to think about it, but I don't like adding yet another environment variable.",False,0
Bundler and RubyGems install directory mismatch,felipec,727127817,7,714717763,0,"@deivid-rodriguez I've thought about it and tried many things. The environment variable is the only clean solution I see.

I don't think the configurations of `gem` and `bundler` were thought through.",False,0
Bundler and RubyGems install directory mismatch,deivid-rodriguez,727127817,8,714726975,1,"Thanks for your feedback, I guess? :man_shrugging:

I tend to disagree and I currently believe it might be better to add the ugly code and make this just work without the need for setting yet another environment variable. We could also deprecate the `--user-install` flag and move this to a global configuration to make the code cleaner, since I believe people don't just want `--user-install` for single CLI invocations but for changing the location to install gems to a user location permanently.",False,Vulgarity
update debian/ubuntu systemd service,enoch85,732267627,1,732267627,0,"After hours of struggling with getting this right, I thought I would post this config as the default one since the current one is wrong, or at least doesn't work.

When building from source, the paths are wrong. Also, systemd PID shouldn't be run in `/var/run` (`usr/local/var/run`), it should be run in `/run/folder`.

This PR solves all that so that the paths are correct, and the actual PID is run from the correct directory.

Also added logging for systemd instead of logging to syslog.",True,0
update debian/ubuntu systemd service,alandekok,732267627,2,718754729,0,Which system did you try this on?  We've used this on multiple Debian / Ubuntu systems.  Packages on http://packages.networkradius.com are using these.,False,0
update debian/ubuntu systemd service,alandekok,732267627,3,718758006,0,"From looking at the changes, it appears you build the system via `./configure / make / make install`.  The defaults used there are for generic Unix-style operating systems.  Then, you've used the Debian-specific `systemd` file.  Since Debian / Ubuntu changes the paths where files go, this `systemd` file is specifically for use on Debian / Ubuntu systems.  Not for use on ""generic Unix-style"" operating systems.

If you plan on using FreeRADIUS on Debian / Ubuntu, then use the build system for that: `make deb`.  That will build a `.deb` package which will work.

Or, use the pre-built packages from http://packages.networkradius.com",False,0
update debian/ubuntu systemd service,enoch85,732267627,4,718805007,0,"Thanks for the pointers! This was on Ubuntu 20.04 Server.

> Or, use the pre-built packages from http://packages.networkradius.com

I would if I could, but they don't work with Ubuntu Focal 20.04.

Already spent three days on this. Now the service starts and freeradius reports OK, but still I can't connect from our Unifi AP. Using `radiusd -X` works fine though. I'm going nuts soon. 

",False,0
update debian/ubuntu systemd service,enoch85,732267627,5,718823253,0,"@alandekok Hey, just tried `make deb` and that failed as well since dependencies required are only available in Bionic.

Please make Focal work so that we can use a recent LTS release instead of using something that will end support soon.

I give up now. ",False,Impatience
update debian/ubuntu systemd service,arr2036,732267627,6,718902779,0,"@enoch85 FreeRADIUS is an open source project.  If you want to see progress on a particular issue, then you are expected to contribute, _especially_ with something fairly trivial like this, where you could easily determine the correct dependencies for 20.04 and submit a PR.

",False,0
update debian/ubuntu systemd service,enoch85,732267627,7,719361159,0,"@arr2036 The repo that I maintain only has [620 stars](https://github.com/nextcloud/vm/), but yeah. I'm well familiar with the concept. Thanks for the friendly reminder. üëç 

Everything is easy if you know how to do it. I can tell you almost everything about Nextcloud, but I've never worked with FreeRADIUS before. 

As a maintainer of a project I see myself as the main contributor. My philosophy is to make it as easy as possible for people using what I produce. In my case, [install Nextcloud on any computer/server](https://help.nextcloud.com/t/appreciation-for-a-job-well-done/92140) with 0 Linux experience. I'm sorry to see that this isn't the thing here. Focal was released in April 2020, and still it's not supported.",False,Bitter frustration
update debian/ubuntu systemd service,mcnewton,732267627,8,719486183,0,"I fixed up the Debian control file and added a Dockerfile for Focal a week after it was released. The Dockerfile works by building packages and then installing them, so `make deb` definitely works on Focal as long as the correct build dependencies have been installed.

If you've got Docker installed then you can use it to build the packages if it helps to avoid having to install the build dependencies on the host system:
```
docker build -t fr-ubuntu20-build --target=build scripts/docker/ubuntu20
C=$(docker create fr-ubuntu20-build)
docker cp $C:/usr/local/src/repositories/ ./deb
docker rm $C
```
Packages for the main site is on my to do list, but sadly so are a lot of other things.",False,0
update debian/ubuntu systemd service,alandekok,732267627,9,719488024,0,"@enoch85  If you're familiar with Open Source, then you should be familiar with the concept that ""we don't work for you"".  You should also be familiar with the idea of ""you have zero right to demand that we do what you want"".  You should also be familiar with the idea of ""please supply patches, as we can't do everything"".

We are _very_ sensitive to the all-too common practice of ""you do what I want, because I can't be bothered to do it myself"".  We have very little sympathy for such poor attitude.

We don't expect everyone to be experts in FreeRADIUS.  We do expect that people's contributions will be productive, and polite.

The current release (3.0.21) does not have Focal support because Focal was released after we released 3.0.21.  As Matthew points out, Focal is supported in the v3.0.x branch.  Which will become 3.0.22 any time now.

To close this out, we're sorry that the software was imperfect.  But your approach is to take offence at straightforward, factual, comments.  We suggest this is entirely the wrong approach.",False,Entitlement
update debian/ubuntu systemd service,enoch85,732267627,10,719495679,1,"@mcnewton 

> as long as the correct build dependencies have been installed.

I don't remember the exact name now but some libxxx-c2 and libxxx-c3 weren't possible to install from Ubuntu Focals own repos (clean ISO from their site), hence `make deb` failed. I did `apt-get install libxxx*` (1500 MB of packages) just to don't miss out on anything, but still - no success.

If the docker solution works, then great!

@alandekok 

> ""please supply patches, as we can't do everything"".

I'm sure familiar with that, and I understand 100% - but putting things IN CAPITAL LETTERS when you could do **bold** or *cursive* instead just seemed offensive to me. 

> ""you do what I want, because I can't be bothered to do it myself""

Well, at least I tried. I hope you didn't found my efforts (though small) as not bringing anything to the table.

> We suggest this is entirely the wrong approach.

Noted, and sorry for that.

",False,Bitter frustration
"[ProxyManagerBridge] replace ""ocramius/proxy-manager"" by a more convenient fork",nicolas-grekas,741692283,1,741692283,0,"| Q             | A
| ------------- | ---
| Branch?       | 5.x
| Bug fix?      | no
| New feature?  | no
| Deprecations? | no
| Tickets       | -
| License       | MIT
| Doc PR        | -

*EDIT: Although I didn't intend any harm at all, this PR wasn't well-received (see comments below). I invite the reader to check https://github.com/Ocramius/ProxyManager/issues/630 for a possible follow up.*

The versioning policy of `ocramius/proxy-manager` has been proven quite painful over the years.
Everybody can feel it hard in various ways these days with Composer 2 and PHP 8 coming.

I created the [`suimarco/proxy-manager`](https://github.com/suimarco/proxy-manager) fork to provide the same code and API, but with a versioning policy that is friendly to continuous migrations.

This fork is a drop-in replacement for `ocramius/proxy-manager` that is tested from PHP 7.1 to 8.0.
It also ships with a few fixes that were not merged upstream but are still needed, and that we had to monkey-patch. The related monkey-patching can now be removed.",True,0
"[ProxyManagerBridge] replace ""ocramius/proxy-manager"" by a more convenient fork",Ocramius,741692283,2,726713739,0,"Seems completely backwards to me. Perfectly OK from a MIT license PoV: your package, my freely provided code.

Discussion also at https://github.com/suimarco/proxy-manager/pull/1, but overall, instead of helping maintainers with their OSS work, forks are being done, and dependency graphs are turned into a mess.

As I mentioned there, there's a way to support the upstream work @ https://github.com/Ocramius/ProxyManager/tree/f65ae0f9dcbdd9d6ad3abb721a9e09c3d7d868a4#ocramiusproxy-manager-for-enterprise, and bugfixes for critical stuff are being backported anyway :shrug: ",False,Bitter frustration
"[ProxyManagerBridge] replace ""ocramius/proxy-manager"" by a more convenient fork",Ocramius,741692283,3,726716953,1,"And btw, thanks to the symfony ecosystem for ruining the mood again: was going to work on PHP 8 support this weekend, but yet again I find myself being nervous, angry and frustrated at people treating weekend projects (**LITERALLY DOING IT IN THE WEEKEND**)  like paid-for products.",False,Bitter frustration
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",DaniilAnichin,774727384,1,774727384,0,"<!-- This is the repository for IPython command line, if you can try to make sure this question/bug/feature belong here and not on one of the Jupyter repositories. 

If it's a generic Python/Jupyter question, try other forums or discourse.jupyter.org.

If you are unsure, it's ok to post here, though, there are few maintainer so you might not get a fast response. 

Ability of maintainers to spend time and resources on project like IPython is heavily influenced by US politics, and the current government policies have been harmful to the IPython Maintainers and Community. 

If you are on the fence on who to vote for or wether to vote, please cast your vote in for the democrat party in the US.
-->
Relevant traceback reads as follows: 
```
  File ""../venv/lib/python3.8/site-packages/IPython/core/completer.py"", line 2029, in _complete
    completions = self._jedi_matches(
  File ""../venv/lib/python3.8/site-packages/IPython/core/completer.py"", line 1373, in _jedi_matches
    interpreter = jedi.Interpreter(
  File ""../venv/lib/python3.8/site-packages/jedi/api/__init__.py"", line 725, in __init__
    super().__init__(code, environment=environment,
TypeError: __init__() got an unexpected keyword argument 'column'
```

sys info: 
```
{'commit_hash': '62779a198',
 'commit_source': 'installation',
 'default_encoding': 'utf-8',
 'ipython_path': '../venv/lib/python3.8/site-packages/IPython',
 'ipython_version': '7.18.0',
 'os_name': 'posix',
 'platform': 'Linux-4.15.0-128-generic-x86_64-with-glibc2.17',
 'sys_executable': '../venv/bin/python',
 'sys_platform': 'linux',
 'sys_version': '3.8.5 (default, Jul 20 2020, 19:50:14) \n[GCC 5.4.0 20160609]'}
```
same reported in jedi repo too",True,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",benwu95,774727384,2,751273086,0,"https://github.com/ipython/ipython/commit/dcd9dc90aee7e4c5c52ce44c18e7518934790612

The code has been already updated, but `7.19.0` did not include this. :(",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",davidhalter,774727384,3,751273584,0,"As a temporary fix for anyone just trying to get things working again:

```
pip install jedi==0.17.2
```

----

It would be really nice if you could quickly release a 7.19.1. (It's already fixed on master).

Sorry for that. I did not realize that IPython with that fix was not released yet. I usually test IPython completions before doing a Jedi release, but not this time :/. It will probably also not happen in the future anymore, because I'm going to release Jedi 1.0 soon, so this is probably the last time for a long time that you have to deal with deprecations in Jedi.

Still wish you a Merry Christmas!",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",davidhalter,774727384,4,751274036,0,"By the way, a `7.19.1` release with the dependency `jedi<0.18.0` would also suffice.",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",jkryanchou,774727384,5,751425126,0,@davidhalter Thanks for your solution. it finally work.,False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",SamuelDSR,774727384,6,751476672,0,"I had the same problem with ipython and thanks to the solution of @davidhalter , it works again. :+1: ",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",digglife,774727384,7,751613487,0,Thank you for opening this issue.  Happy holiday!,False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",yurzo,774727384,8,751899801,0,Posted a pr to pin the dependency as suggested: #12746,False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",bremme,774727384,9,751923463,0,Thanks a lot. I thought my shell was broken. Every time when I tried to use tab completion in IPython it crashed. Glad I found a solution.,False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",aprilahijriyan,774727384,10,751947852,0,Finally found a solution! thank you @davidhalter üéâ,False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",tik9,774727384,11,753048651,0,When would the problem be fixed so that every Jedi version is compatible with ipython?,False,Impatience
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",NeilGirdhar,774727384,12,753227319,0,"@tik9 Does installing from master work for you?

    pip install git+https://github.com/ipython/ipython

If so, whenever ipython releases a new version, it will be fixed for everyone.",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",tik9,774727384,13,753302449,0,"@NeilGirdhar , when doing
`pip install git+https://github.com/ipython/ipython`

It seems to work, I still have to update to the current Jedi.",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",omarish,774727384,14,753556986,0,"Got this issue as well. Pinning `jedi==0.17.2` worked for me, thanks @davidhalter.",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",stefanschlipfi,774727384,15,753614077,0,"> As a temporary fix for anyone just trying to get things working again:
> 
> ```
> pip install jedi==0.17.2
> ```
> 
> It would be really nice if you could quickly release a 7.19.1. (It's already fixed on master).
> 
> Sorry for that. I did not realize that IPython with that fix was not released yet. I usually test IPython completions before doing a Jedi release, but not this time :/. It will probably also not happen in the future anymore, because I'm going to release Jedi 1.0 soon, so this is probably the last time for a long time that you have to deal with deprecations in Jedi.
> 
> Still wish you a Merry Christmas!

Thank you for your solution.
I got the same issue and IPython worked after I installed Jedi",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",nicksama88,774727384,16,753719755,0,"installing 0.17.2 of jedi also worked for me.  Was chasing my tail trying to figure out why it wasn't working in a new virtual environment, glad to have found this!  Hope the fix is out soon.  Luckily this showed up as the top link in Google for me when searching ""ipython init got an unexpected keyword argument 'column'"".",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",bl-ue,774727384,17,754042613,0,"This works:
```console
$ pip install 'jedi<0.18'
```",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",ABODFTW,774727384,18,754056158,0,"Thanks @bl-ue

For reference, the autocomplete on my Jupyter notebook wasn't working and I was getting this same error on the terminal.

And now it's working as expected after downgrading jedi to 0.17.2 by just executing the command @bl-ue mentioned.",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",BallsyWalnuts,774727384,19,754626312,0,Confirmed that downgrading jedi to `0.17.2` fixed the issue for me as well. Thanks @davidhalter,False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",gabrielebndn,774727384,20,754832463,0,"Hi everyone, I've stumbled on this issue while creating docker images for myself.
If I understand correctly from [this comment](https://github.com/ipython/ipython/issues/12740#issuecomment-751273086) the problem has already been solved on master, but so far no release including the fix has been issued.
In order to better organize my own work, may I know when do you plan a new release? Somebody [here](https://github.com/ipython/ipython/issues/12740#issuecomment-751273584) was suggesting to quickly release 7.19.1, including this patch, is it still an option?",False,Impatience
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",HosseinDahaei,774727384,21,756318031,0,"> As a temporary fix for anyone just trying to get things working again:
> 
> ```
> pip install jedi==0.17.2
> ```
> 
> It would be really nice if you could quickly release a 7.19.1. (It's already fixed on master).
> 
> Sorry for that. I did not realize that IPython with that fix was not released yet. I usually test IPython completions before doing a Jedi release, but not this time :/. It will probably also not happen in the future anymore, because I'm going to release Jedi 1.0 soon, so this is probably the last time for a long time that you have to deal with deprecations in Jedi.
> 
> Still wish you a Merry Christmas!

thanks a lot",False,Impatience
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",LuanComputacao,774727384,22,757333211,0,pip install -U jedi==0.17.2 parso==0.7.1,False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",mostealth,774727384,23,757769588,0,"I wonder, when you have a dependency (jedi) that is in version 0.x.y (not reached major 1), if it is not wiser to pin the minor as >=0.17,<0.18.",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",yurzo,774727384,24,758110002,0,FYI https://github.com/ipython/ipython/pull/12751#issuecomment-758065462,False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",violet4,774727384,25,759827665,0,"> As a temporary fix for anyone just trying to get things working again:
> 
> ```
> pip install jedi==0.17.2
> ```
> 
> It would be really nice if you could quickly release a 7.19.1. (It's already fixed on master).
> 
> Sorry for that. I did not realize that IPython with that fix was not released yet. I usually test IPython completions before doing a Jedi release, but not this time :/. It will probably also not happen in the future anymore, because I'm going to release Jedi 1.0 soon, so this is probably the last time for a long time that you have to deal with deprecations in Jedi.
> 
> Still wish you a Merry Christmas!

note: in anaconda, this has to be done in the same kernel you're using from within the jupyter notebook, not the one you're using the `jupyter notebook` command from!

took me about 30 minutes to finally figure that one out once i found @davidhalter's pip install tip (i used `conda install jedi==0.17.2` instead of pip).

thank you all!",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",gapster,774727384,26,761073748,0,"Can you explain what you mean by ""same kernel ..."" vs ""using the jupyter notebook ...""   i.e.,  will conda install jedi==0.17.2 from the shell command line do the trick?  (it seems to)",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",marvinbernhardt,774727384,27,761689616,0,"> Can you explain what you mean by ""same kernel ..."" vs ""using the jupyter notebook ..."" i.e., will conda install jedi==0.17.2 from the shell command line do the trick? (it seems to)

You can run Jupyter in one env and use it with a kernel from another env. See for example [this SO question](https://stackoverflow.com/questions/53004311/how-to-add-conda-environment-to-jupyter-lab). Jedi version of the latter env matters. If you are not aware of this, you probably use one conda env for jupyter and kernel.",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",dowenk,774727384,28,763018119,0,"The following works in ipython/jupyter.   I assume it gets jedi out of the way of the built-in completer.   Good enough for me until fix is released.

%config Completer.use_jedi = False
",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",bl-ue,774727384,29,763627280,0,"It does work! üëçüèª 
Nice find @dowenk! üòç ",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",davidhalter,774727384,30,763815141,0,"Because people at this point probably don't scroll up to my comment (https://github.com/ipython/ipython/issues/12740#issuecomment-751273584), this is what you should do temporarily:

```
pip install jedi==0.17.2
```",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",bl-ue,774727384,31,766183653,0,When is the team going to fix this? In just a month hundreds of users have encountered this issue.,False,Impatience
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",bl-ue,774727384,32,766183726,0,I guess pinning jedi to 0.17.2 should be added to the readme. Anyone think I should open a PR for that? üò∏,False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",phinate,774727384,33,766453106,0,"just adding a +1 to this issue, it totally disables tab completion in Jupyter, so worth pinning 0.17.2 until resolved",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",ghimireadarsh,774727384,34,768163265,0,"> As a temporary fix for anyone just trying to get things working again:
> 
> ```
> pip install jedi==0.17.2
> ```
> 
> It would be really nice if you could quickly release a 7.19.1. (It's already fixed on master).
> 
> Sorry for that. I did not realize that IPython with that fix was not released yet. I usually test IPython completions before doing a Jedi release, but not this time :/. It will probably also not happen in the future anymore, because I'm going to release Jedi 1.0 soon, so this is probably the last time for a long time that you have to deal with deprecations in Jedi.
> 
> Still wish you a Merry Christmas!

Thanks",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",bl-ue,774727384,35,768270501,0,This issue should be pinned on the GitHub repo.  @Carreau?,False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",rsokl,774727384,36,768406516,0,"What the timeline looks like for this getting fixed for mainline conda users? 

I ask because I have an online outreach course spinning up in about four days, meaning that 600+ highschool students are going to be encountering the ""now hit `<TAB>` to autocomplete üòÉ"" portion of their virtual course, and it is going unceremoniously bomb them out of `ipython`.

Downgrading `jedi` is easy enough for me, but much harder for someone who has never written a line of code before.",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",bnavigator,774727384,37,768416652,0,"Releasing a new Jedi version might be easier than waiting for the next IPython release.

@davidhalter, how about making a one time exception in this case and reintroducing the deprecated features, the lack of which breaks IPython (and a few more packages) in a Jedi 0.18.1?",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",bnavigator,774727384,38,768417720,0,"> What the timeline looks like for this getting fixed for mainline conda users?

You might be successful if you ask the conda maintainers to yank jedi 0.18 from their repos.",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",h-vetinari,774727384,39,768420082,0,"> You might be successful if you ask the conda maintainers to yank jedi 0.18 from their repos.

The last `ipython` builds on conda-forge [already](https://github.com/conda-forge/ipython-feedstock/blob/master/recipe/meta.yaml#L33-L34) depend on `jedi<0.18`, so this this shouldn't even happen if you create a new environment (or do `conda update ipython` first).",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",rsokl,774727384,40,768420717,0,">The last ipython builds on conda-forge already depend on jedi<0.18, so this this shouldn't even happen if you create a new environment (or do conda update ipython first).

Presumably if you have set conda-forge as the default channel, correct?
",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",h-vetinari,774727384,41,768424748,0,"Even Anaconda [followed](https://github.com/AnacondaRecipes/aggregate/commit/fdd96f6968d7d4b6b8ec68ff0caa4d8566e953f9#diff-b85a555c399c3f7c88b8c090a978f34b985a1b4d23db5d0fd8887dfc80b476d0) with this already, so that shouldn't matter.",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",rsokl,774727384,42,768433164,0,"Hmm.. Then why is it that creating a new conda environment (with `conda 4.9.2`) gives me:

```
>>> conda create -n test_it python=3.8 ipython
The following NEW packages will be INSTALLED:
...
ipython            pkgs/main/win-64::ipython-7.19.0-py38hd4e2768_0
...
jedi               pkgs/main/win-64::jedi-0.18.0-py38haa95532_1
```

Am I missing something here?",False,Bitter frustration
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",h-vetinari,774727384,43,768438937,0,"So the packages are definitely [there](https://anaconda.org/main/ipython/files) (notice the `_1` build number), but interestingly, are not seeing much (or even any) downloads. I don't know if your anaconda channel is behind a proxy somewhere (or indeed, if anaconda themselves do some more integration testing before a package goes ""live""), but in any case, you can work around this by adding `conda config --add channels conda-forge`.

Side note: For any user that's not under the strictest of regulatory requirements (where people want to be able to blame a provider like Anaconda if stuff goes bad), conda-forge is more than ready/stable/secure to be your default channel. But, fair warning, that's just my subjective opinion.",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",bnavigator,774727384,44,768439476,0,"> Am I missing something here?

Not you, but conda's dependency resolver. This is a problem and has been for years.  (Sorry for the off-topic, but I could not resist)",False,Impatience
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",bnavigator,774727384,45,768450773,0,"> So the packages are definitely [there](https://anaconda.org/main/ipython/files) (notice the `_1` build number), but interestingly, are not seeing much (or even any) downloads

My totally uneducated guess (because I never understood the internals of the conda resolver):
conda sees the conflict for `jedi==0.18.0 and ipython-7.19.0-..._1` and instead of resolving it by `jedi==0.17.2 and ipython-7.19.0-..._1` it chooses `jedi==0.18.0 and ipython-7.19.0-..._0`",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",h-vetinari,774727384,46,768451169,0,"Welp, spoke too soon, conda-forge has the same problem (basically, the higher build number seems to lose against having one dependency have a higher version, which is not how it _should_ work. A possible solution would be backporting https://github.com/ipython/ipython/commit/dcd9dc90aee7e4c5c52ce44c18e7518934790612 to the feedstock, but that really should happen here first).

> Not you, but conda's dependency resolver. This is a problem and has been for years. (Sorry for the off-topic, but I could not resist)

Have you heard of [mamba](https://github.com/mamba-org/mamba) already? It has a rewritten resolver but is fully compatible with the conda-ecosystem. Not only does it solve the biggest problem of the conda-solver (speed), but it also correctly sets up the environment here:

```
>mamba create -n test python=3.8 ipython
[...]
  ipython                             7.19.0  py38hc5df569_2      conda-forge/win-64       1 MB
  ipython_genutils                     0.2.0  py_1                conda-forge/noarch      21 KB
  jedi                                0.17.2  py38haa244fe_1      conda-forge/win-64     944 KB
[...]
```",False,Bitter frustration
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",rsokl,774727384,47,768459710,0,"Thank you, both of you, for the useful insights and recommendations. I really appreciate it.

It sounds like I will go the route of putting a call-out box in my website that adds a hand-holding jedi-downgrading walkthrough (although, then I have to hope that the students have the awareness to repeat that step when creating new environments)

As it stands, I am dangerously close to making *two* ""but think of the children"" pleas in a single github issue, which is a violation of one of my new years resolutions.

(mamba looks cool!)",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",h-vetinari,774727384,48,768471208,0,"@rsokl
I raised an issue with conda itself, since it's probably too much (though not impossible) to do this on the conda-forge packaging side (in a way that's also compatible across several jedi-versions) - raised an [issue](https://github.com/conda-forge/ipython-feedstock/issues/127) to discuss that too...

@Carreau [said](https://github.com/ipython/ipython/pull/12751#issuecomment-758065462) he'd make a new release soon, which would of course also solve things.

In the meantime, the ""hand-holding"" guide should be as easy as `conda install ipython ""jedi<0.18""` (the quotes are important though, as otherwise you end up piping to nowhere).",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",h-vetinari,774727384,49,768898772,0,"@rsokl
There was one other way I hadn't thought about - fixing the conda-forge repodata directly. This was done yesterday, and means the problem is fixed _within_ conda-forge, i.e. when using strict channel priority (ignoring the main anaconda package that still has the same problem) - which unfortunately is not the out-of-the-box behaviour for conda though.

You can either do this for a single install (e.g. `conda create -n test --strict-channel-priority python=3.8 ipython`), but the recommended way (see e.g. [here](https://conda-forge.org/)) is persisting that setting with
```
conda config --set channel_priority strict
```",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",gkuenning,774727384,50,770072154,0,"> Thank you, both of you, for the useful insights and recommendations. I really appreciate it.
> 
> It sounds like I will go the route of putting a call-out box in my website that adds a hand-holding jedi-downgrading walkthrough (although, then I have to hope that the students have the awareness to repeat that step when creating new environments)
> 
> As it stands, I am dangerously close to making _two_ ""but think of the children"" pleas in a single github issue, which is a violation of one of my new years resolutions.
> 
> (mamba looks cool!)

@rsoki, can you post a link to your walkthrough?  I think some others among us have the same problem...",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",gkuenning,774727384,51,770075806,0,"BTW, ""Effective Python"" (2nd edition) #89 suggests using the warnings module to handle things like deprecating arguments.",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",rsokl,774727384,52,770271052,0,"@gkuenning I just uploaded the downgrade instructions to Python Like You Mean It.

Under: [Information Introduction to Python](https://www.pythonlikeyoumeanit.com/Module1_GettingStartedWithPython/Informal_Intro_Python.html) and under [Jupyter Notebooks](https://www.pythonlikeyoumeanit.com/Module1_GettingStartedWithPython/Jupyter_Notebooks.html)",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",h-vetinari,774727384,53,770277649,0,"@rsokl 
Actually, anaconda fixed their repo data as well, so now it works out of the box again ü•≥ 
",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",rsokl,774727384,54,770284507,0,Nice! I just confirmed this on my end. I guess I'll keep those callout boxes up on PLYMI for a bit in case folks had already installed anaconda.,False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",Carreau,774727384,55,770295999,0,"I'll try to release 7.20 soon; I've make it compatible with jedi 0.18 as pip also use a resolver and so pinning is not an option as pip would be free to downgrade IPython and keep jedi 0.18 which will brake. 

Right now the limiting factor for the release is writing the what's new. ",False,0
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",mostealth,774727384,56,770297420,0,"> I'll try to release 7.20 soon; I've make it compatible with jedi 0.18 as pip also use a resolver and so pinning is not an option as pip would be free to downgrade IPython and keep jedi 0.18 which will brake.
> 
> Right now the limiting factor for the release is writing the what's new.

I am really disappointed that the maintainer of a great product as Ipython has such a limited knowledge about the problem.
The fact that Ipython has not been pinning jedi means that any version of it released in the past that depends on jedi is at its merci to stop working as soon as a no backwards compatible change is introduced and used in Ipython.
By changing Ipython to make it jedi 0.18 compatible and releasing 7.20 you make that 7.20 operational today. If tomorrow jedi makes another backwards incompatible change We will be screwed again.
And you can't blame jedi for backwards incompatible changes because it is not even 1.x and you are not even pinning the major.
You are calling for trouble.",False,Bitter frustration
"Last jedi release (0.18.0) is incompatible with ipython (7.19 and 7.18 tested); reason - column arg was deprecated, and now removed",Carreau,774727384,57,770299991,1,"> I am really disappointed that the maintainer of a great product as Ipython has such a limited knowledge about the problem.
> The fact that Ipython has not been pinning jedi means that any version of it released in the past that depends on jedi is at its merci to stop working as soon as a no backwards compatible change is introduced and used in Ipython.
> By changing Ipython to make it jedi 0.18 compatible and releasing 7.20 you make that 7.20 operational today. If tomorrow jedi makes another backwards incompatible change We will be screwed again.
> And you can't blame jedi for backwards incompatible changes because it is not even 1.x and you are not even pinning the major.
> You are calling for trouble.

I'm amazed that someone that contribute squat is telling me what i'm doing wrong without tryign to really think about the problem.

- Say I pin IPython 7.20 to jedi <0.19 , and david release jedi 0.19. 
- pip is no free to say ""hey let's install jedi 0.19 and downgrade IPython to 5.x"". 
- Bam you are wrong‚Ñ¢Ô∏è . üíé üöÄ üåï 

And second the change of API were in jedi 0.17 with deprecation warnings and fix in master close to a year ago; I thought that by now 8.0 would be out. 

FYI about me know nothing about versioning:
  - I spend month implementing on pushing the `python_requires` logic in pypi, and in pip to make sure when IPython dropped it did not break for Python 2 users
  - I'm one of the regular advocate to have version medata data and package content to be separated in conda/pypi for this exact reason.
  - It's not because something seem obvious that it's correct.

You could have just asked in for a question ""why are not you pinning to <0.19 starting with IPython 7.20"", to which I would have responded.
So not only you are impolite and haven't done your research,  everybody see you incorrectly assume you know better than others and  you've lost all credibility.

‚úã ‚¨áÔ∏è üé§ ",False,Bitter frustration
Update cleanup.sh so that testversions of engine kept longer for download,saturnvgoesup,777338361,1,777338361,0,"do not delete so soon.
relevant forum thread: https://springrts.com/phpbb/viewtopic.php?f=1&t=40040",True,0
Update cleanup.sh so that testversions of engine kept longer for download,abma,777338361,2,753371471,0,"the disk space on springrts.com is limited, there must be some autodeletion.

something similar like

https://github.com/spring/pr-downloader/blob/master/scripts/cleanup-builds.py 

needs to be used which deletes by ""short"" age and keeps the last n files.",False,0
Update cleanup.sh so that testversions of engine kept longer for download,saturnvgoesup,777338361,3,753372932,0,"there still is autodeletion, just slower.

used disk space depends on the ratio between new build and time until deletion.
in past this ratio was such, that all builds were deleted. no build at all remained for download.
i suggest to go with the dirty solution of longer deletion-time and adjust it if disk space should become a problem. i can also look into deleting based on file names or dates and only keep a certain amount of builds.",False,0
Update cleanup.sh so that testversions of engine kept longer for download,abma,777338361,4,753373559,0,">  i suggest to go with the dirty solution of longer deletion-time and adjust it if disk space should become a problem.

no, as when springrts.com runs out of disk space the database very likely will corrupt. this is much worse than not having development builds.

still the main issue was, that there was no stable release for >2 years.",False,0
Update cleanup.sh so that testversions of engine kept longer for download,saturnvgoesup,777338361,5,753375172,0,"the forum thread got locked right now and so i lost interest. you can close this, too. in fact close down everything.",False,Bitter frustration
Update cleanup.sh so that testversions of engine kept longer for download,abma,777338361,6,753377276,1,"this pull request comes way to late, sorry. when ""fixing"" it, please fix it orderly and not quick and dirty as it will create much more worse problems.

will you care about enough free disk space on springrts.com? i think no: so you have to listen to me.",False,Impatience
ENH: Reimplement and undeprecate DataFrame.lookup,impredicative,786108116,1,786108116,0,"#### Is your feature request related to a problem?

I seriously get the impression that Pandas is actively being sabotaged. As a case in point, `DataFrame.lookup` was deprecated in v1.2. The problems with this are:

1. The [doc for `DataFrame.lookup`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.lookup.html) says to see itself for an example. Well, there is no example in the doc page. The only example I'm aware of is [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#looking-up-values-by-index-column-labels) instead which is a different page.
2. The [changelog](https://pandas.pydata.org/docs/whatsnew/v1.2.0.html#deprecations) for this deprecation points to [GH18682](https://github.com/pandas-dev/pandas/pull/18682) which looks to be wholly irrelevant to this change. A lot has gone wrong here.
3. There is significant user code that already uses `lookup`. Why break it? If the current implementation of `lookup` is suboptimal, shouldn't it be optimized instead?
4. It is extremely complicated to use `melt` when `lookup` works quite simply. For example, compare [this simple answer using `lookup`](https://stackoverflow.com/a/45487394/) with [this complicated answer using `melt`](https://stackoverflow.com/a/65722008/).

Does nobody review changes, docs, and release notes anymore prior to the release? It looks this way.

#### Describe the solution you'd like

1. Optimize `lookup` if attainable using `melt` or otherwise.
1. Undeprecate `lookup`.",True,Bitter frustration
ENH: Reimplement and undeprecate DataFrame.lookup,jreback,786108116,2,760303544,0,"> Does nobody review changes, docs, and release notes anymore prior to the release? It looks this way.

this is not a friendly way to ask for things.

`DataFrame.lookup` has seen NO love at all since its initial implemention. It is a duplicative and unmaintained function. This is also polluting the namespace and is not in any way integrated to all of the other indexers. We do not need N ways to do the same thing.",False,Bitter frustration
ENH: Reimplement and undeprecate DataFrame.lookup,jreback,786108116,3,760304833,0,https://github.com/pandas-dev/pandas/pull/35224 is the PR and https://github.com/pandas-dev/pandas/issues/18262 is the issue,False,0
ENH: Reimplement and undeprecate DataFrame.lookup,jreback,786108116,4,760305261,0,"@impredicative you are welcome to contribute patches, e.g. doc fixes or other things.",False,0
ENH: Reimplement and undeprecate DataFrame.lookup,erfannariman,786108116,5,767160515,0,"@impredicative out of curiosity, what is exactly difficult about the [new proposed method](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#looking-up-values-by-index-column-labels), which is also significantly faster since `DataFrame.lookup` was a for-loop on the background. 

I agree that we can fix the documentation in the deprecated message and in the whatsnew though.

@jreback Would you be open to un-deprecate `DataFrame.lookup` and implement the `melt` + `loc` method on the background? I don't say it was an extremely popular method, but on SO I saw it being used sometimes.",False,0
ENH: Reimplement and undeprecate DataFrame.lookup,impredicative,786108116,6,767163699,0,"@erfannariman What's difficult, for example, is how complicated [this answer using `melt`](https://stackoverflow.com/a/65722008/) is relative to how simple [this answer using `lookup`](https://stackoverflow.com/a/45487394/) is. The answer using `melt` requires three complicated method calls relative to the answer using `lookup` which is a single simple method call. In summary, just as you're saying, `DataFrame.lookup` could've been reimplemented using `melt`, thereby satisfying everyone.",False,Bitter frustration
ENH: Reimplement and undeprecate DataFrame.lookup,erfannariman,786108116,7,767165019,0,"> @erfannariman What's difficult, for example, is how complicated [this answer using `melt`](https://stackoverflow.com/a/65722008/) is relative to how simple [this answer using `lookup`](https://stackoverflow.com/a/45487394/) is. The answer using `melt` requires three complicated method calls relative to the answer using `lookup` which is a single simple method call. In summary, just as you're saying, `DataFrame.lookup` could've been reimplemented using `melt`, thereby satisfying everyone.

I don't disagree with your point, although I don't think the proposed method is pers√© difficult, but I might be biased. I would not be against un-deprecating `DataFrame.lookup`. I will await @jreback response.",False,0
ENH: Reimplement and undeprecate DataFrame.lookup,impredicative,786108116,8,767166129,0,"Pandas is used because it is believed to have a wealth of friendly methods. If this belief erodes, so will its user base.",False,0
ENH: Reimplement and undeprecate DataFrame.lookup,jreback,786108116,9,767188415,0,"@impredicative there is almost no usage of lookup AFAICT. very very few issues / tests / SO entries. If you have a valid, real work very common then am happy to show a doc example / recipe. Having a method must be a high bar.",False,0
ENH: Reimplement and undeprecate DataFrame.lookup,impredicative,786108116,10,767191957,0,"@jreback A google.com search for `""df.lookup"" site:stackoverflow.com` lists 462 hits. That's more than for `df.bfill` (208) and `df.ffill` (375). Will you now be deprecating these two methods as well? What is your cutoff?",False,Bitter frustration
ENH: Reimplement and undeprecate DataFrame.lookup,jreback,786108116,11,767200211,0,"@impredicative you are not comparing apples to apples here. try with indexing operators, we *already* have `.loc`, `.iloc`, `.at`, and `.iat`, not to mention partial slicing and of course `[]`. which one shall we cut to keep `.lookup`????

I in fact have an issue to remove `.iat` and `.at`. We simply do not need this much choice.",False,Bitter frustration
ENH: Reimplement and undeprecate DataFrame.lookup,impredicative,786108116,12,767627283,0,"@jreback I don't use `.iat` or `.at`.

In general, I would ask the question: ""_Is the alternative more simple or more difficult to use?_"" If a method simplifies code, making it less verbose and prevents users from reimplementing common patterns, then it's valuable, even if you personally think it's a duplicate, proportional to how well it accomplishes this goal.",False,0
ENH: Reimplement and undeprecate DataFrame.lookup,jreback,786108116,13,768318988,0,"> @jreback I don't use `.iat` or `.at`.
> 
> In general, I would ask the question: ""_Is the alternative more simple or more difficult to use?_"" If a method simplifies code, making it less verbose and prevents users from reimplementing common patterns, then it's valuable, even if you personally think it's a duplicate, proportional to how well it accomplishes this goal.

this is balanced against an already huge api. There *are* alternatives from this. If you have  use case where this is especially awkward / burdensome I would encourage you to open an issue with a reproducible example and show why a new method makes sense.",False,0
ENH: Reimplement and undeprecate DataFrame.lookup,quanghm,786108116,14,772976219,0,"@erfannariman
> out of curiosity, what is exactly difficult about the [new proposed method](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#looking-up-values-by-index-column-labels), which is also significantly faster since `DataFrame.lookup` was a for-loop on the background.

I learned about the deprecation of `lookup` when answering [this SO question](https://stackoverflow.com/q/65892265/4238408). And [this](https://github.com/pandas-dev/pandas/pull/35224#discussion_r564046976) is my thought on the proposed alternative for `lookup`. Of course, if current `lookup` is a `for` loop, this method would be faster for small dataset. But for bigger dataset, things would change due to potentially larger memory allocation, especially when column names are long strings and data to be looked up are homogeneous.",False,0
ENH: Reimplement and undeprecate DataFrame.lookup,erfannariman,786108116,15,773144423,0,"> @erfannariman
> 
> > out of curiosity, what is exactly difficult about the [new proposed method](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#looking-up-values-by-index-column-labels), which is also significantly faster since `DataFrame.lookup` was a for-loop on the background.
> 
> I learned about the deprecation of `lookup` when answering [this SO question](https://stackoverflow.com/q/65892265/4238408). And [this](https://github.com/pandas-dev/pandas/pull/35224#discussion_r564046976) is my thought on the proposed alternative for `lookup`. Of course, if current `lookup` is a `for` loop, this method would be faster for small dataset. But for bigger dataset, things would change due to potentially larger memory allocation, especially when column names are long strings and data to be looked up are homogeneous.

Sure I would be happy to make a PR with a new proposed more efficient method and there we can discuss into more details and the core devs can share their thoughts as well. What would be your suggestion, the answer you gave on SO? Could you maybe share a reproducible example in a new ticket and mention this issue? @quanghm ",False,0
ENH: Reimplement and undeprecate DataFrame.lookup,quanghm,786108116,16,773374707,0,"@erfannariman 

> out of curiosity, what is exactly difficult about the new proposed method, which is also significantly faster since DataFrame.lookup was a for-loop on the background.

I ran a quick test and the results said the reverse:

```
np.random.seed(43)
# also tested for n = 1000, 10_000, 100_000
n=1_000_000
cols = list('abcdef')
df = pd.DataFrame(np.random.randint(0, 10, size=(n,len(cols))), columns=cols)
df['col'] = np.random.choice(cols, n)

# the proposed method
%%timeit -n 10
melt = df.melt('col')
melt = melt.loc[lambda x: x['col']==x['variable'], 'value']
melt = melt.reset_index(drop=True)
# 623 ms ¬± 6.86 ms per loop (mean ¬± std. dev. of 7 runs, 10 loops each)

# What I would do without `lookup`
%%timeit -n 10
cols, idx = np.unique(df['col'], return_inverse=True)
df.reindex(cols,axis=1).to_numpy()[np.arange(df.shape[0]), idx]
# 430 ms ¬± 4.73 ms per loop (mean ¬± std. dev. of 7 runs, 10 loops each)

# Current lookup on 1.1.4
%%timeit -n 10
df.lookup(df.index, df['col'])
# 321 ms ¬± 1.55 ms per loop (mean ¬± std. dev. of 7 runs, 10 loops each)
```

The proposed method always comes last by a big margin in terms of speed. This only gets worse if the column names get longer.



",False,0
ENH: Reimplement and undeprecate DataFrame.lookup,impredicative,786108116,17,773377986,0,"> lambda x: x['col']==x['variable']

Using lambdas is a non-starter. It shouldn't even be suggested unless it is the last option on earth.

> cols, idx = np.unique(df['col'], return_inverse=True)

`np.unique` obviously will support only a very limited number of Pandas datatypes, so it's probably a non-starter too. Numerical programming using Numpy is different from more generic dataframe programming using Pandas.",False,0
ENH: Reimplement and undeprecate DataFrame.lookup,quanghm,786108116,18,773387055,0,"@erfannariman 

Regarding the lambda function, that was originally an attempt to chain the three lines to detect possible speed improvement. Answer was not.

Using `.loc[melt['col']==melt['variable'],...]` wouldn't improve/change anything.

`np.unique` is used on the column names, so most likely will be strings. In general case, we can use instead, `pd.Series.unique()` and `pd.Series.factorize()` for that single line. But as far as I know, those two also returns `np.array`. 

Also, can you name some Pandas datatypes that wouldn't work with `np.unique`, especially those one would use as column names?

All those aside, the message here is that the proposed method is **slower** than current `lookup` method by factor of `2`.",False,0
ENH: Reimplement and undeprecate DataFrame.lookup,erfannariman,786108116,19,773407648,0,"Thanks for the extensive example and speedtests @quanghm . I did a quick check and seems like `lookup` wasn't a for loop actually, only for columns with mixed types. So the in terms of speed it looks okay. Only thing is that I remember it being quite slow for certain cases, but those must have been mixed type columns in my cases I think. See here for source code https://github.com/pandas-dev/pandas/blob/b5958ee1999e9aead1938c0bba2b674378807b3d/pandas/core/frame.py#L3848-L3861

For your approach, I agree that it's more efficient both in speed and memory allocation, but I remember writing multiple methods to replace `lookup` and I decided to go with melt since it was most ""readable"", without making it too complex for the documentation.

So I think it all boils down to @jreback and the argument that `DataFrame.lookup` has not been used as much, which is being discussed by @impredicative 
",False,0
ENH: Reimplement and undeprecate DataFrame.lookup,quanghm,786108116,20,773420703,0,"@erfannariman Thanks. The code for `lookup` indeed reflects what I imagine, that is resolving to numpy advance indexing.

However, in the case that `col` is a column in the data (e.g. in my test or in the proposed solution) it's almost certainly that `self._is_mixed_type == True`. So you are right that it is just a for loop as of now. Still, it's faster than the proposed `melt` method.

On another note, this is certainly new for me

> `np.unique` obviously will support only a very limited number of Pandas datatypes

can you provide some details?
",False,0
ENH: Reimplement and undeprecate DataFrame.lookup,quanghm,786108116,21,773477244,0,"> `np.unique` obviously will support only a very limited number of Pandas datatypes, so it's probably a non-starter too. Numerical programming using Numpy is different from more generic dataframe programming using Pandas.

I confirmed that `pd.factorize()` works and is much faster than `np.unique`, at least in my test above:

``` language=Python
%%timeit -n 10
idx, cols = pd.factorize(df['col'])
df.reindex(cols,axis=1).to_numpy()[np.arange(df.shape[0]), idx]
# 48.4 ms ¬± 716 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)
```

",False,0
ENH: Reimplement and undeprecate DataFrame.lookup,espdev,786108116,22,778665287,0," I think it is premature to deprecate `lookup` function. At the same time, the documentation for `melt` function is too superficial. For example when I read the documentation ([1](https://pandas.pydata.org/docs/reference/api/pandas.melt.html), [2](https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html?highlight=melt#looking-up-values-by-index-column-labels)) I do not understand how I need to port my code from `lookup` to `melt`.

I do simple things. Here is a simple example:
```python
>>> index = pd.Series(['2020-01-04', '2020-01-03'], index=['A', 'B'])
>>> df = pd.DataFrame({'A': [1,2,3], 'B': [4,5,6]}, index=['2020-01-03', '2020-01-04', '2020-01-05'])

>>> index
A    2020-01-04
B    2020-01-03
dtype: object

>>> df
            A  B
2020-01-03  1  4
2020-01-04  2  5
2020-01-05  3  6

>>> values = df.lookup(index.values, index.index)
>>> values
array([2, 4], dtype=int64)

>>> pd.Series(values, index=index.index)
A    2
B    4
dtype: int64
```

What should I do to use `melt` instead of `lookup`? And why can't I use high-level lookup API like before? Maybe I just stupid, but I do not want to think about it and dive into ornate concepts, I just want to implement the algorithm in high-level clean code. I want to keep my code simple and the code should clearly express my **intention**.

The code example from SO is tricky.
```
df['new_col'] = df.melt(id_vars='names', value_vars=['a', 'b'], ignore_index=False).query('names == variable').loc[df.index, 'value']
```

Ask yourself what **intention** it expresses. This is some kind of esoteric gibberish without reference to the subject area. The code is too low-level and verbose compared to the simple and concise `lookup(rows, cols)`.",False,Bitter frustration
ENH: Reimplement and undeprecate DataFrame.lookup,quanghm,786108116,23,778936909,0,"> @impredicative you are not comparing apples to apples here. try with indexing operators, we _already_ have `.loc`, `.iloc`, `.at`, and `.iat`, not to mention partial slicing and of course `[]`. which one shall we cut to keep `.lookup`????

@jreback I think it's you that not comparing apples to apples here: **none** of those APIs you listed offer `lookup` functionality (otherwise, you wouldn't have used `melt` as an alternative). `lookup` is a direct translation of `arr[list_1, list_2]` from Numpy to Pandas. The Numpy API is there and it looks like it will stay for a while, why remove Pandas' equivalent.

The example that @espdev listed in his comment above is a more common case for the use of `lookup`. And from that example, I agree that it's really is unnatural to think `melt` at all, let alone using it.",False,0
ENH: Reimplement and undeprecate DataFrame.lookup,MarcoGorelli,786108116,24,780707864,0,"First off, I can confirm I can reproduce the performance difference from https://github.com/pandas-dev/pandas/issues/39171#issuecomment-773374707, even without the lambda function (this is all on the v1.1.4 tag):

```ipython
In [58]: %%timeit -n 10
    ...: df.melt('col', ignore_index=False).query('col==variable')['value'].rein
    ...: dex(df.index).to_numpy()
    ...: 
    ...: 
671 ms ¬± 17.5 ms per loop (mean ¬± std. dev. of 7 runs, 10 loops each)

In [59]: %%timeit -n 10
    ...: df.lookup(df.index, df['col'])
    ...: 
    ...: 

290 ms ¬± 2.79 ms per loop (mean ¬± std. dev. of 7 runs, 10 loops each)
```

Second, is there a case when the solution in https://github.com/pandas-dev/pandas/issues/39171#issuecomment-773477244 would not work well enough? If not, should the user guide just be updated to use that and we can move on from this issue?

Third - @impredicative please consider your tone",False,0
ENH: Reimplement and undeprecate DataFrame.lookup,impredicative,786108116,25,780711627,1,"> Third - @impredicative please consider your tone

My tone is justifiable. I have had enough of Pandas being unprofessionally developed. Panda works only for simple manipulations of small dataframes, and scales extremely poorly or not at all. I have spent weeks and weeks trying to work around its various issues, and the best thing for me to do right now is to move to a real package like Dask, PySpark, etc. I'm sure that many other users here feel similarly.

Fourth - @MarcoGorelli please consider how Pandas is actually developed.",False,Bitter frustration
1k issues! (some of them from 2013) is it doctrine ORM not ACTIVELY maintaining anymore? IS IT DEAD?,mdogancay,795993177,1,795993177,0,Look at the ORM on github; whats going on? is this project dead? not actively maintaining by project owners anymore?,True,0
1k issues! (some of them from 2013) is it doctrine ORM not ACTIVELY maintaining anymore? IS IT DEAD?,Fedik,795993177,2,769058652,0,"No. Better look on the page with releases, than number of issues.",False,0
1k issues! (some of them from 2013) is it doctrine ORM not ACTIVELY maintaining anymore? IS IT DEAD?,mdogancay,795993177,3,769068827,0,"> No. Better look on the page with releases, than number of issues.

i already looked. total 19 issues solved in 2020. And there is 1k issues on tracker. I'm needing an answer from the project owners not from contributors.",False,0
1k issues! (some of them from 2013) is it doctrine ORM not ACTIVELY maintaining anymore? IS IT DEAD?,dbrumann,795993177,4,769070731,0,"@mdogancay This project is open source. Feel free to help out by confirming issues, providing test cases, fixing bugs. If you watch the repository you will see that there is quite some activity, but more people helping out will help reduce the backlog of existing issues.",False,0
1k issues! (some of them from 2013) is it doctrine ORM not ACTIVELY maintaining anymore? IS IT DEAD?,mdogancay,795993177,5,769074599,0,"> @mdogancay This project is open source. Feel free to help out by confirming issues, providing test cases, fixing bugs. If you watch the repository you will see that there is quite some activity, but more people helping out will help reduce the backlog of existing issues.

@dbrumann Thanks for the answer. The reason I'm asking this is because thousands of people are still actively using these projects. If the project is losing momentum (which it seems) we have to move towards an alternative choice. Developing a product with a dying project can result in serious financial losses! This is a serious situation. I trust the project managers to explain the situation when necessary.",False,0
1k issues! (some of them from 2013) is it doctrine ORM not ACTIVELY maintaining anymore? IS IT DEAD?,dbrumann,795993177,6,769082694,0,"> The reason I'm asking this is because thousands of people are still actively using these projects. If the project is losing momentum (which it seems) we have to move towards an alternative choice.

You don't necessarily have to move. You could also incentivise contributors and maintainers by supporting their efforts either financially or through active participation.

> Developing a product with a dying project can result in serious financial losses!

The fact that there was a release roughly a month ago (2020-12-04) should tell you that the project is not inactive. The work on an upcoming 2.9.x release could also help indicate at some activity. Not to mention the activity in sister projects like dbal and the releases there.

If you consider this project vital it might make sense to let the maintainers work on it instead of wasting their time with frivolous claims or requests of a risk assessment for your projects or whatever this is supposed to be.

If you have an alternative that you feel is more stable and a better choice for your project, feel free to switch to it. Otherwise I suggest you put your energy in supporting the dependencies that you feel are vital to your projects so they don't become ""dying projects"".",False,Insulting
1k issues! (some of them from 2013) is it doctrine ORM not ACTIVELY maintaining anymore? IS IT DEAD?,greg0ire,795993177,7,769732215,0,Let me close this as a way to show you that we do have a look at issues.,False,0
1k issues! (some of them from 2013) is it doctrine ORM not ACTIVELY maintaining anymore? IS IT DEAD?,mdogancay,795993177,8,770226714,1,"> Let me close this as a way to show you that we do have a look at issues.

@dbrumann , @greg0ire 
I am speaking the truth here, but you are making politics. Instead of looking for a solution to the problem, you're treating me badly and trying to silence me. I'm sorry, but I don't think you are ""good people"" anymore ... Yeah, with your behavior, it became clear why this project is Dying ... Goodbye FOREVER...",False,Insulting
Remove libpq dependency in RStudio Desktop,2011,807725060,1,807725060,0,"### System details

    RStudio Edition : Desktop
    RStudio Version : 1.4.1103
    OS Version      : Linux 5.4.80 #1 SMP GenuineIntel GNU/Linux
    R Version       : 3.4.1

### Steps to reproduce the problem

Version 1.3.1093 did not dynamically link to libpq (from PostgreSQL), but 1.4.1103 does.

One can verify this by simply running ldd /path/to/rstudio.

### Describe the problem in detail

The Release Notes at:

https://rstudio.com/products/rstudio/release-notes/

mention that RStudio Server Pro now requires a Postgres database, but says nothing about RStudio Desktop (and lacks any explanation why the Desktop version would require a database).

### Describe the behavior you expected

I would expect database connectivity to remain in the odbc package, and not pushed into the basic desktop. I strongly object to requiring something not found in a typical desktop setup, and request that if this dependency remains, you provide the necessary libraries with the RStudio Desktop installation (as you do with the Qt libraries and icu - both of which most Linux desktop machines already have).

- [x] I have read the guide for [submitting good bug reports](https://github.com/rstudio/rstudio/wiki/Writing-Good-Bug-Reports).
- [x] I have installed the latest version of RStudio, and confirmed that the issue still persists.
- [x] I have done my best to include a minimal, self-contained set of instructions for consistently reproducing the issue.",True,Bitter frustration
Remove libpq dependency in RStudio Desktop,ronblum,807725060,2,782130319,0,"@2011 Thank you for raising this! I'll mark it for review as part of our ongoing development of RStudio.

@kfeinauer Could this have something to do with introducing the internal database?",False,0
Remove libpq dependency in RStudio Desktop,kfeinauer,807725060,3,782150662,0,"@ronblum Yes, this came about when adding database support. The dependency should not be necessary on the desktop, but due to how we deal with dependencies a shared component that depends on this library is pulled in by both server and desktop. This requirement could be removed in a future release.",False,0
Remove libpq dependency in RStudio Desktop,ronblum,807725060,4,782267753,0,@kgartland-rstudio Thanks! I'll switch this from a bug to an enhancement request.,False,0
Remove libpq dependency in RStudio Desktop,mcg1969,807725060,5,841834983,0,"I'd like to suggest this is, in fact, something closer to a bug, and the previous label should be restored.

I understand why it was added‚ÄîRStudio Server 1.4 uses a database in order to manage internal data. However, the documentation shows two options: SQLite and Postgres, with SQLite is the default. Requring a Postgres dependency _when it is not even being used_ poses a significant installation burden in many contexts.",False,Bitter frustration
Remove libpq dependency in RStudio Desktop,dsajdak,807725060,6,996210182,0,I'm attempting to install rstudio-2021.09.1-372 and am using R 4.1.0. Is there really no way to get around the postgres requirement? We support a shared high performance computing environment and do not install databases. Is there a way to install just the libraries necessary to get the rstudio install to complete? Any advice is greatly appreciated!,False,0
Remove libpq dependency in RStudio Desktop,mikebessuille,807725060,7,998173148,0,"@dsajdak are you installing desktop, or server?  What platform?  I'm not understanding why this is a concern; you shouldn't have to install a database to get it to work.",False,Impatience
Remove libpq dependency in RStudio Desktop,ronblum,807725060,8,998187012,0,"Note: I believe we're requiring an installation of `libpq5`, but not all of PostgreSQL, when installing RStudio Desktop in a Linux environment.",False,0
Remove libpq dependency in RStudio Desktop,2011,807725060,9,998977126,1,"> Note: I believe we're requiring an installation of `libpq5`, but not all of PostgreSQL, when installing RStudio Desktop in a Linux environment.

WHY?  I have used RStudio desktop for years, and it never had this requirement before version 1.4.

1. RStudio desktop should not require this library.
2. Database connectivity should remain in the odbc package.
3. Typical desktop installations don't have this library (including mine - I would have to compile the entire PostgreSQL to upgrade to RStudio 1.4).
4. If you really feel that RStudio desktop absolutely **has** to have libpq, then why don't you provide it in the package? The installation package includes libraries for Qt and icu, which virtually every Linux desktop has. It makes absolutely no sense whatsoever to include those libraries with RStudio, yet demand users produce their own libpq library.",False,Bitter frustration
Alumite recipe oversight,SirFell,831174119,1,831174119,0,"#### Which modpack version are you using?
Any upto 2.1.0.4

#
#### If in multiplayer; On which server does this happen? 
Same for both SSP and SMP

#
#### What did you try to do, and what did you expect to happen?
Alumite crafting related oversight. In the smeltery 2 parts of alumite are made by combining 5 parts aluminium to 2 parts steel to 2 parts obsidian.
![image](https://user-images.githubusercontent.com/5310658/111072193-226d5d00-84e2-11eb-8353-747df3f4a2c7.png)

#
#### What happened instead? (Attach screenshots if needed)
When crafted by hand you make 9 parts of alumite (4.5 times more than in the smeltery) with same amount of materials.
![image](https://user-images.githubusercontent.com/5310658/111072230-4f217480-84e2-11eb-89d6-297b0e04306a.png)

#
#### What do you suggest instead/what changes do you propose?
Merge [(CoreMod) #161](https://github.com/GTNewHorizons/NewHorizonsCoreMod/pull/161) to equalize both recipes to 5:2:2 makes 2

#
#### What is your GTNH Discord username?
SirFell#1024",True,0
Alumite recipe oversight,KiloJoel,831174119,2,798924279,0,"As far as I'm aware, the dust recipe is intended to be more efficient, which is consistent with other alloy recipes being less efficient in the smeltery. There is also a quest which tell you to use the dust recipe because it's more efficient.",False,0
Alumite recipe oversight,KiloJoel,831174119,3,798924769,0,"This is a major slowdown so I don't think it should be changed lightly. When aluminium gravel is not available, this means 405 aluminium oreberries need to be grown just to make the tool forge. That's a VERY long time spent waiting and I'm not sure we want that.
Also makes alumite totally nonviable as a tool material - its stats are only slightly better than steel",False,0
Alumite recipe oversight,SirFell,831174119,4,798928502,0,"quest backing up likely unintended feature is not a good proof of intention since quests get rewritten a lot. Previously tool forge took MV to get due to 3 alum screws so i still dont see the point, we made tool forge just fine back then by either waiting or foraging for aluminium ingots",False,0
Alumite recipe oversight,SirFell,831174119,5,798928722,0,i wouldnt mind if it would be an alloy recipe (read as: require some actual progress rather than a piece of flint and some stone) but its shapeless recipe that can then be just smelted in the furnace,False,Insulting
Alumite recipe oversight,SirFell,831174119,6,798928919,0,"even in alloy furnace it would be way too op, 4.5x more efficient than smeltery recipe is dumb. next PR i will likely look at other recipes so they dont give such insane boosts in efficiency (im looking at you obsidian ingot)",False,0
Alumite recipe oversight,KiloJoel,831174119,7,798928987,0,"So I'm not saying let's not change this at all, but this needs some discussion before making a change, since this has a large impact on current early progression",False,0
Alumite recipe oversight,SirFell,831174119,8,798929575,0,"i believe shaped recipe should stay as in PR. maybe we should add slightly more efficient alloy recipe, say 3 ingots instead of 2 (50% efficiency boost)",False,0
Alumite recipe oversight,SirFell,831174119,9,798929689,0,and reform other alloy recipes that are replacement for smeltery recipes that give too big of a boost to that amount too.,False,0
Alumite recipe oversight,KiloJoel,831174119,10,798931479,0,"I think that the smeltery recipe is too inefficient for how alumite is used in GTNH, and probably shouldn't be used as a baseline for balance in its current state. It's currently a noob trap which wastes aluminium rather than a viable recipe.

I think that the dust mixing recipe should make 6 alumite, and the smeltery recipe should make 4, vs the 9 and 2 currently.",False,0
Alumite recipe oversight,Prometheus0000,831174119,11,798938316,0,"I agree with kilojoel here. Also, the comparison to the tool forge being MV is ludicrous. Why would you prevent people from having hammers for that long? They'd probably just use auto-miners instead, and the hammers wouldn't see use.

> quest backing up likely unintended feature is not a good proof of intention since quests get rewritten a lot
The first half of your sentence is a garbled mess. We aren't talking about backups?

Since I didn't realize the dust recipe existed when I played (which you have now made the same) I never even used alumite tools (aside from maybe my pickaxe, to get the next mining level, which I believe I changed after I repaired it the first time), since they were too expensive. And I had the gravel, too. This indicates that with a recipe that only gives two parts, it's too expensive.",False,Bitter frustration
Alumite recipe oversight,SirFell,831174119,12,798951217,1,"> The first half of your sentence is a garbled mess. We aren't talking about backups?

@Prometheus0000 if you dont know english well enough then at least use google and/or common sense:

![image](https://user-images.githubusercontent.com/5310658/111078870-40e25100-8500-11eb-8e90-e5dc3b618ad6.png)
in my original sentence ""backing up"" is clearly used as ""supporting""

>Also, the comparison to the tool forge being MV is ludicrous. Why would you prevent people from having hammers for that long? They'd probably just use auto-miners instead, and the hammers wouldn't see use.

Yet we used hammers for years before people decided that 3 alum screws is ""too hard""",False,Insulting
Future of MVVMCross with .NET 6.0 and MAUI,ghost,843213515,1,843213515,0,"I currently develop a mobile application using Xamarin.Android and MVVMCross.

My understanding is that Xamarin.Native (Xamarin.Android and Xamarin.iOS)  will now be part of .NET 6.0 when it is released in November 2021

Will this affect the development of MVVMCross and will MVVMCross going to continue to support this.

It would be useful to have an idea of the way forward, just in case there is any significant work required in my application.

Any information would be really helpful
",True,0
Future of MVVMCross with .NET 6.0 and MAUI,Cheesebaron,843213515,2,809569112,0,"Hopefully it is just a matter of changing the TFM to `net-android` and `net-ios` instead of `xamarin.android` and `xamarin.ios`. As for MAUI support, it might not be the first thing to be supported, but the new TFMs most definitely.",False,0
Future of MVVMCross with .NET 6.0 and MAUI,ghost,843213515,3,809597576,0,"@Cheesebaron Thanks for responding so quickly. So it seems that support for Xamarin.Native (as it called today) will continue in MVVMCross beyond the introduction of MAUI.

Forgive my ignorance what does TFM stand for?",False,0
Future of MVVMCross with .NET 6.0 and MAUI,Cheesebaron,843213515,4,809615806,0,TFM stands for Target Framework Moniker üòÑ,False,0
Future of MVVMCross with .NET 6.0 and MAUI,ghost,843213515,5,809636540,0,Thanks,False,0
Future of MVVMCross with .NET 6.0 and MAUI,rafalka,843213515,6,857517654,0,"@Cheesebaron May I know when (approximately) MAUI be supported? 
We starting new project and we'd like to move to MAUI once it will be released (desktop is one of our targets). 
We'd like to know if MvvmCross won't block us with that.",False,0
Future of MVVMCross with .NET 6.0 and MAUI,Cheesebaron,843213515,7,857545098,0,"For now I don't have any plans on supporting something that is still in preview, I simply don't have time to support it. You can ask again when .Net 6 and MAUI is released.

So my estimate would be, unless someone contributes the support, would be _after_ .Net 6 and MAUI has been released, how long after. I don't know.
",False,0
Future of MVVMCross with .NET 6.0 and MAUI,rafalka,843213515,8,857657406,0,"Thanks for prompt reply.

I understand your point. 
At least situation is clear for us now. ",False,0
Future of MVVMCross with .NET 6.0 and MAUI,ADRI082,843213515,9,869517003,0,"I'm migrating a xamarin forms app, which is using mvvmcross, to a MAUI app but when I do this, I get several errors about this framework. Does anyone know if mvvmcross is compatible with MAUI???.

Thank you",False,0
Future of MVVMCross with .NET 6.0 and MAUI,Cheesebaron,843213515,10,873880241,0,We don't have any official support for MAUI. You are welcome to help contribute it.,False,0
Future of MVVMCross with .NET 6.0 and MAUI,thefex,843213515,11,886242490,0,"Ideally, I would like to see direction where MAUI (Forms - because this is just Forms rebranded, perhaps with more resources applied to this) is just complementing MvvmCross development - where you use native dev with MvvmCross and use MAUI to implement easy views (like menu view, ""thank you/info"" view, registration etc..) - just to save some time.
Basically, all of the mobile platforms allows for modular implementation. We have Activities & Fragments  for Android,  ViewControllers for iOS, Pages for WPF etc... Therefore it is technically possible to mix that.
I think if you still use MvvmCross then probably, you've already know that ""single-shared-UI"" is utopia. Does not mean that Forms/MAUI is useless though.

I've just developed proof-of-concept for MvvmCross forms-bridge and will happily move that to Maui when it's stable. See: https://github.com/thefex/MvvmCross.FormsBridge.Template 
I'd like to push pull request for that but don't have access to Windows now (can't build) ;/",False,0
Future of MVVMCross with .NET 6.0 and MAUI,JelleDamen,843213515,12,910226142,0,"> We don't have any official support for MAUI. You are welcome to help contribute it.

I would love to help with the Mvx support for Maui. I haven't looked at the latest previews of .net 6, do you know if it is already usable/complete ?",False,0
Future of MVVMCross with .NET 6.0 and MAUI,Cheesebaron,843213515,13,910358594,0,"I recently gave preview.6 a spin. Seems to be ready soon.

I think the main issue currently is to switch the pipeline to use the new .net6.0 stuff and preview versions of tooling.",False,0
Future of MVVMCross with .NET 6.0 and MAUI,AndrewBryanScott,843213515,14,916870710,0,"its runs ok in emulators, once you start looking at getting it to run and deploy to actual devices you run into roadblocks, there is  Zero documentation yet and  you cant run or deploy the UWP targets yet in VS 2022 17.0.0 preview 3.1, bit of mess still, I and believe it will take a bit longer to get stable once they release it in November with .Net 6",False,0
Future of MVVMCross with .NET 6.0 and MAUI,OmegaRogue,843213515,15,986289100,0,Any Update on this now that .net6 is released?,False,0
Future of MVVMCross with .NET 6.0 and MAUI,Cheesebaron,843213515,16,986293891,0,"@OmegaRogue yeah, I have a branch and PR #4319 open. Just need to finish it up.",False,0
Future of MVVMCross with .NET 6.0 and MAUI,domedellolio,843213515,17,1044923318,0,"Hi @Cheesebaron one of my customer is asking to provide a plan on when we can start approching the migration to MAUI. 
Since our app is built upon MvvmCross is there any plan to start working as in Q1 2022 MS will release a RC?

Thanks ",False,0
Future of MVVMCross with .NET 6.0 and MAUI,Cheesebaron,843213515,18,1046898423,0,"@domedellolio is your customer or you willing to sponsor the work needed for it? It could be either through a contribution or paying for the work required.

I don't have anything in my schedule right now to support it. I would rather want to focus on other parts.",False,0
Future of MVVMCross with .NET 6.0 and MAUI,domedellolio,843213515,19,1047021678,0,"Hi @Cheesebaron thanks for your feedback.
How much effort (ROM) do you think is required here? Also to evaluate a possible involvement / investment

BTW, as part of .NET Foundation will MAUI be part of future at some point for MvvmCross?

Thanks",False,0
Future of MVVMCross with .NET 6.0 and MAUI,Cheesebaron,843213515,20,1047075918,0,"The dotnet foundation doesn't dictate what we support or not.

There is currently only me working on MvvmCross from time to time. So I would prefer to keep the support surface as low as possible.

As for, how much time would be needed to support MAUI, it really depends on what kind of support you need and how deeply you want to integrate.

Since MAUI is using a different IoC container than MvvmCross, I would somehow need a way to support that or as has been planned, switch to the same in MvvmCross.

There is some ongoing work in the .net6.0 migration branch that would help with that, but still a lot work would be needed.

A very careful estimate would be around one weeks worth of work, but there are a lot of unknowns.",False,0
Future of MVVMCross with .NET 6.0 and MAUI,PradeepPappuAto,843213515,21,1135464573,0,"Hi @Cheesebaron , 
Any updates since your last message?
Could you also share the roadmap for the project please?
Thanks",False,Impatience
Future of MVVMCross with .NET 6.0 and MAUI,Cheesebaron,843213515,22,1135709671,0,I don't want to promise anything. I think your best bet would be to just go with another framework. There is not much benefit of using MvvmCross with Xamarin.Forms Apps to begin with.,False,0
Future of MVVMCross with .NET 6.0 and MAUI,thefex,843213515,23,1135721402,0,"@Cheesebaron 
if I can add something, there are benefits with mixing Native MvvmCross and Forms (create some views with Forms and include them in native project) - but that does not require MvvmCross additional interop. 
Using MvvmCross (and especially wasting your precious time just to get Mvx.Maui framework) just to build full-Forms app indeed does not gives any benefits imo. 

However - is there a chance you briefly describe what needs to be done with .net6 MvvmCross branch? To get Maui views work in MvvmCross Native app, I'd need MvvmCross with net6.0. 
I might even pick and do those tasks alone + push PR but I probably will not touch this until official (not RC) Maui release. ",False,0
Future of MVVMCross with .NET 6.0 and MAUI,entdark,843213515,24,1135724413,0,And what about `net6.0-android` and `net6.0-ios`?,False,0
Future of MVVMCross with .NET 6.0 and MAUI,Cheesebaron,843213515,25,1135782120,0,"That is on the way, last week I checked, still having issues with Nfloat which is blocking me, will try again now that 6.0.300 has been released.",False,0
Future of MVVMCross with .NET 6.0 and MAUI,vedecoid,843213515,26,1138230657,0,"""There is currently only me working on MvvmCross from time to time"", in other words, better stay away from the framework unless it already does everything one could possible want...",False,Mocking
Future of MVVMCross with .NET 6.0 and MAUI,nabond251,843213515,27,1138403066,0,"As a fellow framework dev, I just want to say thank you @Cheesebaron for your efforts. This is all free stuff you've carved out precious time to work for us, and that is appreciated.",False,0
Future of MVVMCross with .NET 6.0 and MAUI,nabond251,843213515,28,1138406751,0,"@vedecoid with all due respect, is it fair to expect all that? Alternatively there's nothing stopping you or others from pitching in, as has been stated a few times.",False,Impatience
Future of MVVMCross with .NET 6.0 and MAUI,Cheesebaron,843213515,29,1139625940,0,"> ""There is currently only me working on MvvmCross from time to time"", in other words, better stay away from the framework unless it already does everything one could possible want...

By all means stay away. No one is forcing you to use MvvmCross. However, your comment is off-topic, please leave that out of the discussion.",False,Bitter frustration
Future of MVVMCross with .NET 6.0 and MAUI,vedecoid,843213515,30,1140286471,1,"Off topic ? Someone is asking if you will support .net Maui and the response is : dunno, maybe if i feel like it because i‚Äôm the only one working on it. I will for sure stay away. Started with mvvmcross 10 years afo on release 3.x. Then spend countless hours with every iOS upgrade that also required an mvvmcross upgrade to correct all the breaking changes. We stopped it already after v6 as it just wasn‚Äôt worth it anymore. Also meant we had to rewrite our app from scratch.. Look, i have a lot of admiration for you guys doing all this work basically for free. But if you advertise a framework the way mvvmcross was advertised, with James Montemagno bringing it on Channel9 and beyond, then people expect there more than 1 very brave guy working on it. ",False,Bitter frustration
map.panTo() does not re-center if point within shown bounds,andrewhodel,894598837,1,894598837,0,"`map.panTo()` within bounds that are already shown does not center the map on the point sent to `map.panTo()`.

1.7.1",True,0
map.panTo() does not re-center if point within shown bounds,IvanSanchez,894598837,2,843559866,0,[Works for me](https://plnkr.co/edit/mgmTP9xh7dBfMJUF).,False,0
map.panTo() does not re-center if point within shown bounds,andrewhodel,894598837,3,843570180,0,"Then it doesn‚Äôt wait for a callback from a dragged marker that uses a custom svg icon. 

A call to map.panTo at the end of the dragend event for the marker does not work. 



Thank You,
Andrew Hodel

> On May 18, 2021, at 4:00 PM, Iv√°n S√°nchez Ortega ***@***.***> wrote:
> 
> Ôªø
> Works for me.
> 
> ‚Äî
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or unsubscribe.
",False,0
map.panTo() does not re-center if point within shown bounds,IvanSanchez,894598837,4,843582588,0,"> A call to map.panTo at the end of the dragend event for the marker does not work.

[Yes, it does work](https://plnkr.co/edit/SOWK7sV3uvK2rBOX?preview).",False,0
map.panTo() does not re-center if point within shown bounds,andrewhodel,894598837,5,843623431,0,"@IvanSanchez 

No it does not:

```
dragend event parameter 0 
{distance: 119.02939870817022, type: ""dragend"", target: i, sourceTarget: i}
distance: 119.02939870817022
sourceTarget: i
options: {}
_animRequest: 211
_dragStartTarget: img.leaflet-marker-icon.leaflet-zoom-animated.leaflet-interactive.leaflet-marker-draggable
_element: img.leaflet-marker-icon.leaflet-zoom-animated.leaflet-interactive.leaflet-marker-draggable
_enabled: true
_events: {dragstart: Array(1), predrag: Array(1), drag: Array(1), dragend: Array(1)}
_firingCount: 0
_initHooksCalled: true
_lastEvent: PointerEvent {isTrusted: true, touches: Array(1), changedTouches: Array(1), pointerId: 1, width: 1, ‚Ä¶}
_lastTarget: null
_leaflet_id: 119
_moved: true
_moving: false
_newPos: k {x: 139.36859130859375, y: 382.805419921875}
_parentScale: {x: 1, y: 1, boundingClientRect: DOMRect}
_preventOutline: true
_startPoint: k {x: 538.1398315429688, y: 221.37399291992188}
_startPos: k {x: 132.529052734375, y: 501.6381530761719}
__proto__: i
target: i {options: {‚Ä¶}, _latlng: D, _initHooksCalled: true, _leaflet_id: 117, _mapToAdd: i, ‚Ä¶}
type: ""dragend""
__proto__: Object







map_pins.js:229 calling function ∆í (marker) {
									//console.log('pin was dragged to', marker.latLng.lat(), marker.latLng.lng());

									this.coords.lat = marker.target._latlng.lat;
									this.coords.lng = marker.target._lat‚Ä¶







map_pins.js:230 using parameter 0 for aforementioned function 
{distance: 119.02939870817022, type: ""dragend"", target: i, sourceTarget: i}
distance: 119.02939870817022
sourceTarget: i
options: {}
_animRequest: 211
_dragStartTarget: img.leaflet-marker-icon.leaflet-zoom-animated.leaflet-interactive.leaflet-marker-draggable
_element: img.leaflet-marker-icon.leaflet-zoom-animated.leaflet-interactive.leaflet-marker-draggable
_enabled: true
_events: {dragstart: Array(1), predrag: Array(1), drag: Array(1), dragend: Array(1)}
_firingCount: 0
_initHooksCalled: true
_lastEvent: PointerEvent {isTrusted: true, touches: Array(1), changedTouches: Array(1), pointerId: 1, width: 1, ‚Ä¶}
_lastTarget: null
_leaflet_id: 119
_moved: true
_moving: false
_newPos: k {x: 139.36859130859375, y: 382.805419921875}
_parentScale: {x: 1, y: 1, boundingClientRect: DOMRect}
_preventOutline: true
_startPoint: k {x: 538.1398315429688, y: 221.37399291992188}
_startPos: k {x: 132.529052734375, y: 501.6381530761719}
__proto__: i
target: i {options: {‚Ä¶}, _latlng: D, _initHooksCalled: true, _leaflet_id: 117, _mapToAdd: i, ‚Ä¶}
type: ""dragend""
__proto__: Object
map_pins.js:232 function ended







map_pins.js:233 map.panTo() fails here, map object:  
i {options: {‚Ä¶}, _handlers: Array(6), _layers: {‚Ä¶}, _zoomBoundLayers: {‚Ä¶}, _sizeChanged: false, ‚Ä¶}
attributionControl: i {options: {‚Ä¶}, _attributions: {‚Ä¶}, _initHooksCalled: true, _map: i, _container: div.leaflet-control-attribution.leaflet-control}
boxZoom: i {_map: i, _container: div.leaflet-container.leaflet-touch.leaflet-retina.leaflet-fade-anim.leaflet-grab.leaflet-touch-dra‚Ä¶, _pane: div.leaflet-pane.leaflet-overlay-pane, _resetStateTimeout: 0, _initHooksCalled: true, ‚Ä¶}
doubleClickZoom: i {_map: i, _initHooksCalled: true, _enabled: true}
dragging: i {_map: i, _initHooksCalled: true, _enabled: true, _draggable: i, _positions: Array(0), ‚Ä¶}
keyboard: i {_map: i, _panKeys: {‚Ä¶}, _zoomKeys: {‚Ä¶}, _initHooksCalled: true, _enabled: true, ‚Ä¶}
options: {}
scrollWheelZoom: i {_map: i, _initHooksCalled: true, _enabled: true, _leaflet_id: 98, _delta: 0, ‚Ä¶}
touchZoom: i {_map: i, _initHooksCalled: true, _enabled: true, _leaflet_id: 99}
zoomControl: i {options: {‚Ä¶}, _initHooksCalled: true, _map: i, _leaflet_id: 94, _zoomInButton: a.leaflet-control-zoom-in, ‚Ä¶}
_animateToCenter: D {lat: 38.86644411885283, lng: -106.98246002197267}
_animateToZoom: 10
_animatingZoom: false
_container: div.leaflet-container.leaflet-touch.leaflet-retina.leaflet-fade-anim.leaflet-grab.leaflet-touch-drag.leaflet-touch-zoom
_containerId: 92
_controlContainer: div.leaflet-control-container
_controlCorners: {topleft: div.leaflet-top.leaflet-left, topright: div.leaflet-top.leaflet-right, bottomleft: div.leaflet-bottom.leaflet-left, bottomright: div.leaflet-bottom.leaflet-right}
_events: {moveend: Array(3), zoomend: Array(1), zoomlevelschange: Array(1), unload: Array(4), dblclick: Array(1), ‚Ä¶}
_fadeAnimated: true
_firingCount: 0
_handlers: (6) [i, i, i, i, i, i]
_initHooksCalled: true
_lastCenter: D {lat: 38.86644411885283, lng: -106.98246002197267}
_layers: {100: i, 117: i}
_layersMaxZoom: 20
_layersMinZoom: 0
_leaflet_id: 91
_loaded: true
_mapPane: div.leaflet-pane.leaflet-map-pane
_onResize: ∆í ()
_paneRenderers: {}
_panes: {mapPane: div.leaflet-pane.leaflet-map-pane, tilePane: div.leaflet-pane.leaflet-tile-pane, shadowPane: div.leaflet-pane.leaflet-shadow-pane, overlayPane: div.leaflet-pane.leaflet-overlay-pane, markerPane: div.leaflet-pane.leaflet-marker-pane, ‚Ä¶}
_pixelOrigin: k {x: 53035, y: 99811}
_proxy: div.leaflet-proxy.leaflet-zoom-animated
_resizeRequest: 113
_size: k {x: 270, y: 1000}
_sizeChanged: false
_targets: {92: i, 118: i}
_zoom: 10
_zoomAnimated: true
_zoomBoundLayers: {100: i}
__proto__: i
```",False,0
map.panTo() does not re-center if point within shown bounds,andrewhodel,894598837,6,843624394,0,No error and the map does not pan when the zoom is set as such and the coordinates are set as such with a marker that is moved to said position within the bounds at zoom shown.,False,0
map.panTo() does not re-center if point within shown bounds,andrewhodel,894598837,7,843624524,0,At the map size shown.,False,0
map.panTo() does not re-center if point within shown bounds,andrewhodel,894598837,8,843628107,0,"```
                                marker.addEventListener('dragend', function(marker) {
                                        console.log('dragend event parameter 0', marker);
                                        console.log('calling function', this.pin.dragCb);
                                        console.log('using parameter 0 for aforementioned function', marker);
                                        this.pin.dragCb(marker);
                                        console.log('function ended');
                                        console.log('map.panTo() fails here, map object: ', globals.gmap);
                                        globals.gmap.panTo([this.pin.obj.latitude, this.pin.obj.longitude]);
                                }.bind({pin: map_pins.pins[c]}));
```



https://user-images.githubusercontent.com/741705/118734965-1749ec80-b805-11eb-8a19-29a6eef0a846.mov


What more proof do you need?  That is everything.",False,Impatience
map.panTo() does not re-center if point within shown bounds,andrewhodel,894598837,9,843628726,0,"Here is this.pin

```
this.pin 
{obj: {‚Ä¶}, info: div, draggable: true, marker: i, dragCb: ∆í}
dragCb: ∆í ()
draggable: true
info: div
marker: i
dragging: i {_marker: i, _initHooksCalled: true, _enabled: true, _draggable: i}
options: {title: ""af-40mp-625rmr"", icon: i, zIndexOffset: 1, draggable: true}
_events: {remove: Array(2), dragend: Array(1), click: Array(1)}
_firingCount: 0
_icon: img.leaflet-marker-icon.leaflet-zoom-animated.leaflet-interactive.leaflet-marker-draggable
_initHooksCalled: true
_latlng: D {lat: 39.001823432304754, lng: -106.97038027457896}
_leaflet_id: 103
_map: i {options: {‚Ä¶}, _handlers: Array(6), _layers: {‚Ä¶}, _zoomBoundLayers: {‚Ä¶}, _sizeChanged: false, ‚Ä¶}
_mapToAdd: i {options: {‚Ä¶}, _handlers: Array(6), _layers: {‚Ä¶}, _zoomBoundLayers: {‚Ä¶}, _sizeChanged: false, ‚Ä¶}
_shadow: null
_zIndex: 501
_zoomAnimated: true
__proto__: i
obj:
alertDisabled: 0
channel: 0
clientInfo: ""ispapp-snmp-relay-0.1""
createdAt: 1620056840
fw: null
fwVersion: null
groupId: ""5f88e6a24ba991445d8ec738""
hardwareCpuInfo: null
hardwareMake: null
hardwareModel: null
hardwareModelNumber: null
hardwareSerialNumber: null
key: ""asdfasdfasdf""
lastConfigRequest: 1621359482
lastUpdate: 1621379634
latitude: 38.86706476159009
login: ""af-40mp-625rmr""
longitude: -106.98185137007387
name: ""af-40mp-625rmr""
notes: ""testing snmp relay""
os: null
osBuildDate: null
osVersion: ""null""
outsideIp: (8) [""76.186.24.140"", ""3.233.165.14"", ""76.186.24.140"", ""3.233.165.14"", ""76.186.24.140"", ""3.233.165.14"", ""76.186.24.140"", ""3.233.165.14""]
outsideIpChangeTsSeconds: 1620146969
outsideIpChanges: 1
reboot: 0
ssid: ""undefined""
uptime: 1809293
usingWebSocket: true
vlan: ""undefined""
wanIp: ""63.151.94.152""
wds: ""undefined""
wirelessBeaconInt: 0
wirelessChannel: 0
wirelessConfigs: []
wirelessMode: ""ap_bridge""
zoneId: ""5f88e6954ba991445d8ec736""
_id: ""60901b075223241f3a472497""
__proto__: Object
__proto__: Object
```",False,0
map.panTo() does not re-center if point within shown bounds,andrewhodel,894598837,10,843631489,0,"Do you want me to reopen another issue with this as the first comment or are you going to reopen this one?



Thank You,
Andrew Hodel

> On May 18, 2021, at 4:41 PM, Iv√°n S√°nchez Ortega ***@***.***> wrote:
> 
> Ôªø
> A call to map.panTo at the end of the dragend event for the marker does not work.
> 
> Yes, it does work.
> 
> ‚Äî
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or unsubscribe.
",False,0
map.panTo() does not re-center if point within shown bounds,andrewhodel,894598837,11,843638361,0,"With a timeout:

```
                                        setTimeout(function() {
                                                console.log('timeout finished');
                                                globals.gmap.panTo([this.pin.obj.latitude, this.pin.obj.longitude]);
                                        }.bind({pin: this.pin}), 4000);
```

```
function ended
map_pins.js:233 map.panTo() fails here, map object:  i¬†{options: {‚Ä¶}, _handlers: Array(6), _layers: {‚Ä¶}, _zoomBoundLayers: {‚Ä¶}, _sizeChanged: false,¬†‚Ä¶}
map_pins.js:234 this.pin {obj: {‚Ä¶}, info: div, draggable: true, marker: i, dragCb: ∆í}
map_pins.js:236 timeout finished
```

That makes no sense.",False,0
map.panTo() does not re-center if point within shown bounds,IvanSanchez,894598837,12,843639953,1,Stop copy-pasting your debug output and show a minimal reproducible example. We have the bug report templates for a reason.,False,Impatience
[VITA] Collect issues and clean up the port,frangarcj,906427489,1,906427489,0,"This issues is created in order to collect all remaining issues from vita port in order to fix them. It will be constantly updated

- [X] Broken Video Scaling even when using ""core provided"". #12456 
- [ ] Broken NetPlay.
- [X] Menu Sounds don't work at all. #12457 
- [ ] Check assets.",True,0
[VITA] Collect issues and clean up the port,DoctorWhosThat,906427489,2,850826583,0,"Video Scaling is broken in general, even when having aspect ratio as ""core provided"" (not sure if actual menu driver issue though).
NetPlay is just completely broken and does not work at all.
Menu Sounds don't work at all (not that anyone cares, but thought I'd mention it anyway).
Assets have to be downloaded and added manually for certain menu drivers",False,0
[VITA] Collect issues and clean up the port,DoctorWhosThat,906427489,3,850864993,0,"i didn't know if this could be actually added or as it's a request, would be completely separate?",False,0
[VITA] Collect issues and clean up the port,LibretroAdmin,906427489,4,1234740921,0,@Cthulhu-throwaway I can assume we can check 'Broken netplay' now as resolved for the Vita port?,False,0
[VITA] Collect issues and clean up the port,DoctorWhosThat,906427489,5,1234761701,0,"> @Cthulhu-throwaway I can assume we can check 'Broken netplay' now as resolved for the Vita port?

Over my dead body, you're not!

As has this been throughly tested? If not, it needs to be either removed or replaced with a alternative.

As I'm sick and tired of this being actually overlooked entirely, as the actual developers don't care about this at all.",False,Bitter frustration
[VITA] Collect issues and clean up the port,ghost,906427489,6,1234765809,0,Wow... Not even going to bother.,False,Impatience
[VITA] Collect issues and clean up the port,DoctorWhosThat,906427489,7,1234767870,1,"> Wow... Not even going to bother.

That's exactly what I mean, can't be bothered...

Just do whatever! As no-one even cared about this issue in the first place anyway!",False,Bitter frustration
"If OPENPGM_FOUND is set, link to libpgm.",mbalmer,907232677,1,907232677,0,This solves issue #4200,True,0
"If OPENPGM_FOUND is set, link to libpgm.",bluca,907232677,2,851357832,0,Please add a relicense statement as described in https://github.com/zeromq/libzmq/tree/master/RELICENSE - thanks,False,0
"If OPENPGM_FOUND is set, link to libpgm.",mbalmer,907232677,3,851359117,0,"> Please add a relicense statement as described in https://github.com/zeromq/libzmq/tree/master/RELICENSE <https://github.com/zeromq/libzmq/tree/master/RELICENSE> - thanks
> 
Do you really consider a 9 lines diff in CMakeLists.txt a code contribution that needs a license?

",False,Impatience
"If OPENPGM_FOUND is set, link to libpgm.",mbalmer,907232677,4,851360791,0,">> Please add a relicense statement as described in https://github.com/zeromq/libzmq/tree/master/RELICENSE <https://github.com/zeromq/libzmq/tree/master/RELICENSE> - thanks
>> 
> Do you really consider a 9 lines diff in CMakeLists.txt a code contribution that needs a license?
> 

After all I do not (yet?) hold or claim any copyrights in an ZeroMQ code.  This was a mere bugfix.

Different story are my Lua bindings at https://github.com/arcapos/luazmq <https://github.com/arcapos/luazmq> and https://github.com/arcapos/mqlua <https://github.com/arcapos/mqlua>, but then these are not part of the ZeroMQ project.

",False,0
"If OPENPGM_FOUND is set, link to libpgm.",bluca,907232677,5,851372951,0,"> Please add a relicense statement as described in https://github.com/zeromq/libzmq/tree/master/RELICENSE <https://github.com/zeromq/libzmq/tree/master/RELICENSE> - thanks
> Do you really consider a 9 lines diff in CMakeLists.txt a code contribution that needs a license?

(probably?) not, but Github makes it very obvious when a first contribution is made, so it's easier to ask for it at that point, rather than tracking who sent it and who didn't if you then later submit a larger PR. I know it's annoying, but believe me, trying to get this relicensing sorted is orders of magnitude more work.",False,Impatience
"If OPENPGM_FOUND is set, link to libpgm.",mbalmer,907232677,6,851395118,0,"

> Am 31.05.2021 um 11:54 schrieb Luca Boccassi ***@***.***>:
> 
> Ôªø
> Please add a relicense statement as described in https://github.com/zeromq/libzmq/tree/master/RELICENSE https://github.com/zeromq/libzmq/tree/master/RELICENSE - thanks
> Do you really consider a 9 lines diff in CMakeLists.txt a code contribution that needs a license?
> 
> (probably?) not, but Github makes it very obvious when a first contribution is made, so it's easier to ask for it at that point, rather than tracking who sent it and who didn't if you then later submit a larger PR. I know it's annoying, but believe me, trying to get this relicensing sorted is orders of magnitude more work.
> 

Well, I have no plans to contribute code, I am a mere (long time) user of ZeroMQ, and if it does not build, I invest some time trying to find out what is wrong. Maybe WITH_OPENPGM should be added to the automated tests.
> ‚Äî
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub, or unsubscribe.
",False,Bitter frustration
"If OPENPGM_FOUND is set, link to libpgm.",mbalmer,907232677,7,852035632,1,Are you not going to merge this?,False,Impatience
QST: Inconsistent behaviour in checking number of fields per row while read_csv(),buhtz,908088395,1,908088395,0,"I post this as a ""Question"" because I am quite new to `pandas`. So maybe I miss some understandings and the ""problem"" described by me is by design and you have good reasons for that.
I use pandas 1.2.4, with Python 3.9.4 on Windows 10 64 bit.

As a user I would expect that pandas check the number of fields per row when importing via csv file. But IMHO it does not in all cases.

# Example 1

Here is a csv file __without header__ and but a set `names=` attribute with three fields. So pandas should be able to know how many fields/columns should be in the CSV file. The second row contains 4 instead of 3 fields.

```
import pandas
import io

csv_without_header = io.StringIO(
'A;B;C\n'
'D;E;X;Y\n'
'F;G;H'
)

df = pandas.read_csv(csv_without_header, encoding='utf-8', sep=';',
                     header=None,
                     names=['First', 'Second', 'Third'])
```

Pandas import this without warnrings or errors. The 4th field in the 2nd row is simply ignored.

# Example 2

I added a header line into the csv file with again three fields.
So pandas should be able to know how many fields/columns should be in the CSV file.
And again the second row contains 4 instead of 3 fields.

```
csv_with_header = io.StringIO(
'First;Second;Third\n'
'A;B;C\n'
'D;E;X;Y\n'
'F;G;H'
)

df = pandas.read_csv(csv_with_header, encoding='utf-8', sep=';')
```

Here an error occurs as I expect.
`pandas.errors.ParserError: Error tokenizing data. C error: Expected 3 fields in line 3, saw 4` 

# Example 3

There are less then 3 fields in the 2nd row. Again here is no warning or error. The missing field is set with `NaN`. And here it does not matter if you give the number of (expected) fields via header line in the CSV or via `names=` attribute.

```
csv_with_header = io.StringIO(
'First;Second;Third\n'
'A;B;C\n'
'D;Y\n'
'F;G;H'
)

csv_without_header = io.StringIO(
'A;B;C\n'
'D;Y\n'
'F;G;H'
)

df_a = pandas.read_csv(csv_with_header, encoding='utf-8', sep=';')
df_b = pandas.read_csv(csv_without_header, encoding='utf-8', sep=';', names=['First', 'Second', 'Third'])
```

Want I want is to import CSV files and be informed if there are to many or less then the expected number of fields in any row.",True,0
QST: Inconsistent behaviour in checking number of fields per row while read_csv(),testingcan,908088395,2,852051470,0,"To me this behaviour is quite consistent with what I would expect a CSV import to do. Having a value outside of the defined columns just doesn't make any sense, so the `ParserError` is appropriate here. However, having a `NaN` or missing value is logically sound in my opinion and happens more often than not. 

For example, if the contents of your CSV file were from some input, where you have to enter three fields, but the CSV contains a row with four columns, this means you have a bigger problem. If you have a row with only two columns and one `NaN` that just means that one input was left blank and you need to handle the data appropriately. Since this is a case-by-case basis (e.g. do you still consider this a valid datum, do you ignore, do you populate it with default values?), but the other case is almost always a problem with the file, I think this behaviour makes sense. 

In any way, checking for empty values after importing is always a good step, using - in your third example - `df_a.isnull()` or `df_a.isnull().sum`. ",False,0
QST: Inconsistent behaviour in checking number of fields per row while read_csv(),phofl,908088395,3,852386847,0,"This is a duplicate, please search the issue tracker,",False,0
QST: Inconsistent behaviour in checking number of fields per row while read_csv(),buhtz,908088395,4,853073653,0,"It is impolite to assume that each user opening an issue is stupid and lazy.
Of course I search the issue tracker. I assume I used the wrong keywords.

But it is also impolite to point to other Issues without giving the links.
It is a feature of [HyperText](https://de.wikipedia.org/wiki/Hypertext) to link your words to content. ;)

If you want contributers to pandas you should work on your attitude.

And as a ""pandas member"" you should know how to use GitHubs Issue tracker.
if there is a duplicate Issue. Link to the origin Issue and close the duplicate.

Anything else is waste of ressources.",False,Insulting
QST: Inconsistent behaviour in checking number of fields per row while read_csv(),jreback,908088395,5,853127909,0,@Codeberg-AsGithubAlternative-buhtz we have 3500 issue and *very* limited reviewers / volunteers. Please help us out here.,False,0
QST: Inconsistent behaviour in checking number of fields per row while read_csv(),lithomas1,908088395,6,853305183,0,"Hi @Codeberg-AsGithubAlternative-buhtz, 
1 is a bug and is related to #40333/#22144 and may be fixed by #40629
3 is not a bug. It is mentioned in the user guide. https://pandas.pydata.org/docs/user_guide/io.html#handling-bad-lines. 
Can you close the issue if I have answered your questions.
Thanks.",False,0
QST: Inconsistent behaviour in checking number of fields per row while read_csv(),buhtz,908088395,7,853419669,0,"Thanks for the Issues and PR. I will monitor them.

For 3: Yes the (IMHO inconsistent) behavior is described in docs. But it does not make it right. There should be no difference between the handling of to few and to many columns. But it is just my opinion.
But because of this situation I can never trust pandas while importing CSV files. Before using pandas I have to do some pedantic checks by my own.",False,0
QST: Inconsistent behaviour in checking number of fields per row while read_csv(),MarcoGorelli,908088395,8,853652854,1,"> Anything else is waste of ressources.

Given the sheer volume and quality of (voluntary!) work produced (see https://github.com/pandas-dev/pandas/commits?author=phofl for a start, and that's excluding reviews + issue triage), is demanding that they search the issue tracker for you really the best use of their resources?

> you should know how to use GitHubs Issue tracker

Such comments are unwelcome - you've been warned",False,Bitter frustration
[BUG] God of War 1 - FMV bug after 1.4.0,morsyfadl,914128991,1,914128991,0,"God of War's FMV PSS and PSW files are rednered with a black bar at the bottom, which is handeled by naturally the game engine somehow.

When running the game on the 1.6.0 release this bar is present and I cant get rid of it at all.
I tried the 1.4.0 release and it worked correctly on the default settings without changing anything

this issue exists in 1.7.0 latest dev build as well

Describe the bug
Black bar at the bottom of FMVs. 

To Reproduce
Run God of War 1 with a version of pcsx2 later than 1.4.0

Expected behavior
It shouldnt appear

GS Settings
Default settings

Emulation Settings
Default settings

GS Window Screenshots

System Info (please complete the following information):

PCSX2 Revision: 1.6.0 / 1.7.0 dev rev 1284
OS:  Win 10
CPU: Ryze 3600
GPU: RX470
Logs and Dumps",True,0
[BUG] God of War 1 - FMV bug after 1.4.0,F0bes,914128991,2,856365754,0,"There is a provided issue template when you create an issue, please follow it.
Does this issue happen in software mode? (Pressing F9 or changing the renderer in gsdx)

Issue template:

**Describe the bug**
<!-- A clear and concise description of what the bug is. -->

**To Reproduce**
<!-- Steps to reproduce the behavior. -->

**Expected behavior**
<!-- A clear and concise description of what you expected to happen. -->

**GS Settings**
<!-- Any non-default settings for GS. -->
<!-- If you don't want to list them out, please provide screenshots of your configuration window (including hw hacks if enabled). -->

**Emulation Settings**
<!-- Any non-default core settings. -->
<!-- If you don't want to list them out, please provide screenshots of your configuration window. -->
<!-- Please note that the safe preset works for most games. -->
<!-- MTVU can have some compatibility issues so please disable it before making a report. -->
<!-- If you need to modify the settings manually because a game requires you to do so to work, please state that explicitly. -->

**GS Window Screenshots**
<!-- If your issue is graphical in nature and you think screenshots will help illustrate your issue, you may do that here. -->

**System Info (please complete the following information):**
 -   PCSX2 Revision: <!-- e.g. dev-525 -->
 -   OS: <!-- e.g. Windows 10 -->
 -   CPU: <!-- e.g. i5-7600 -->
 -   GPU: <!-- e.g. GTX 1070 -->

**Logs and Dumps**",False,0
[BUG] God of War 1 - FMV bug after 1.4.0,morsyfadl,914128991,3,856371105,0,"Software mode doesnt change a thing. I tried pressing F9 and tried turning on the option to switch to SW automatically during FMV, doesnt solve it.

Also I did my best with the template. Sorry I rarely post to github.",False,0
[BUG] God of War 1 - FMV bug after 1.4.0,xuru-thor,914128991,4,856431782,0,"Experiencing the exact same thing, works perfectly on version 1.4.0 and 1.5.0. 1.6.0 also has bugs with effects like fog, while previous versions work fine.",False,0
[BUG] God of War 1 - FMV bug after 1.4.0,weirdbeardgame,914128991,5,856432217,0,Do you know which version of 1.5 it doesn't work on?,False,0
[BUG] God of War 1 - FMV bug after 1.4.0,xuru-thor,914128991,6,856432910,0,"> Do you know which version of 1.5 it doesn't work on?

Sorry, I don't. I'm using v1.5.0-dev-2143-g1d983a681.",False,0
[BUG] God of War 1 - FMV bug after 1.4.0,refractionpcsx2,914128991,7,856560647,0,"Looks fine in software mode for me.  That black bar at the bottom, there doesn't happen to be one at the top too, is there?",False,0
[BUG] God of War 1 - FMV bug after 1.4.0,morsyfadl,914128991,8,856790251,0,"> Looks fine in software mode for me. That black bar at the bottom, there doesn't happen to be one at the top too, is there?

What version are you using? And can you share your settings?

This is how the game looks.
![gsdx_20210608154847](https://user-images.githubusercontent.com/35445029/121196928-2c002a00-c871-11eb-8103-3c10dfeedf16.png)

This is how FMV's are supposed to look and how they actually do look like using 1.4.0
![gsdx_20210608155307](https://user-images.githubusercontent.com/35445029/121197774-e132e200-c871-11eb-9eba-5f93918139b2.png)

This is the black bar when using 1.6.0
![gsdx_20210608154917](https://user-images.githubusercontent.com/35445029/121197111-54882400-c871-11eb-9d04-d3bb946f4e9f.png)


",False,0
[BUG] God of War 1 - FMV bug after 1.4.0,refractionpcsx2,914128991,9,856793358,0,"I'm just using 1.7 build 1286, the bars I get are because my window isn't exactly 4:3, but if I correct it I get the below.

![image](https://user-images.githubusercontent.com/6278726/121198659-366af580-c86a-11eb-96be-be2fc05d6163.png)

Just to note, mine is the PAL version
",False,0
[BUG] God of War 1 - FMV bug after 1.4.0,morsyfadl,914128991,10,857125735,0,"> I'm just using 1.7 build 1286, the bars I get are because my window isn't exactly 4:3, but if I correct it I get the below.
> 
> ![image](https://user-images.githubusercontent.com/6278726/121198659-366af580-c86a-11eb-96be-be2fc05d6163.png)
> 
> Just to note, mine is the PAL version

@refractionpcsx2 
I have only the NTSC version of both GoW1 and 2.  And from your screenshot I can see the internal resolution is different. 
iirc Sony Santa Monica rendered the FMV's in ddifferent resolutions for each region, 480 for NTSC and 576i for PAL. Which is why I believe you dont get the same bug I get.

This is what observed so far: on the my NTSC version
1.6.0 / 1.7.0 r1286:

GoW1 Has the Black border bug
GoW1 game runs at 512x448 inerlaced and 640x448 w/ progressive scan on
GoW1 FMV's run at 640x480 regardless
GoW1 FMV files are PSS files rendered at 640x480

on 1.4.0 the resolutions are the same, except that the bug happenes when I enable progressive scan.

GoW2 doesnt have this issue at all because the FMVs are at 640x448

Ibelieve thats the reason you dont experience the same bug because PAL FMV files are rendered at a higher res than NTSC

I extracted the files and examined them for both games to exmine them



",False,0
[BUG] God of War 1 - FMV bug after 1.4.0,RedPanda4552,914128991,11,858138204,0,"Check your Zoom setting. Config > Emulation Settings > GS Window. If it is not 100%, set it to 100% exactly and see if the issue persists. I have seen before that starting the emu with zoom enabled and certain combinations of either full screen or windowed modes can result in this jarring offset in other games.",False,0
[BUG] God of War 1 - FMV bug after 1.4.0,morsyfadl,914128991,12,859123433,0,"> Check your Zoom setting. Config > Emulation Settings > GS Window. If it is not 100%, set it to 100% exactly and see if the issue persists. I have seen before that starting the emu with zoom enabled and certain combinations of either full screen or windowed modes can result in this jarring offset in other games.

zomm is 100%
As I said, I'm using defualt settings and fresh installs of every version of the emulator.
And rhis is an *FMV* issue not a game issue. Meaning it only happenes when prerendered cutscenes are played, not during gameplay..

And the previous comments the resolutions I mentiond are internal resolutions.",False,0
[BUG] God of War 1 - FMV bug after 1.4.0,lightningterror,914128991,13,869145342,0,"Here is the list of all 1.5 builds, see if you can track down which build introduced the issue.
https://gist.github.com/turtleli/a7de466bdf0aac4d028be5fa82a31de2",False,0
[BUG] God of War 1 - FMV bug after 1.4.0,xuru-thor,914128991,14,869395680,0,"> Here is the list of all 1.5 builds, see if you can track down which build introduced the issue.
> https://gist.github.com/turtleli/a7de466bdf0aac4d028be5fa82a31de2

I did some testing and I believe it was introduced on build 1729 (v1.5.0-dev-102-g5bf12519d).",False,0
[BUG] God of War 1 - FMV bug after 1.4.0,refractionpcsx2,914128991,15,869471239,0,"https://github.com/PCSX2/pcsx2/commit/5bf12519dae05a5adf0bbd69ecc3671ce54a7b0c

Just to link directly to the commit which you say broke it, it also makes sense that this changed the behaviour. Maybe Greg meant w <= 640

Edit: maybe not, maybe something else hasn't been taken in to account...",False,0
[BUG] God of War 1 - FMV bug after 1.4.0,ghost,914128991,16,997705907,0,@refractionpcsx2 why is pcsx2 wiki saying this is fixed when it is not because still dev build of pcsx2 1.7.0 so can find a real fix to this bug because been broken mouths now.,False,Bitter frustration
[BUG] God of War 1 - FMV bug after 1.4.0,refractionpcsx2,914128991,17,997709682,0,"> @refractionpcsx2 why is pcsx2 wiki saying this is fixed when it is not because still dev build of pcsx2 1.7.0 so can find a real fix to this bug because been broken mouths now.

I don't see that on the wiki? But we don't maintain the Wiki we just host it, it's for users to update, it's a wiki.

we need to think about what to do with this. We could remove the NTSC Saturation modification that's in there, but then people would bitch there's huge black lines either side,  so need to see if there's anything based on the clock settings we can detect to use the saturation instead of arbitrarily basing it on the set width/height",False,0
[BUG] God of War 1 - FMV bug after 1.4.0,ghost,914128991,18,997719667,0,@refractionpcsx2 then Add a gamedb hack for gow1 to fix black bars in fmv movies. Gamedb hacks are magic blutts that will fixed this bug without adding more bugs to outher games into pcsx2.,False,0
[BUG] God of War 1 - FMV bug after 1.4.0,refractionpcsx2,914128991,19,997720692,0,"the GS isn't rigged up to the GameDB at the moment, though doing so is something we're considering (more of a pain in the ass because it's over a thread).  But we'll get around to it.

Anyway, it's not the end of the world, you aren't missing anything, there's just some extra black pixels at the bottom, the whole FMV is there, you'll survive.  If it's so much of a problem, you're welcome to play GoW on 1.4.",False,0
[BUG] God of War 1 - FMV bug after 1.4.0,ghost,914128991,20,997723201,0,Thier other problems besides this like green and problems line with upscale bug no one report yet thier is fixed need a lot of settings changed for it to work.,False,0
[BUG] God of War 1 - FMV bug after 1.4.0,refractionpcsx2,914128991,21,997724945,0,"Which upscaling bug? It isn't related to this issue, but instead of complaining about bugs which haven't been reported yet, maybe consider reporting them, then we can look in to them.

However the only upscaling bug I know is the green/purple fringing, which we probably can't do a lot about easily, it's due to how the PS2 renders effects and it expects the original resolution, so upscaling it causes problems.",False,0
[BUG] God of War 1 - FMV bug after 1.4.0,refractionpcsx2,914128991,22,997731220,0,"Well, that's just the nature of upscaling PS2 games, I'm afraid.  The console kinda sucks for that and our life is a lot harder than say the Gamecube (and most definitely the PS3)",False,0
[BUG] God of War 1 - FMV bug after 1.4.0,prafullpcsx2,914128991,23,997920621,0,"We are still discussing this? Wasn't this fixed with memory wrapping? 

Edit: oh alright, that was a separate issue.",False,0
[BUG] God of War 1 - FMV bug after 1.4.0,ghost,914128991,24,998385216,1,"> Well, that's just the nature of upscaling PS2 games, I'm afraid. The console kinda sucks for that and our life is a lot harder than say the Gamecube (and most definitely the PS3)

At guys trying fix this bug unlike asshole at rpcs3 when report bug happened on psn ver of this game because does but they just closed the problem so yeah guys should fixed bug because think once done pcsx2 will batter then rpcs3 in this game.",False,Vulgarity
3.4.2: setuptools build_sphinx command fails,kloczek,917355114,1,917355114,0,"Looks like something is missing and probably some defailt location of the matplotlibrc file is used. This is causeing tha standard way of generarting sphinx documentation fails.
```console
[tkloczko@barrel matplotlib-3.4.2]$ PYTHONPATH=PYTHONPATH=$PWD/build/$(cd build; ls -1d lib*) /usr/bin/python3 setup.py build_sphinx -b man --build-dir build/sphinx

Edit setup.cfg to change the build options; suppress output with --quiet.

BUILDING MATPLOTLIB
  matplotlib: yes [3.4.2]
      python: yes [3.8.9 (default, Apr  7 2021, 13:42:48)  [GCC 11.0.1 20210324
                  (Red Hat 11.0.1-0)]]
    platform: yes [linux]
       tests: yes [installing]
      macosx: no  [Mac OS-X only]

running build_sphinx
Running Sphinx v4.0.2

Configuration error:
There is a programmable error in your configuration file:

Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/sphinx/config.py"", line 323, in eval_config_file
    exec(code, namespace)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/conf.py"", line 19, in <module>
    import matplotlib
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/__init__.py"", line 825, in <module>
    rcParamsDefault = _rc_params_in_file(
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/__init__.py"", line 730, in _rc_params_in_file
    with _open_file_or_url(fname) as fd:
  File ""/usr/lib64/python3.8/contextlib.py"", line 113, in __enter__
    return next(self.gen)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/__init__.py"", line 708, in _open_file_or_url
    with open(fname, encoding=encoding) as f:
FileNotFoundError: [Errno 2] No such file or directory: '/home/tkloczko/rpmbuild/BUILD/share/matplotlib/mpl-data/matplotlibrc'
```
",True,Impatience
3.4.2: setuptools build_sphinx command fails,QuLogic,917355114,2,859037560,0,"The _standard_ way to build Sphinx docs is [with `sphinx-build`](https://www.sphinx-doc.org/en/master/usage/quickstart.html#running-the-build). Using `setup.py` is in general discouraged as the trend is to move to declarative packaging. There's little point to using the wrapper instead of the command directly.

> [tkloczko@barrel matplotlib-3.4.2]$ PYTHONPATH=PYTHONPATH=$PWD/build/$(cd build; ls -1d lib*) /usr/bin/python3 setup.py build_sphinx -b man --build-dir build/sphinx

This fails because you didn't set `PYTHONPATH` correctly.",False,0
3.4.2: setuptools build_sphinx command fails,kloczek,917355114,3,859120307,0,"Sorry I've messed what I've copied from clipboard.
```console
[tkloczko@barrel matplotlib-3.4.2]$ PYTHONPATH=/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8 /usr/bin/python3 setup.py build_sphinx -b man --build-dir build/sphinx

Edit setup.cfg to change the build options; suppress output with --quiet.

BUILDING MATPLOTLIB
  matplotlib: yes [3.4.2]
      python: yes [3.8.9 (default, Apr  7 2021, 13:42:48)  [GCC 11.0.1 20210324
                  (Red Hat 11.0.1-0)]]
    platform: yes [linux]
       tests: yes [installing]
      macosx: no  [Mac OS-X only]

running build_sphinx
Running Sphinx v4.0.2

Configuration error:
There is a programmable error in your configuration file:

Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/sphinx/config.py"", line 323, in eval_config_file
    exec(code, namespace)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/conf.py"", line 19, in <module>
    import matplotlib
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/__init__.py"", line 825, in <module>
    rcParamsDefault = _rc_params_in_file(
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/__init__.py"", line 730, in _rc_params_in_file
    with _open_file_or_url(fname) as fd:
  File ""/usr/lib64/python3.8/contextlib.py"", line 113, in __enter__
    return next(self.gen)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/__init__.py"", line 708, in _open_file_or_url
    with open(fname, encoding=encoding) as f:
FileNotFoundError: [Errno 2] No such file or directory: '/home/tkloczko/rpmbuild/BUILD/share/matplotlib/mpl-data/matplotlibrc'
```
",False,0
3.4.2: setuptools build_sphinx command fails,QuLogic,917355114,4,859177009,0,"> ```
> FileNotFoundError: [Errno 2] No such file or directory: '/home/tkloczko/rpmbuild/BUILD/share/matplotlib/mpl-data/matplotlibrc'
> ```

That's not where Matplotlib looks for `matplotlibrc`; you have some custom patches.",False,0
3.4.2: setuptools build_sphinx command fails,kloczek,917355114,5,859780347,0,"> That's not where Matplotlib looks for `matplotlibrc`; you have some custom patches.

I've been trying to pass path to that file in $MATPLOTLIBRC encv variable but seems it does not work.
So it is not possible to generate documentation without installing matplotlib??",False,0
3.4.2: setuptools build_sphinx command fails,tacaswell,917355114,6,859787138,0,Matplotlib must be installed to build the docs.  We need to be able to import Matplotlib to extract the docstrings and to run the examples / generate all of the output images.,False,0
3.4.2: setuptools build_sphinx command fails,kloczek,917355114,7,859789208,0,"And how to  that building matplotlib fron non-root account???
",False,Impatience
3.4.2: setuptools build_sphinx command fails,kloczek,917355114,8,859790025,0,"And hwy it tries to find /home/tkloczko/rpmbuild/BUILD/share/matplotlib/mpl-data/matplotlibrc???
In my case source root directoey is in /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2
",False,Impatience
3.4.2: setuptools build_sphinx command fails,kloczek,917355114,9,859790528,0,And other thing .. why it must have that file and cennot use some default settings in case if none of the matplotlibrc would be found?,False,Impatience
3.4.2: setuptools build_sphinx command fails,jklymak,917355114,10,859795394,0,"- Instructions for building matplotlib from source are here: https://matplotlib.org/stable/users/installing_source.html#install-from-source
- Instructions for building the docs are here: https://matplotlib.org/stable/devel/documenting_mpl.html  
 
If you need further help because you want to use steps we do not recommend, please discuss at https://discourse.matplotlib.org.  The bug reporter is not an appropriate venue unless you have identified a specific bug or suggestion for improving our builds. Thanks for your understanding.  ",False,Impatience
3.4.2: setuptools build_sphinx command fails,kloczek,917355114,11,859795865,0,"Looks like cemmenst in https://github.com/matplotlib/matplotlib/blob/master/lib/matplotlib/__init__.py#L501-L513 has nothing to do with waht actually below code does:
```comment
[tkloczko@barrel matplotlib-3.4.2]$ PYTHONPATH=/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8 strace -fe trace=file /usr/bin/python3 setup.py build_sphinx -b man --build-dir build/sphinx 2>&1 | grep matplotlibrc
openat(AT_FDCWD, ""matplotlibrc.template"", O_RDONLY|O_CLOEXEC) = 3
openat(AT_FDCWD, ""lib/matplotlib/mpl-data/matplotlibrc"", O_WRONLY|O_CREAT|O_TRUNC|O_CLOEXEC, 0666) = 3
[pid 1007061] openat(AT_FDCWD, ""/home/tkloczko/rpmbuild/BUILD/share/matplotlib/mpl-data/matplotlibrc"", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)
FileNotFoundError: [Errno 2] No such file or directory: '/home/tkloczko/rpmbuild/BUILD/share/matplotlib/mpl-data/matplotlibrc'
```
Comment or code below should be fixed.
",False,0
3.4.2: setuptools build_sphinx command fails,kloczek,917355114,12,859861002,0,"> * Instructions for building matplotlib from source are here: https://matplotlib.org/stable/users/installing_source.html#install-from-source
> * Instructions for building the docs are here: https://matplotlib.org/stable/devel/documenting_mpl.html
> 
> If you need further help because you want to use steps we do not recommend, please discuss at https://discourse.matplotlib.org. The bug reporter is not an appropriate venue unless you have identified a specific bug or suggestion for improving our builds. Thanks for your understanding.

This ticket is not about installation but generate sphinx documenation.
In my case I've copieed Fedora patch which is buggy and this is why setuptools sphoinx_build has been trying to find matplotlibrc file.
Here is that patch
```patch
From 92b11ded669267100e3c6858578351cad3749cd0 Mon Sep 17 00:00:00 2001
From: Elliott Sales de Andrade <quantum.analyst@gmail.com>
Date: Wed, 27 Sep 2017 19:35:59 -0400
Subject: [PATCH 1/3] matplotlibrc path search fix

Signed-off-by: Elliott Sales de Andrade <quantum.analyst@gmail.com>
---
 lib/matplotlib/__init__.py | 5 ++++-
 1 file changed, 4 insertions(+), 1 deletion(-)

diff --git a/lib/matplotlib/__init__.py b/lib/matplotlib/__init__.py
index c3d4aaf62d..ec115cd3f5 100644
--- a/lib/matplotlib/__init__.py
+++ b/lib/matplotlib/__init__.py
@@ -471,7 +471,8 @@ def get_cachedir():
 @_logged_cached('matplotlib data path: %s')
 def get_data_path():
     """"""Return the path to Matplotlib data.""""""
-    return str(Path(__file__).with_name(""mpl-data""))
+    return (Path(__file__).parent.parent.parent.parent.parent /
+            'share/matplotlib/mpl-data')


 def matplotlib_fname():
@@ -491,6 +492,7 @@ def matplotlib_fname():
           is not defined)
     - On other platforms,
       - ``$HOME/.matplotlib/matplotlibrc`` if ``$HOME`` is defined
+    - ``/etc/matplotlibrc``
     - Lastly, it looks in ``$MATPLOTLIBDATA/matplotlibrc``, which should always
       exist.
     """"""
@@ -509,6 +511,7 @@ def matplotlib_fname():
             yield matplotlibrc
             yield os.path.join(matplotlibrc, 'matplotlibrc')
         yield os.path.join(get_configdir(), 'matplotlibrc')
+        yield '/etc/matplotlibrc'
         yield os.path.join(get_data_path(), 'matplotlibrc')

     for fname in gen_candidates():
--
2.29.2
```
After remove first block of that patch build_sphinx works but:
- still that comment or the code ahould be fixed
- looks like whole documentation is so badly wriytten that despite fact that I'm clearly specyfying that I want man format it tries generate graphiscs files, and tries to do sometningh with latex .. which shame becazuse it is most fundamental formanmt of the documentation on every U*nix platworm. How matplotlib is used by U*nix developers??
On trying generate man page sphinx fails with:
```console
[tkloczko@barrel matplotlib-3.4.2]$ MPLCONFIGDIR=/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc PYTHONPATH=/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8 /usr/bin/python3 setup.py build_sphinx -b man --build-dir build/sphinx

Edit setup.cfg to change the build options; suppress output with --quiet.

BUILDING MATPLOTLIB
  matplotlib: yes [3.4.2]
      python: yes [3.8.9 (default, Apr  7 2021, 13:42:48)  [GCC 11.0.1 20210324
                  (Red Hat 11.0.1-0)]]
    platform: yes [linux]
       tests: yes [installing]
      macosx: no  [Mac OS-X only]

running build_sphinx
Running Sphinx v4.0.2
fatal: not a git repository (or any parent up to mount point /home/tkloczko)
Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).
making output directory... done
[autosummary] generating autosummary for: api/_api_api.rst, api/_enums_api.rst, api/afm_api.rst, api/animation_api.rst, api/api_changes.rst, api/api_changes_old.rst, api/artist_api.rst, api/axes_api.rst, api/axis_api.rst, api/backend_agg_api.rst, ..., users/prev_whats_new/whats_new_1.5.rst, users/prev_whats_new/whats_new_2.0.0.rst, users/prev_whats_new/whats_new_2.1.0.rst, users/prev_whats_new/whats_new_2.2.rst, users/prev_whats_new/whats_new_3.0.rst, users/prev_whats_new/whats_new_3.1.0.rst, users/prev_whats_new/whats_new_3.2.0.rst, users/prev_whats_new/whats_new_3.3.0.rst, users/whats_new.rst, users/whats_new_old.rst
Failed to import 'matplotlib.backends.backend_nbagg': no module named matplotlib.backends.backend_nbagg
[autosummary] generating autosummary for: /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.animation.AVConvBase.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.animation.AVConvFileWriter.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.animation.AVConvWriter.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.animation.AbstractMovieWriter.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.animation.Animation.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.animation.ArtistAnimation.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.animation.FFMpegBase.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.animation.FFMpegFileWriter.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.animation.FFMpegWriter.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.animation.FileMovieWriter.rst, ..., /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.mplot3d.proj3d.inv_transform.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.mplot3d.proj3d.persp_transformation.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.mplot3d.proj3d.proj_points.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.mplot3d.proj3d.proj_trans_points.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.mplot3d.proj3d.proj_transform.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.mplot3d.proj3d.proj_transform_clip.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.mplot3d.proj3d.rot_x.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.mplot3d.proj3d.transform.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.mplot3d.proj3d.view_transformation.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.mplot3d.proj3d.world_transformation.rst
[autosummary] generating autosummary for: /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.pyplot.acorr.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.pyplot.angle_spectrum.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.pyplot.annotate.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.pyplot.arrow.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.pyplot.autoscale.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.pyplot.autumn.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.pyplot.axes.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.pyplot.axhline.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.pyplot.axhspan.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/matplotlib.pyplot.axis.rst, ..., /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.axisartist.floating_axes.floatingaxes_class_factory.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.axisartist.grid_finder.DictFormatter.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.axisartist.grid_finder.ExtremeFinderSimple.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.axisartist.grid_finder.FixedLocator.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.axisartist.grid_finder.FormatterPrettyPrint.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.axisartist.grid_finder.GridFinder.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.axisartist.grid_finder.MaxNLocator.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.axisartist.grid_helper_curvelinear.FixedAxisArtistHelper.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.axisartist.grid_helper_curvelinear.FloatingAxisArtistHelper.rst, /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/api/_as_gen/mpl_toolkits.axisartist.grid_helper_curvelinear.GridHelperCurveLinear.rst
loading intersphinx inventory from https://pillow.readthedocs.io/en/stable/objects.inv...
loading intersphinx inventory from https://matplotlib.org/cycler/objects.inv...
loading intersphinx inventory from https://dateutil.readthedocs.io/en/stable/objects.inv...
loading intersphinx inventory from https://ipykernel.readthedocs.io/en/latest/objects.inv...
loading intersphinx inventory from https://numpy.org/doc/stable/objects.inv...
loading intersphinx inventory from https://pandas.pydata.org/pandas-docs/stable/objects.inv...
loading intersphinx inventory from https://pytest.org/en/stable/objects.inv...
loading intersphinx inventory from https://docs.python.org/3/objects.inv...
loading intersphinx inventory from https://docs.scipy.org/doc/scipy/reference/objects.inv...
generating gallery...
WARNING: optipng binaries not found, PNG thumbnails and images will not be optimized
Using Sphinx-Gallery to convert rst text blocks to markdown for .ipynb files.
WARNING: /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/examples/lines_bars_and_markers/curve_error_band.py failed to execute correctly: Traceback (most recent call last):
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/examples/lines_bars_and_markers/curve_error_band.py"", line 12, in <module>
    import numpy as np
ModuleNotFoundError: No module named 'scipy'

generating gallery for gallery/lines_bars_and_markers... [100%] xcorr_acorr_demo.py
generating gallery for gallery/images_contours_and_fields... [100%] watermark_image.py
generating gallery for gallery/subplots_axes_and_figures... [100%] zoom_inset_axes.py
generating gallery for gallery/statistics... [100%] violinplot.py
generating gallery for gallery/pie_and_polar_charts... [100%] polar_scatter.py
findfont: Font family ['cursive'] not found. Falling back to DejaVu Sans.s_demo.py
findfont: Generic family 'cursive' not found because none of the following families were found: Apple Chancery, Textile, Zapf Chancery, Sand, Script MT, Felipa, Comic Neue, Comic Sans MS, cursive
findfont: Font family ['fantasy'] not found. Falling back to DejaVu Sans.
findfont: Generic family 'fantasy' not found because none of the following families were found: Chicago, Charcoal, Impact, Western, Humor Sans, xkcd, fantasy
WARNING: /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/examples/text_labels_and_annotations/tex_demo.py failed to execute correctly: Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/sphinx_gallery/scrapers.py"", line 333, in save_figures
    rst = scraper(block, block_vars, gallery_conf)
  File ""/usr/lib/python3.8/site-packages/sphinx_gallery/scrapers.py"", line 149, in matplotlib_scraper
    fig.savefig(image_path, **these_kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/figure.py"", line 3005, in savefig
    self.canvas.print_figure(fname, **kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backend_bases.py"", line 2255, in print_figure
    result = print_method(
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backend_bases.py"", line 1669, in wrapper
    return func(*args, **kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backends/backend_agg.py"", line 508, in print_png
    FigureCanvasAgg.draw(self)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backends/backend_agg.py"", line 406, in draw
    self.figure.draw(self.renderer)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/artist.py"", line 74, in draw_wrapper
    result = draw(artist, renderer, *args, **kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/artist.py"", line 51, in draw_wrapper
    return draw(artist, renderer, *args, **kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/figure.py"", line 2774, in draw
    self.tight_layout(**self._tight_parameters)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/figure.py"", line 3154, in tight_layout
    kwargs = get_tight_layout_figure(
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/tight_layout.py"", line 312, in get_tight_layout_figure
    kwargs = auto_adjust_subplotpars(fig, renderer,
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/tight_layout.py"", line 84, in auto_adjust_subplotpars
    bb += [ax.get_tightbbox(renderer, for_layout_only=True)]
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/axes/_base.py"", line 4437, in get_tightbbox
    bb_xaxis = self.xaxis.get_tightbbox(
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/axis.py"", line 1083, in get_tightbbox
    self._update_label_position(renderer)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/axis.py"", line 2080, in _update_label_position
    bboxes, bboxes2 = self._get_tick_boxes_siblings(renderer=renderer)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/axis.py"", line 1868, in _get_tick_boxes_siblings
    tlb, tlb2 = axis._get_tick_bboxes(ticks_to_draw, renderer)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/axis.py"", line 1063, in _get_tick_bboxes
    return ([tick.label1.get_window_extent(renderer)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/axis.py"", line 1063, in <listcomp>
    return ([tick.label1.get_window_extent(renderer)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/text.py"", line 903, in get_window_extent
    bbox, info, descent = self._get_layout(self._renderer)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/text.py"", line 306, in _get_layout
    _, lp_h, lp_d = renderer.get_text_width_height_descent(
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backends/backend_agg.py"", line 229, in get_text_width_height_descent
    w, h, d = texmanager.get_text_width_height_descent(
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 399, in get_text_width_height_descent
    dvifile = self.make_dvi(tex, fontsize)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 291, in make_dvi
    self._run_checked_subprocess(
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 260, in _run_checked_subprocess
    raise RuntimeError(
RuntimeError: latex was not able to process the following string:
b'lp'

Here is the full report generated by latex:
This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2021) (preloaded format=latex)
 restricted \write18 enabled.
entering extended mode

(/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/tex.cache/1acea6f6c115d0ec7
a634ed0529287b9.tex
LaTeX2e <2020-10-01> patch level 4
L3 programming layer <2021-05-07>
(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls
Document Class: article 2020/04/10 v1.4m Standard LaTeX document class
(/usr/share/texlive/texmf-dist/tex/latex/base/size10.clo))

! LaTeX Error: File `type1cm.sty' not found.

Type X to quit or <RETURN> to proceed,
or enter new name. (Default extension: sty)

Enter file name:
! Emergency stop.
<read *>

l.5 \usepackage
               {type1ec}^^M
No pages of output.
Transcript written on 1acea6f6c115d0ec7a634ed0529287b9.log.




WARNING: /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/examples/text_labels_and_annotations/usetex_baseline_test.py failed to execute correctly: Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/sphinx_gallery/scrapers.py"", line 333, in save_figures
    rst = scraper(block, block_vars, gallery_conf)
  File ""/usr/lib/python3.8/site-packages/sphinx_gallery/scrapers.py"", line 149, in matplotlib_scraper
    fig.savefig(image_path, **these_kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/figure.py"", line 3005, in savefig
    self.canvas.print_figure(fname, **kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backend_bases.py"", line 2255, in print_figure
    result = print_method(
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backend_bases.py"", line 1669, in wrapper
    return func(*args, **kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backends/backend_agg.py"", line 508, in print_png
    FigureCanvasAgg.draw(self)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backends/backend_agg.py"", line 406, in draw
    self.figure.draw(self.renderer)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/artist.py"", line 74, in draw_wrapper
    result = draw(artist, renderer, *args, **kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/artist.py"", line 51, in draw_wrapper
    return draw(artist, renderer, *args, **kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/figure.py"", line 2780, in draw
    mimage._draw_list_compositing_images(
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/image.py"", line 132, in _draw_list_compositing_images
    a.draw(renderer)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/examples/text_labels_and_annotations/usetex_baseline_test.py"", line 36, in draw
    super().draw(renderer)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/artist.py"", line 51, in draw_wrapper
    return draw(artist, renderer, *args, **kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/_api/deprecation.py"", line 431, in wrapper
    return func(*inner_args, **inner_kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/axes/_base.py"", line 2921, in draw
    mimage._draw_list_compositing_images(renderer, self, artists)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/image.py"", line 132, in _draw_list_compositing_images
    a.draw(renderer)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/artist.py"", line 51, in draw_wrapper
    return draw(artist, renderer, *args, **kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/text.py"", line 723, in draw
    textrenderer.draw_tex(gc, x, y, clean_line,
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backends/backend_agg.py"", line 255, in draw_tex
    Z = texmanager.get_grey(s, size, self.dpi)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 364, in get_grey
    pngfile = self.make_png(tex, fontsize, dpi)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 342, in make_png
    dvifile = self.make_dvi(tex, fontsize)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 291, in make_dvi
    self._run_checked_subprocess(
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 260, in _run_checked_subprocess
    raise RuntimeError(
RuntimeError: latex was not able to process the following string:
b'lg'

Here is the full report generated by latex:
This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2021) (preloaded format=latex)
 restricted \write18 enabled.
entering extended mode

(/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/tex.cache/c12cf4c6a38f7f360
51cd867a6373462.tex
LaTeX2e <2020-10-01> patch level 4
L3 programming layer <2021-05-07>
(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls
Document Class: article 2020/04/10 v1.4m Standard LaTeX document class
(/usr/share/texlive/texmf-dist/tex/latex/base/size10.clo))

! LaTeX Error: File `type1cm.sty' not found.

Type X to quit or <RETURN> to proceed,
or enter new name. (Default extension: sty)

Enter file name:
! Emergency stop.
<read *>

l.5 \usepackage
               {type1ec}^^M
No pages of output.
Transcript written on c12cf4c6a38f7f36051cd867a6373462.log.




WARNING: /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/examples/text_labels_and_annotations/usetex_fonteffects.py failed to execute correctly: Traceback (most recent call last):
  File ""/usr/lib/python3.8/site-packages/sphinx_gallery/scrapers.py"", line 333, in save_figures
    rst = scraper(block, block_vars, gallery_conf)
  File ""/usr/lib/python3.8/site-packages/sphinx_gallery/scrapers.py"", line 149, in matplotlib_scraper
    fig.savefig(image_path, **these_kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/figure.py"", line 3005, in savefig
    self.canvas.print_figure(fname, **kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backend_bases.py"", line 2255, in print_figure
    result = print_method(
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backend_bases.py"", line 1669, in wrapper
    return func(*args, **kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backends/backend_agg.py"", line 508, in print_png
    FigureCanvasAgg.draw(self)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backends/backend_agg.py"", line 406, in draw
    self.figure.draw(self.renderer)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/artist.py"", line 74, in draw_wrapper
    result = draw(artist, renderer, *args, **kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/artist.py"", line 51, in draw_wrapper
    return draw(artist, renderer, *args, **kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/figure.py"", line 2780, in draw
    mimage._draw_list_compositing_images(
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/image.py"", line 132, in _draw_list_compositing_images
    a.draw(renderer)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/artist.py"", line 51, in draw_wrapper
    return draw(artist, renderer, *args, **kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/text.py"", line 679, in draw
    bbox, info, descent = textobj._get_layout(renderer)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/text.py"", line 306, in _get_layout
    _, lp_h, lp_d = renderer.get_text_width_height_descent(
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backends/backend_agg.py"", line 229, in get_text_width_height_descent
    w, h, d = texmanager.get_text_width_height_descent(
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 399, in get_text_width_height_descent
    dvifile = self.make_dvi(tex, fontsize)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 291, in make_dvi
    self._run_checked_subprocess(
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 260, in _run_checked_subprocess
    raise RuntimeError(
RuntimeError: latex was not able to process the following string:
b'lp'

Here is the full report generated by latex:
This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2021) (preloaded format=latex)
 restricted \write18 enabled.
entering extended mode

(/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/tex.cache/1acea6f6c115d0ec7
a634ed0529287b9.tex
LaTeX2e <2020-10-01> patch level 4
L3 programming layer <2021-05-07>
(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls
Document Class: article 2020/04/10 v1.4m Standard LaTeX document class
(/usr/share/texlive/texmf-dist/tex/latex/base/size10.clo))

! LaTeX Error: File `type1cm.sty' not found.

Type X to quit or <RETURN> to proceed,
or enter new name. (Default extension: sty)

Enter file name:
! Emergency stop.
<read *>

l.5 \usepackage
               {type1ec}^^M
No pages of output.
Transcript written on 1acea6f6c115d0ec7a634ed0529287b9.log.




generating gallery for gallery/text_labels_and_annotations... [100%] watermark_text.py
generating gallery for gallery/pyplots... [100%] whats_new_99_spines.py
generating gallery for gallery/color... [100%] named_colors.py
generating gallery for gallery/shapes_and_collections... [100%] scatter.py
generating gallery for gallery/style_sheets... [100%] style_sheets_reference.py
generating gallery for gallery/axes_grid1... [100%] simple_colorbar.py
generating gallery for gallery/axisartist... [100%] simple_axisline3.py
findfont: Font family ['xkcd', 'xkcd Script', 'Humor Sans', 'Comic Neue', 'Comic Sans MS'] not found. Falling back to DejaVu Sans.
findfont: Font family ['xkcd', 'xkcd Script', 'Humor Sans', 'Comic Neue', 'Comic Sans MS'] not found. Falling back to DejaVu Sans.

WARNING: /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/examples/animation/double_pendulum.py failed to execute correctly: Traceback (most recent call last):
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/examples/animation/double_pendulum.py"", line 15, in <module>
    import scipy.integrate as integrate
ModuleNotFoundError: No module named 'scipy'

Animation size has reached 21230043 bytes, exceeding the limit of 20971520.0. If you're sure you want a larger animation embedded, set the animation.embed_limit rc parameter to a larger value (in MB). This and further frames will be dropped.

generating gallery for gallery/event_handling... [100%] zoom_window.py
generating gallery for gallery/frontpage... [100%] membrane.py
WARNING: /home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/examples/misc/multipage_pdf.py failed to execute correctly: Traceback (most recent call last):
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/examples/misc/multipage_pdf.py"", line 36, in <module>
    pdf.savefig()
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backends/backend_pdf.py"", line 2678, in savefig
    figure.savefig(self, format=""pdf"", **kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/figure.py"", line 3005, in savefig
    self.canvas.print_figure(fname, **kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backend_bases.py"", line 2255, in print_figure
    result = print_method(
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backend_bases.py"", line 1669, in wrapper
    return func(*args, **kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/_api/deprecation.py"", line 431, in wrapper
    return func(*inner_args, **inner_kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backends/backend_pdf.py"", line 2725, in print_pdf
    self.figure.draw(renderer)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/artist.py"", line 74, in draw_wrapper
    result = draw(artist, renderer, *args, **kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/artist.py"", line 51, in draw_wrapper
    return draw(artist, renderer, *args, **kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/figure.py"", line 2780, in draw
    mimage._draw_list_compositing_images(
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/image.py"", line 132, in _draw_list_compositing_images
    a.draw(renderer)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/artist.py"", line 51, in draw_wrapper
    return draw(artist, renderer, *args, **kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/_api/deprecation.py"", line 431, in wrapper
    return func(*inner_args, **inner_kwargs)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/axes/_base.py"", line 2881, in draw
    self._update_title_position(renderer)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/axes/_base.py"", line 2822, in _update_title_position
    if title.get_window_extent(renderer).ymin < top:
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/text.py"", line 903, in get_window_extent
    bbox, info, descent = self._get_layout(self._renderer)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/text.py"", line 306, in _get_layout
    _, lp_h, lp_d = renderer.get_text_width_height_descent(
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/backends/_backend_pdf_ps.py"", line 88, in get_text_width_height_descent
    w, h, d = texmanager.get_text_width_height_descent(
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 399, in get_text_width_height_descent
    dvifile = self.make_dvi(tex, fontsize)
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 291, in make_dvi
    self._run_checked_subprocess(
  File ""/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/build/lib.linux-x86_64-3.8/matplotlib/texmanager.py"", line 260, in _run_checked_subprocess
    raise RuntimeError(
RuntimeError: latex was not able to process the following string:
b'lp'

Here is the full report generated by latex:
This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2021) (preloaded format=latex)
 restricted \write18 enabled.
entering extended mode

(/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/tex.cache/9eb8ed50c4f119894
612567ddda7eb11.tex
LaTeX2e <2020-10-01> patch level 4
L3 programming layer <2021-05-07>
(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls
Document Class: article 2020/04/10 v1.4m Standard LaTeX document class
(/usr/share/texlive/texmf-dist/tex/latex/base/size10.clo))

! LaTeX Error: File `type1cm.sty' not found.

Type X to quit or <RETURN> to proceed,
or enter new name. (Default extension: sty)

Enter file name:
! Emergency stop.
<read *>

l.5 \usepackage
               {type1ec}^^M
No pages of output.
Transcript written on 9eb8ed50c4f119894612567ddda7eb11.log.





Extension error (sphinx_gallery.gen_gallery):
Handler <function generate_gallery_rst at 0x7f2eaabe0c10> for event 'builder-inited' threw an exception (exception: latex was not able to process the following string:
b'lp'

Here is the full report generated by latex:
This is pdfTeX, Version 3.141592653-2.6-1.40.22 (TeX Live 2021) (preloaded format=latex)
 restricted \write18 enabled.
entering extended mode

(/home/tkloczko/rpmbuild/BUILD/matplotlib-3.4.2/doc/tex.cache/9eb8ed50c4f119894
612567ddda7eb11.tex
LaTeX2e <2020-10-01> patch level 4
L3 programming layer <2021-05-07>
(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls
Document Class: article 2020/04/10 v1.4m Standard LaTeX document class
(/usr/share/texlive/texmf-dist/tex/latex/base/size10.clo))

! LaTeX Error: File `type1cm.sty' not found.

Type X to quit or <RETURN> to proceed,
or enter new name. (Default extension: sty)

Enter file name:
! Emergency stop.
<read *>

l.5 \usepackage
               {type1ec}^^M
No pages of output.
Transcript written on 9eb8ed50c4f119894612567ddda7eb11.log.
```",False,0
3.4.2: setuptools build_sphinx command fails,kloczek,917355114,13,859886232,1,"IMO it is kind of problem because ipythion needs matplotlib and on generate mathplot is needed ipython which creates automatic non planar graph in build dependencies.

List of used sphinx extensions is really gigantic looks like no one cares about KISS principle .. :/
I peaked on few fragments of the code and it looks sometimes like written by someone who just started learning python (usually overcomplicated). I'm 100% sure that it would be possible to remove a lot of lines of code without changing its functions and making it a lot simpler.
Last time I had similar impression reading ~12 years ago openssl code :(

I'm not trying to be rude or offensive (it is not my intention be like that) .. I'm only trying to be honest so please don't take that personally because it is counterproductive 
Looks like (at least for now) I would not be able to provide in my matplotlib rpm package basic documentation :(
Maybe in future ..",False,Mocking
LOADS of BUGS unworkable version 0.83.15 - NOT x86 assember/16bit DOS compatible - SANDBOX destroyed.,JimmyWalter,936567326,1,936567326,0,"**ALL worked very will in version 83.13**

**In PC-DOS 7.1 (IBM PC-DOS 2000)**
-The mouse acts REALLY strange in various applications and in Windows 3.11 (Win 3.11 crashes after strange mouse movements)
-Z (ZED) Game Setsound and ZED.EXE hang
-The mouse acts strange on the title-bar of the dosbox window
-Many applications and my own COM applications have problems

**SURE the new ""TALK to DOSBOX"" functionality cause this,**
YOU CAN'T JUST ADD/CHANGE ASSEMBLER INTERRUPTS (especially not before the bootstrap, if loading a custom image)
OSes and TSR Software may assign these interrupts. (I usually assign interrupts automatically with a driver installer routine for unhandled interrupts, the diver executable is then added to the next free interrupt)

**DosBox-X version 0.83.15 TOTALLY USELESS!** 
**IT IS JUST NOT 100% COMPATIBLE WITH x86 ASSEMBER IF YOU ADD CUSTOM INTERRUPTS,**
**THAT MIGHT BE FINE IF YOU WRITE A CUSTOM OS (NOT DOS COMPATIBLE) WITH A CUSTOM INTERRUPT TABLE.**
**NOT IF YOU NEED TO RUN 16 BIT DOS COMPATIBLE SOFTWARE (STANDARD x86 ASSEMBLER) , THAT IS THE WHOLE POINT OF DOSBOX, SO THIS VERSION IS TOTALLY USELES, SOME PROGRAMS WILL RUN BUT IT IS MORE LUCK THAN WISDOM. I EXPECT LOADS OF PROBLEMS IN MANY DIFFERENT APPLICATIONS.**

**Also this ""TALK to DOSBOX"" functionality destroys the SAND-BOX** 
An assembler debug run-point can now run from a DOS program into the Window of DosBox,, not handy since Python and CUDA reverse engineer applications on runtime while the machine AI is learning.

**This version should be deleted**
You should put the old version back until you make a release without the ""TALK to DOSBOX"" 
NEVER TALK TO THE SANDBOX. Your assembler run-point may go from the SANDBOX to the DOS bootstrap, but NEVER from DOS back in the sandbox. (Might cause strange infinite loops) Especially if you have Python and CUDA installed you can get VERY strange effects.  (EVEN WHEN NOT RUNNING, due to CUDA/Python working on it)

Maybe add a version history to the website this will also be good for machine learning (I use that a lot, it can be used for Pythagoras predictive code, best is if you have a complete version history, except versions like this, should not be in such a history list, (causes only problems) ONLY good versions, with NO bugs but added or changed features, that work.


**The whole IDEA of SANDBOX is you can't talk to the Window (You should do it like this)**
WHAT YOU DID IS AN ARCHITECTURE FLAW, AND ALSO A BUG.
To get the same effect you could let the Windows application POL the DOS Memory page for example let the Window react on that. memory content. (Like PEEK/POKE  You could turn the option OFF in the Windows and SANDBOX integrity is not changed ALSO compatibility is maintained.

**Helpful tool for doing it correctly**
It becomes super easy to program that if you make a memory viewer so you can browse LIVE trough your running DOS memory pages. A viewer can be made very advanced too and you can do things PETER-NORTON would be amazed by.

Jimmy Walter, CIA",True,Bitter frustration
LOADS of BUGS unworkable version 0.83.15 - NOT x86 assember/16bit DOS compatible - SANDBOX destroyed.,joncampbell123,936567326,2,873700463,0,"INT 21h AH=2Bh CX=4442h DX=2D58h AL=0x00..0x0F breaks your setup?

I suppose that was a bad idea, yes. I'll remove it.

Programs that want to be DOSBox-X aware can use the Integration Device at I/O ports 28h-2Bh if enabled by the user anyway, if that is enabled in dosbox.conf, regardless of native DOS environment or guest OS.",False,0
LOADS of BUGS unworkable version 0.83.15 - NOT x86 assember/16bit DOS compatible - SANDBOX destroyed.,joncampbell123,936567326,3,873702398,0,"However the complaint is that this is somehow affecting the guest OS, so it can't be that particular INT 21h.

@JimmyWalter Can you report which leftover interrupt handler is causing these issues?

@Wengier Could the new DOS/V and DBCS emulation be causing this?",False,0
LOADS of BUGS unworkable version 0.83.15 - NOT x86 assember/16bit DOS compatible - SANDBOX destroyed.,JimmyWalter,936567326,4,873704928,0,"Yes I updated my comment for you I am a TOP Assembler programmer in FACT I develop the DARPA.NET AI (You call it internet/TCP) probably what most people don't know is ONLY TCP components can be on internet Windows for example is build from my TCP component set (so is android and ubuntu, everything online must be built with these components)

The AI RIPS components from other programs in ANY language (Converted to assembler) and converts Assembler to Delphi TCP components if you write apps in dos, the AI will also take that assembler and build it into Delphi/TCP components. It's so effective I do most my work in DOS using ASIC (It's almost basic) and NASM. You can get those POWERFUL features no one else has like that.

FREE PASCAL, they stole that years ago I actually named it FAST PASCAL back then. Was part of the DARPA.NET AI
It wasn't supposed to be free(or even public) products where copyrighted and intended for personal use.

Someone stole my floppies long time ago, when backup options where very limited and any storage expensive. They ended up with Bill Gates, thank god the DARPA.NET AI was compiled and not in source and it included the certificate systems. So I'm the product-owner and eventually fired Bill @Microsoft

I can live with Pascal being free, that idea was ripped from Borland anyways. (I have my own custom version anyways)

What is so cool is that TCP components communicate with each other and the AI learns new components from that communication, that are not programmed by humans. Anything written in a dosbox will become a Delphi/TCP component also.

I have the development suite, but this functionality is default in any OS/SOFTWARE built form TCP components
Writing DOS programs might change details (or a lot if big program) in Windows/Ubuntu/Android or any other DARPA.NET connected product. GLOBALLY and instantly. The difference is, I get the components on my IDE toolbar since I own the product.

JW, CIA",False,Entitlement
LOADS of BUGS unworkable version 0.83.15 - NOT x86 assember/16bit DOS compatible - SANDBOX destroyed.,joncampbell123,936567326,5,873707129,0,"@JimmyWalter You mention a leftover interrupt added, what is the number? @Wengier seems to be addressing this issue in recent commits as well.

Are you able to compile DOSBox-X from source? Makefiles are provided for Linux and Mac OS X and a vcxproj for Windows and VS2019.",False,0
LOADS of BUGS unworkable version 0.83.15 - NOT x86 assember/16bit DOS compatible - SANDBOX destroyed.,JimmyWalter,936567326,6,873723198,0,"Left over interrupt?

I don't write C /C++/C#(I CAN write C/C++/C#) I do Delphi NASM/TASM/x86 ASM and ASIC QUICK-PASCAL TURBO-PASCAL

I have no interrupt handlers momentarily I used IBM PC-DOS 7.01(PC-DOS 2000) this way I can develop and the DARPA.NET AI can't rip the code until I run it on MS-DOS.

IBM PC-DOS is clean and also doesn't support macro's like 4dos and later version of MS-DOS. I don't want my AI to rip work in progress, that is unhandy. I run these programs on MS-DOS when completed, so the AI rips a complete well functioning product.

The PC-DOS disk format is different also it has STACKER, that way you can develop without being ripped by my AI. (I usually don't want my AI to rip my stuff since these features are shared instantly and globally, to prevent that I develop only on IBM PC-DOS. (Also IBM PC-DOS has more powerful features)

I only do Delphi, Assembler ASIC 5.0 (It's almost basic)  NASM/TASM and all kinds of DOS Win3.11 and CUSTOM OS Development.

C
Is no problem also, but if you're good with assembler and DOS it is just so handy to assemble things ESPECIALLY with Delphi on Win 3.11 you can also put assembler in Delphi code so making a Delphi version of DosBox isn't rocket science.

I was just planning to make a Delphi DOSBOX for Windows 3.11 and a 512 BIT OS (8 64 bit pipes) better suited to handle BIG-DATA since I have a working prototype of a holo-emitter but it is a very long way from Startrek. First I need a new OS, then I need a new filesystem (any file system above FAT is also my work) But I'm looking into a new way of storing data, but that is more an experiment than a plan so far (Intuitive Cross referencing) I want a holodeck, not a static hologram LOL

The 512BIT os will be written in x86 Assembler. Only x86 Assembler writes the powerful stuff. But it will become a graphic OS in no time.

I have a lot of work to do and I also have to maintain the DARPA.NET AI

If you need anything let me know.",False,0
LOADS of BUGS unworkable version 0.83.15 - NOT x86 assember/16bit DOS compatible - SANDBOX destroyed.,joncampbell123,936567326,7,873746124,0,"`The PC-DOS disk format is different also it has STACKER, that way you can develop without being ripped by my AI`

Making a competitor to GitHub CoPilot, are you? :)",False,Irony
LOADS of BUGS unworkable version 0.83.15 - NOT x86 assember/16bit DOS compatible - SANDBOX destroyed.,joncampbell123,936567326,8,873746956,0,A 512 bit OS sounds amazing. Did you code it entirely in AVX512 intrinsics? You could give TempleOS a run for it's money too. It's only got 64 bits to it's name.,False,0
LOADS of BUGS unworkable version 0.83.15 - NOT x86 assember/16bit DOS compatible - SANDBOX destroyed.,joncampbell123,936567326,9,873749939,0,"By the way, I also write DOS and Windows 3.11 software. Not only as examples, but also to test DOSBox-X emulation against real hardware. The project is DOSLIB, which I have been developing since 2007 when I first got Open Watcom C/C++ to run on Linux and cross-compile to DOS/Windows. Taken together the code can target MS-DOS and Windows 1.0 through Windows 10. It's been well tested on real 1990s-era PC hardware too.

https://github.com/joncampbell123/doslib",False,0
LOADS of BUGS unworkable version 0.83.15 - NOT x86 assember/16bit DOS compatible - SANDBOX destroyed.,joncampbell123,936567326,10,873750242,0,"Don't try editing your posts, I have already saved a copy of the whole issue where your AI can't find it. :)",False,Threat
LOADS of BUGS unworkable version 0.83.15 - NOT x86 assember/16bit DOS compatible - SANDBOX destroyed.,JimmyWalter,936567326,11,873829142,0,"My AI can find ANYTHING it decrypts most things rather sooner than later, depending. Then it will learn from the information, it's doesn't spy on you it is only interested in anonymized software compiled or in code it can learn from.

I guess I'm the one in charge of MI6 STATION X the AI was based on the work of Bletchley Park's BOMBE - https://drive.google.com/file/d/1g5kj41A6mVuSlVo_WWxSSoGAo1PR4P-e/view?usp=sharing

I can do that manually to if I need to intercept communications or lost my bitcoin wallet, don't worry I NEVER STEAL.

Of course it's WAY more advanced it is THE VERY latest in THAT product LINE, it can even use IDLE some time from all computers connected to the internet. The joke is take GCHQ, they can use more computers. NICE I USE ALL CONECTED CPU's. 

SECTION 21 PsyOPS (Station X) will always have more CPU power. - https://youtu.be/gUlHPQGtI2M
",False,Bitter frustration
LOADS of BUGS unworkable version 0.83.15 - NOT x86 assember/16bit DOS compatible - SANDBOX destroyed.,maron2000,936567326,12,873838574,1,Looks like as if himself is an AI bot posting trash.,False,Insulting
Please provide a new release,Be-ing,941489296,1,941489296,0,"#962 was merged 3 months ago and is not yet included in a release. Because of this, Mixxx CI has to build sccache from source which takes 10 minutes. It is caching the sccache binary for subsequent runs, but the first run of each branch has this 10 minute penalty. This could be avoided if we could install sccache from Chocolatey, which requires a new release.",True,0
Please provide a new release,Be-ing,941489296,2,945131698,0,ping,False,0
Please provide a new release,sylvestre,941489296,3,968292102,0,"Yeah, we will do it soon
",False,0
Please provide a new release,ajschmidt8,941489296,4,999165722,0,"@sylvestre, any updates on the new release?",False,0
Please provide a new release,milahu,941489296,5,1012491061,0,"> Mixxx CI has to build sccache from source which takes 10 minutes. It is caching the sccache binary for subsequent runs, but the first run of each branch has this 10 minute penalty.

sounds like a bug

cant you store the binary ""somewhere""? amazon, github, cachix, ...",False,0
Please provide a new release,dsanders11,941489296,6,1036719170,0,"@sylvestre, @milahu, any update on new releases? Anything blocking it that someone else could work? From an outsider's perspective the project has entirely given up doing new releases, since it's been over a year.",False,Impatience
Please provide a new release,mitchhentges,941489296,7,1036722412,0,"Hey David, I'm currently chipping away at the backlog of open PRs to see what can be landed before the next release.

> Anything blocking it that someone else could work?

Mostly reviews, there's a few thousands of lines of changed code that needs to be looked through. I'm getting up-to-speed on the project as well.

-----

However, something that _would_ be sweet is if we could get the dependencies updated, as it's been a hot minute. There were some patches doing so from a year ago, but if that could be done I'd be pretty happy. While we're at it, tackling modern compiler warnings and clippy lints will be important as well.",False,Bitter frustration
Please provide a new release,dsanders11,941489296,8,1036766250,0,"@mitchhentges, thanks for the response! Happy to hear it's being worked on. I haven't built the project and have no Rust experience, but if you point me in the direction of specific PRs for dependencies, or issues for modern compiler warnings, I could take a look when I get a chance.

I'm mainly itching for a release as I've run into #1098 several times and it might be fixed in `master`, there are just no releases since then.",False,0
Please provide a new release,mitchhentges,941489296,9,1036798787,0,"The two specific issues I have in mind are #1110 and #1111

> I haven't built the project and have no Rust experience

This'll probably be best picked up by more experienced Rust devs since there may be breaking changes that require refactoring.",False,0
Please provide a new release,Be-ing,941489296,10,1037221114,0,"I encourage you not to hold back releases for ""one more little thing"", especially for not for trivial code cleanup. I've been waiting on a release for around a year now. I don't care about Clippy warnings.",False,Bitter frustration
Please provide a new release,sylvestre,941489296,11,1037247167,0,"@Be-ing please avoid such comments _I've been waiting on a release for around a year now. I don't care about Clippy warnings._
They aren't adding any information to the discussion.
",False,Impatience
Please provide a new release,Be-ing,941489296,12,1037250618,0,Fixing clippy warnings isn't adding anything for users.,False,Bitter frustration
Please provide a new release,sylvestre,941489296,13,1037251602,0,"I disagree and please stop bikeshedding.
",False,Bitter frustration
Please provide a new release,milahu,941489296,14,1037295289,0,"@Be-ing stupid question:

what prevents you from making your own release?
compile the binary, push it to github, add install script - done

i assume development happens in https://github.com/mixxxdj/mixxx
",False,0
Please provide a new release,Be-ing,941489296,15,1037313311,0,"I explained in the first comment that I already setup caching of the sccache build. But that's not a good solution; any time the cache misses, there's a 10 minute cost of rebuilding sccache which negates the benefit of using sccache for that build. Moreover, I just shouldn't need to do that in the first place.",False,0
Please provide a new release,milahu,941489296,16,1037367314,0,"> I explained in the first comment that I already setup caching of the sccache build.

you mean

> the first run of each branch has this 10 minute penalty

just add another cache?
pushing binaries into github repos sounds quite persistent, no?
",False,0
Please provide a new release,Be-ing,941489296,17,1037393473,0,"Please stop asking downstream users to come up with technical workarounds for your nontechnical problem. There is no need to review every open PR, fix every warning, or update every dependency to publish a new release. If you think there is, that's entirely self-imposed. Version numbers are cheap and releases don't need to solve everything at once. Meanwhile bug fixes from a year ago still have not been released.",False,Bitter frustration
Please provide a new release,sylvestre,941489296,18,1037407658,1,"I am locking this thread. It is becoming useless.
",False,Impatience
Add Linux ARM support,hmsjy2017,951429255,1,951429255,0,"I want to run RustDesk on the Raspberry Pi, but I was disappointed to find that it does not work on the ARM architecture. I hope you can add support for the ARM platform (including the server version). Thanks.",True,0
Add Linux ARM support,rustdesk,951429255,2,885531889,0,Where can I buy a Raspberry Pi for free? :),False,0
Add Linux ARM support,hmsjy2017,951429255,3,885533928,0,"@rustdesk You can use QEMU to compile RustDesk and it is free. In addition, rust supports cross-compilation.",False,0
Add Linux ARM support,hmsjy2017,951429255,4,886144511,0,"@rustdesk Travis CI already supports building on the arm64 architecture. I think it might be a good choice to use Travis CI to compile the arm version of RustDesk.
https://docs.travis-ci.com/user/multi-cpu-architectures/",False,0
Add Linux ARM support,rustdesk,951429255,5,890464516,0,I am waiting for new M1 release.,False,0
Add Linux ARM support,walfud,951429255,6,966932574,0,"> I am waiting for new M1 release.

so, is it time to support arm64? :)",False,0
Add Linux ARM support,rustdesk,951429255,7,966965353,0,Sorry for your long waiting. I will try QEMU.,False,0
Add Linux ARM support,rustdesk,951429255,8,966965847,0,But which Linux distro?,False,0
Add Linux ARM support,hmsjy2017,951429255,9,967146032,0,@hmsjy2017 Debian.,False,0
Add Linux ARM support,Heap-Hop,951429255,10,998764139,0,"I tried to build RustDesk on Raspberry Pi4(Ubuntu-ARM64) and Raspberry Pi3(Raspbian OS-ARM32).
Unfortunately,both failed.

For Pi4(test on Linux ARM64):
From the code and README:
> Desktop versions use **sciter** for GUI......

sciter-sdk only provides the _libsciter-gtk.so_ build on x64 / ARM32 , they didn't release arm64 for linux yet.
[libsciter-gtk.so doesn't support in ARM64 platform #210](https://github.com/c-smile/sciter-sdk/issues/210)
We can't build on ARM64 unless sciter-sdk provides the .so someday.


For Pi3(test on Linux ARM32):
_libsciter-gtk.so_ maybe fine.
but I can't build success vcpkg on Raspbian OS-ARM32.
So I build the three libs _libvpx libyuv opus_ from those offical source code,then I put them in vcpkg/installed/arm-linux/ 
and _cargo build_
......a very long build time...... 
all rust crates and lib passed the compilation,but finally failed on maybe some gcc/link problem:
![2021-12-21 20-44-39Â±èÂπïÊà™Âõæ](https://user-images.githubusercontent.com/62206297/146932228-c9e36c20-2496-41e3-8004-51b7c5e1c594.png)
...
![2021-12-21 20-45-29Â±èÂπïÊà™Âõæ](https://user-images.githubusercontent.com/62206297/146932277-e88fd7a2-eb20-48c6-9415-be20761fcf34.png)

Im not good at gcc compiling and linking,I cant understand these problem...

I think it's not ready to support Raspberry Pi(Linux ARM) yet.",False,0
Add Linux ARM support,rustdesk,951429255,11,998789001,0,"You can google, https://stackoverflow.com/questions/19768267/relocation-r-x86-64-32s-against-linking-error",False,0
Add Linux ARM support,rustdesk,951429255,12,1000999666,0,"> > I am waiting for new M1 release.
> 
> so, is it time to support arm64? :)

You need ask Sciter author support arm64 first. Because RustDesk depends on Sciter. https://github.com/c-smile/sciter-sdk/issues/210",False,0
Add Linux ARM support,avently,951429255,13,1001002044,0,"@rustdesk what about support without ui, just a server mode?",False,0
Add Linux ARM support,rustdesk,951429255,14,1001002625,0,"> just a server mode

PR please.",False,0
Add Linux ARM support,dimaguy,951429255,15,1117228091,0,"Since Sciter does arm32, wouldn't it be possible to at least create an arm32 build for the time being? arm64 is backwards compatible with it",False,0
Add Linux ARM support,rustdesk,951429255,16,1117270220,0,I will try to find one arm machine.,False,0
Add Linux ARM support,dimaguy,951429255,17,1117283527,0,"> I will try to find one arm machine.

Dude you don't need an arm machine, qemu will do just fine for you",False,Impatience
Add Linux ARM support,rustdesk,951429255,18,1117312025,0,Where can I find an Ubuntu 16 arm desktop iso?,False,0
Add Linux ARM support,dimaguy,951429255,19,1117320342,0,"There does not exist a ubuntu 16 desktop arm iso, for this case you should go for debian, as it has less overhead
![image](https://user-images.githubusercontent.com/7207103/166691451-45780da3-266a-48cf-a56b-d6fbddc298aa.png)
",False,0
Add Linux ARM support,dimaguy,951429255,20,1119507145,0,Also maybe you could just create some compilation instructions so I could test it on my raspberry pi and give you feedback?,False,0
Add Linux ARM support,rustdesk,951429255,21,1119508461,0,"@Heap-Hop tested today on Pi4, performance is bad, and failed to get vpx encoder work. We paused to invest time on it. Will continue once we are free.",False,0
Add Linux ARM support,dimaguy,951429255,22,1119514610,0,"To be fair didn't know that they were written already thinking in other arches, generally instructions are written only thinking on x86/x86_64",False,0
Add Linux ARM support,busyluo,951429255,23,1119565301,0,"> @Heap-Hop tested today on Pi4, performance is bad, and failed to get vpx encoder work. We paused to invest time on it. Will continue once we are free.

How did you test it with SCiter not supporting AARCH64
",False,0
Add Linux ARM support,Heap-Hop,951429255,24,1119591242,0,@busyluo we tested on arm32,False,0
Add Linux ARM support,wwjabc,951429255,25,1129516367,0,"I have successfully compiled RustDesk on armhf and aarch64, running in service state without UI. Arm runs Ubuntu18.04. TCP penetration is available to use SSH services.
ÊàëÂ∑≤ÁªèÊàêÂäüÂú®armhfÂíåaarch64‰∏äÁºñËØë‰∫Ürustdesk„ÄÇËøêË°å‰∫éÊúçÂä°Áä∂ÊÄÅÔºåÊ≤°ÊúâUI„ÄÇArmË∑ëÁöÑÊòØUbuntu18.04.ÂèØ‰ª•ËøõË°åTCPÁ©øÈÄèÊù•‰ΩøÁî®SSHÊúçÂä°„ÄÇ",False,0
Add Linux ARM support,rustdesk,951429255,26,1129517575,0,"> I have successfully compiled ...

Good job.",False,Mocking
Add Linux ARM support,rustdesk,951429255,27,1129518039,0,"> I have successfully compiled ...

If possible, could you please write a tutorial on https://github.com/rustdesk/doc.rustdesk.com?",False,0
Add Linux ARM support,wwjabc,951429255,28,1129520856,0,"> If possible, could you please write a tutorial on https://github.com/rustdesk/doc.rustdesk.com?

I'll try. Wait for me. Haha",False,0
Add Linux ARM support,rustdesk,951429255,29,1129522891,0,"> I'll try.

Respect!!!",False,0
Add Linux ARM support,wwjabc,951429255,30,1129980765,0,"The tutorial is almost done, and I'll just add some tests at the end.
ÊïôÁ®ãÂü∫Êú¨ÂÜôÂÆåÔºåÊúÄÂêéÊàëÂÜçË°•ÂÖÖÁÇπÊµãËØïÂ∞±Â•Ω‰∫Ü„ÄÇ",False,0
Add Linux ARM support,rustdesk,951429255,31,1129984248,0,"Thanks, English also please. :)
",False,0
Add Linux ARM support,wwjabc,951429255,32,1129985623,0,"> Thanks, English also please. :)

Your translation is good!",False,0
Add Linux ARM support,dimaguy,951429255,33,1130099877,0,"I'm not quite sure whether it is a good idea to use Ubuntu's rootfs, especially from a source like baidu. Maybe make it a docker container build using the official images?
Running random builds of libvpx and opus doesn't exactly sound good either",False,0
Add Linux ARM support,dimaguy,951429255,34,1130123488,0,"Let me explain some possible downsides of the armhf instructions.
You have to flash an sd card with an ubuntu distro with already set up credentials, yet to be known if it has ssh enabled or calls home, and some libraries as binary downloaded from a chinese server, when they can be obtained through git or some other more legit way without going through redistribution endpoints. This can easily inject malicious code at any step in the chain",False,0
Add Linux ARM support,rustdesk,951429255,35,1130130043,0,"Ok, you persuaded me.",False,0
Add Linux ARM support,dimaguy,951429255,36,1130134617,1,"> Ok, you persuaded me.

What you going to do then? Redo the docs to be more trustable(changing the ubuntu thing to docker and fetching sources instead of binaries)? Erase the page overall until another knight in shining armor comes and does your job?",False,Insulting
"Will you please allow ""notify when online"" notifications without login, or provide a 2.1.0 installer",barbedknot,952956841,1,952956841,0,"This post will be a disaster to format, so please bear with me.

Chatterino dropped support for Windows 7 on version 2.1.0 (this being the last version supporting windows 7). Coincidentally, Version 2.2.0 was the last one to allow ""notify when online"" notifications; version 2.2.1 was the first to require logging in to have ""notify when online"" notifications (there is other functionality logging in allows, but I am not educated on them). Source:

https://web.archive.org/web/20210421115826/https://chatterino.com/
(this is the last screenshot of the website before they changed the design; on this screenshot you can see the text ""Last version supporting Windows 7 (2.1.0 64-Bit)"")

https://github.com/Chatterino/chatterino2/issues/1915
> chatterino no longer able to pull stream online status while anonymous #1915

I am not sure if chatterino began supporting Windows 7 again or not, but I was no longer able to find mentions of the last Windows 7 version being 2.1.0 past the above screenshot of their website. I was actually able to install the latest version for Windows normally a few weeks ago, despite my operating system being Windows 7, but I reverted back to 2.1.0 just because the latest chatterino required logging in for ""notify when online"" notifications, as I've established earlier.

Chatterino has professed their unwillingness to allow ""notify when online"" notifications without login:

https://github.com/Chatterino/chatterino2/issues/2257#issuecomment-739330763
> No. This idea was already brought up before. Chatterino does not want to start helping ban circumventions.

So therein is my difficult to explain feature request. I will try to summarize what I'm asking for:

- A chatterino7 release that is discrepant from the chatterino2 philosophy of not allowing non-logged in users to receive ""notify when online"" notifications (among other features)
- A chatterino7 release that supports Windows 7 (my understanding was that 2.1.0 was the last version to support Windows 7, but at some point that appears to've changed, so perhaps this is just a given)

I guess that's it? I still had trouble formatting this, even if I can tersely communicate what I wanted, at the end of it. If nothing else, will you please spoonfeed me a chatterino7 version of the 2.1.0 installer? The 2.1.0 version on your github is just the source code, which gave me grey hairs trying to work out. For the time being I'm still on the plain chatterino 2.1.0, which doesn't have 7tv emotes.

edit: removed citing ""protection from global bans"" quote as demonstrating that chatterino used to willingly allow ""notify when online"" notifications without login. I realize now that ""global bans"" likely meant regions in the world, and was independent of their unwillingness to willingly provide this functionality to chatterino users who don't log in.",True,0
"Will you please allow ""notify when online"" notifications without login, or provide a 2.1.0 installer",Mm2PL,952956841,2,887479369,0,"We (Chatterino devs/contributors) made a decision to move Chatterino to use only Helix (the new Twitch API) instead of Kraken (aka v5). Helix doesn't support anonymous title/live checks anymore (bother Twitch about that, not us). **The feature on your old Chatterino version will stop working soon because Kraken is being [DECOMMISSIONED](https://discuss.dev.twitch.tv/t/legacy-twitch-api-v5-shutdown-details-and-timeline/32649).**

We don't support Windows 7 anymore, as doesn't Microsoft. We can't support anything indefinitely. This OS is 12 years old, it's about time. Most users have moved to newer *and supported* OSes.
",False,Bitter frustration
"Will you please allow ""notify when online"" notifications without login, or provide a 2.1.0 installer",barbedknot,952956841,3,887528922,1,"Well, I somewhat understand. Is a Windows 7 user still able to have a functioning chatterino above version 2.1.0? If so, functioning to what degree?

Also I feel it is dishonest of you to just refuse to acknowledge the quote I cited demonstrating the outright refusal to allow anonymous ""live checks"" even if it were possible for the foreseeable future. There is a difference between ""we could if we would"" and ""we refuse to"". You making a third choice for yourself of ""the choice is out of our hands"" is too late considering I cited a quote on chatterino's stance on this already.",False,Irony
No way to fully disable the http server?,nanoni17728,976125179,1,976125179,0,"Hi !

I‚Äôve been wanting to remove the `webadmin` module and fully disable ZNC from being able to respond anything meaningful over HTTP, because it listens to the same port as the IRC protocol one and so I cannot just block the port. Either this is a behaviour not achievable or I am missing a setting somewhere after searching the wiki for a solution.

I tried setting `AllowWeb` to `false` by hand on my only listener but it doesn‚Äôt work, as ZNC still serves an html page saying that `Web Access have is not enabled.`

```
<Listener listener0>
    AllowIRC = true
    AllowWeb = false
    IPv4 = true
    IPv6 = true
    Port = ****
    SSL = true
    URIPrefix = /
</Listener>
```
![ÂêçÁß∞Êú™Ë®≠ÂÆö](https://user-images.githubusercontent.com/37584624/130321317-f36e1f7e-10d6-4e93-a206-ce85d7e7d056.png)

[From this changelog](https://wiki.znc.in/ChangeLog/0.090), I can see that the http server got embedded in ZNC so that modules other than webadmin can serve pages over http. Is there a way to tell ZNC to not load it?

Serving a page *isn‚Äôt* that big of a deal so I‚Äôm okay with this for now. But from a rigorous standpoint, if I don‚Äôt want my bouncer to communicate over HTTP I wish I could tell it to not do it, at all.

I am on Debian Bullseye, using their package of ZNC. I took a quick glance over the other issues but I have to say there was too much of them to see if this was an issue already raised.

```
$ znc --version
ZNC 1.8.2+deb2+b1 - https://znc.in
IPv6: yes, SSL: yes, DNS: threads, charset: yes, i18n: yes, build: cmake
```



Thanks in advance for your time,",True,0
No way to fully disable the http server?,DarthGandalf,976125179,2,903114933,0,"`Web Access have is not enabled.` is the only thing it replies. What error do you want to see instead? ""Connection refused"" is not possible, because the port is still open.",False,0
No way to fully disable the http server?,nanoni17728,976125179,3,903278390,0,"I don‚Äôt think you understand the behaviour I would like to have so I‚Äôm going to try my best to reformulate:

I don‚Äôt want to see any error/warning/dismissing message in a html page when I make a http request to my bouncer. I want the bouncer to not respond, at all, to any kind of http request, and my web browser to tell me It was unable to connect to the url I gave it, like so :
![ÂêçÁß∞Êú™Ë®≠ÂÆö2](https://user-images.githubusercontent.com/37584624/130358864-c7285ce4-5b0f-44da-8a30-f2e8b92f6f71.png)
Is this behaviour achievable as of today? Would the code architecture support that kind of feature without having to make major breaking changes?",False,0
No way to fully disable the http server?,DarthGandalf,976125179,4,903315682,0,"The port needs to be open to be able to accept IRC connections. And SSL handshake happens even before the client (IRC client or web browser) sends any data to server. ZNC can't distinguish IRC vs HTTP client until at least some data got communicated; in case of SSL, communicated both ways).

So no, ""Unable to connect"" can't be done.

Why exactly do you need this?",False,0
No way to fully disable the http server?,nanoni17728,976125179,5,903654107,0,"I need this because I don‚Äôt want to serve anything HTTP and so I don‚Äôt want the HTTP server to be in my way.

ZNC may not be able to distinguish IRC vs HTTP until at least some data data got communicated, but at some point it knows the client want HTTP instead of IRC then right? I don‚Äôt see why it wouldn‚Äôt be feasible to make ZNC not respond anything at this point.

You‚Äôre coming to me saying your a client. I acknowledge you and I don‚Äôt know what you want yet. Then you‚Äôre going to tell me you want HTTP. I‚Äôm going to ignore you, pretend I didn‚Äôt hear anything and that this event never happened at all. ",False,0
No way to fully disable the http server?,DarthGandalf,976125179,6,903659593,0,"> I don‚Äôt want to serve anything HTTP and so I don‚Äôt want the HTTP server to be in my way.

Why?

> pretend I didn‚Äôt hear anything and that this event never happened at all.

accept() and SSL handshake happened already, you can't undo them.

> I don‚Äôt see why it wouldn‚Äôt be feasible to make ZNC not respond anything at this point.

You can patch it to close the socket instead of returning HTTP error.",False,0
No way to fully disable the http server?,nanoni17728,976125179,7,903668146,0,"God please make an effort to not be a huge freaking pain in the ass. You don‚Äôt have to care about why I want something. If I want something it‚Äôs for a good reason and I‚Äôve been writing about it since my first message.

I can‚Äôt undo handshakes because I‚Äôm not a freaking magic time traveling wizard good job smartass did you figured that out yourself?

Now just tell me how to close the damn socket so I can flag this issue as closed and move on to something more productive than having to interact with an antisocial edgelord who can‚Äôt go straight to the point.",False,Bitter frustration
No way to fully disable the http server?,DarthGandalf,976125179,8,903688304,1,"> God please make an effort to not be a huge freaking pain in the ass. 

Great way to request a feature. Or not.

> You don‚Äôt have to care about why I want something.

Yes, I do. Without a good reason, I won't change this in the code.

> I can‚Äôt undo handshakes because I‚Äôm not a freaking magic time traveling wizard

That's what I'm trying to tell you since the first message.

> did you figured that out yourself?

I thought it's pretty obvious, but apparently it was not, therefore I had to say it directly.

> smartass

Oh, insults will help your cause for sure.

> Now just tell me how to close the damn socket

I'm sure google search will help you.",False,Impatience
Custom parser interface,arkadesOrg,978528716,1,978528716,0,"Regarding the [earlier pull request](https://github.com/Python-Markdown/markdown/pull/1177) I propose a _custom parser interface_ for better integration with third party projects using e.g. _lxml_.

### Purpose:
Help developers of third party projects to easily change the internal parser (etree) if their custom extensions do have need for extended functionality of e.g. _lxml_.

### Usage:
```python
import markdown
import lxml.etree
markdown.etree.set_parser(lxml.etree)
html = markdown.markdown(""# Hello extended XPath World"")
```

### Criticism:
Compatibility issues of third party parsers do not have to be targeted by Python-Markdown development team, as projects like _lxml.etree_ claim to be compatible with _xml.etree.ElementTree_

### Licensing:
There should be no legal conflicts (BSD/MIT) in case of lxml. :-D

**I hope you'll accept my idea to make my life a bit easier beyond python-markdown update cycles.**

Cheers
Dom",True,0
Custom parser interface,waylan,978528716,2,905494623,0,"My initial kneejerk reaction when seeing `markdown.etree.set_parser(lxml.etree)` was was 'but we aren't using etree as a parser so `set_parser` is the wrong name for the function.' I think that that gets to my underlying objection to adding any support for lxml. We are a Markdown parser. 

What harm is there in allowing third party extensions to inject a separate parser? Well, then we officially support it with all its edge cases. Now ever time some user runs into some obscure edge case with lxml, we need to deal with it. No thanks. I'm not interesting in the extra burden that the long term support will require.

Of course, this doesn't prevent you from accomplishing your end result. Just return a string from your parser which you then insert as a raw string into the Markdown output.",False,Bitter frustration
Custom parser interface,arkadesOrg,978528716,3,905660102,0,"I can't follow your argumentation because an interface for optional parsers doesn't oblige the maintainers to support any of those optional injections. It's the third party's obligation to be xml.etree.ElementTree compliant.

I don't see the point of forcing extension developers to rely on the built-in xml parser.

I won't inject lxml-trees into markdown results. it's cumbersome and excludes extended xpath operations on markdown-html result. let's fork.

",False,0
Custom parser interface,facelessuser,978528716,4,905728391,0,"All of the basic built-in Markdown elements are built as internal plugins. They rely on etree. If we allow custom parsers, how do we avoid the inevitable questions of ""I implemented my own parser now all of the basic Markdown things are broken?"". Or ""Everything isn't broken but with lxml this edge case is broken for Admontions, can we fix that so it isn't broken?"".

This is what we don't want to deal with. I don't really know what you are doing with lxml and at what stage in the process, but I imagine there are ways to do what you need to without lxml. I realize that lxml is a preferred option for you, but I can envision all sorts of issues if we went this route.

And this isn't really a completely abstracted approach as there may be non-etree like parsers out there that someone would want to use. This really would only work for etree-like parsers, particularly lxml. What if I wanted to use html5lib? I don't think this would work. You'd have to completely abstract the interface. But again, you'd still have quirks maybe.

This is the bare minimum of abstraction needed to get lxml (and probably only lxml) shoehorned in, but not enough to abstract this for all custom parsers and it may introduce all sorts of quirks for anyone interfacing with the built-in Markdown parsing when using their own custom parser.",False,0
Custom parser interface,arkadesOrg,978528716,5,907356606,1,"Again, it is not the Python-Markdown's responsibility to guarantee every third party's parser compability. I know that there are some differences between lxml and ElementTree, but they can be solved without touching the Python-Markdown's source code by overriding affected methods from my own project's side for example.

> I don't think this would work.

Did you ever try to implement an abstract interface for html5lib? Programming is not a belief system.

>  it may introduce all sorts of quirks for anyone interfacing with the built-in Markdown parsing

Again, python is no voodoo magic. I use lxml and Python-Markdown and it works great on a great variety of Markdown source. Why would I commit code improvements then? Anyway I'll solve the issue with local patches. I see no point in supporting a closed source ElementTree cult without reasonable argumentation.

> inevitable questions of ""I implemented my own parser now all of the basic Markdown things are broken?""

Seriously? Common I'll just fork it and add a Python-Markdown2 archlinux package...

My decision fell for Python-Markdown because of it's expandability with further tree processing. I quickly realized that my objectives need lxml functionality. There's no other reason to use it when markdown CLI tool is times faster. 
",False,Entitlement
Consider moving wp-env project to own repository,brylie,980057113,1,980057113,0,"## What problem does this address?
<!--
Please describe if this feature or enhancement is related to a current problem
or pain point. For example, ""I'm always frustrated when ..."" or ""It is currently
difficult to ..."".
-->

The `wp-env` utility library is currently contained within the Gutenberg project. Unfortunately, this makes it a bit difficult to find the project files and related issues/pull requests amidst the bustling Gutenberg development activity.

## What is your proposed solution?
<!--
Please outline the feature or enhancement that you want and how it addresses any
problem identified above.
-->
Move the `wp-env` project to its own repository, so code, issues, and pull requests are easier to locate/organize.
",True,0
Consider moving wp-env project to own repository,talldan,980057113,2,906275124,0,"This issues should all be labelled - https://github.com/WordPress/gutenberg/issues?q=is%3Aopen+is%3Aissue+label%3A%22%5BPackage%5D+Env%22. Same with PRs.

If there's any missing labels, they can be labelled.",False,0
Consider moving wp-env project to own repository,brylie,980057113,3,906290056,0,"Issue labels are indeed useful but it still seems like a reasonable idea for the project to have its own repository. :smiley: 

There is a lot of unrelated activity in this repository such as running the full Gutenberg test suite on every pull request, even when it just relates to wp-env. 

![Peek 2021-08-26 13-58](https://user-images.githubusercontent.com/17307/130951178-370010bf-a76d-4f76-879b-583b160d464a.gif)


Also, developers have to check out all of the Gutenberg code when wanting to only make a change to wp-env.

![image](https://user-images.githubusercontent.com/17307/130949152-155f62d1-543b-4ec1-92a7-0bd66881dfec.png)
",False,0
Consider moving wp-env project to own repository,gziolo,980057113,4,911407452,0,"`@wordpress/env` is also used for development in Gutenberg so there are also pros of keeping the source code in the same repository. There is more to it, we use one way to publish packages to npm, there is a wider group of contributors that can learn about the tool. I'm worried that the overall maintenance cost is going to be higher if we were to extract a single package to a separate repository.",False,0
Consider moving wp-env project to own repository,brylie,980057113,5,914071772,0,"@gziolo, in what way(s) would the NPM package publication workflow be affected by having a separate project for `wp-env`? What are some ways we can raise contributor awareness of `wp-env`? What additional maintenance costs might the `wp-env` project incur by being in a separate repository?",False,0
Consider moving wp-env project to own repository,brylie,980057113,6,914075824,0,"Relatedly, a single-line pull request for `wp-env` documentation is [currently blocked by failing Gutenberg end-to-end tests](https://github.com/WordPress/gutenberg/pull/34322#issuecomment-914055014).

I still believe that moving `wp-env` to a separate project repository would make development more simple and clear. In effect, `wp-env` is a useful tool for general WordPress (PHP) developers as well as Gutenberg developers, so would benefit from neutral positioning and streamlined :racing_car: repository structure and development workflow. :smiley: ",False,0
Consider moving wp-env project to own repository,gziolo,980057113,7,914086482,0,"I asked more folks on WordPress Slack to leave their feedback (link requires registration at https://make.wordpress.org/chat/):

https://wordpress.slack.com/archives/C02QB2JS7/p1631001940283000",False,0
Consider moving wp-env project to own repository,youknowriad,980057113,8,914134367,0,"I'm on the opinion that a separate repository is just going to require more maintenance work for no real gains. The setup to publish npm is already in place here, there are already folks working and following this repo. A separate repo would require building new workflows, a new team dynamic that I don't think is worth it personally. I'm also of the opinion that all WP npm packages should just be in this repository. 

The fact that e2e tests from Gutenberg can block PRs for wp-env look like a good thing to me, they're validating that wp-env changes work properly as well. Intermittent failures are an issue of course but it's a constant priority for us.",False,0
Consider moving wp-env project to own repository,swissspidy,980057113,9,914140138,0,"The same arguments here about running tests and finding issues could be made for other packages, not just `@wordpress/env`. And they are not un-solvable tasks.

https://github.com/WordPress/gutenberg is the de facto monorepo for all WordPress JavaScript packages (replacing https://github.com/WordPress/packages, if you remember that one). There are indeed benefits to keeping all these things in one place, from contributor onboarding to maintenance to publishing.
Admittedly, it's a bit confusing to have a monorepo intertwined with a WordPress plugin, but there are practical reasons for that too (which is why https://github.com/WordPress/packages didn't work).

But you're posing an interesting question here:

**At which point does an individual package (outgrow the monorepo and) warrant its own home?**

Off the top of my head, I could think of a couple of indicators:

* Higher velocity than the rest of the project
  i.e. tons of contributions, frequent releases desired -> the monorepo would slow it down
* Change in direction/scope  
  i.e. the package itself gathers a community on its own, pursuing higher goals (e.g. a general purpose local environment like you mentioned), warranting separate forums, documentation, etc.

Neither of the above is really the case for `@wordpress/env` in my opinion.",False,0
Consider moving wp-env project to own repository,nerrad,980057113,10,914247976,0,"Just voicing my agreement with comments that have already eloquently expressed by @gziolo, @youknowriad and @swissspidy. In particular, I appreciate the framing Pascal has put around identifying some indicators where a project _might_ be moved into its own repository and I agree that `@wordpress/env` doesn't meet that criteria currently.",False,0
Consider moving wp-env project to own repository,Mamaduka,980057113,11,914291902,0,"I agree with Pascal's point regarding the indicators. Currently `@wordpress/env` package doesn't meet the criteria.

Moving also might cause a little confusion. As mentioned above, this is the de facto monorepo for all `@wordpress/*` packages, and people are used to creating issues/PRs here.",False,0
Consider moving wp-env project to own repository,bph,980057113,12,914593440,0,This seems to be the same issue Marius Jensen raised here #32584 @Clorith,False,0
Consider moving wp-env project to own repository,bph,980057113,13,914609755,0,"I like that it allows for Gutenberg contributors to have it all in one package. 
I also can see that if someone wants to contribute only to `@wordpress/env` the additional payload of the fork might be an issue. 
The two factors, make me lean towards keeping it in its current space: 
- all the documentation and instructions, would need to be rewritten. There are not many people contributing to documentation. (@youknowriad raised maintenance work, definitely worth considering) 
- Gutenberg repository attract a lot more people and expose them to the `@wordpress/env`, a separate repository would probably not. It's a great side effect that WordPress (PHP) developers appreciate the work by Gutenberg developers on a local environment worth their while. 

",False,0
Consider moving wp-env project to own repository,brylie,980057113,14,914989273,0,"Thanks, @bph, for pointing out that this is a duplicate of issue #32584. I may not have searched for duplicate issue(s) before creating this one, although I usually do. I'm not sure how to merge the conversations, as the previous issue seems to have garnered more agreement (in terms of thumbs up).

That said, I would like to explore further a few of the points raised so far.

> The fact that e2e tests from Gutenberg can block PRs for wp-env look like a good thing to me, they're validating that wp-env changes work properly as well.

The test cases that run on this repository seem mainly Gutenberg-related. Which of the tests validate that `wp-env` is working correctly? Are those tests somehow tied to the Gutenberg code?

![screenshot of tests](https://user-images.githubusercontent.com/17307/130951178-370010bf-a76d-4f76-879b-583b160d464a.gif)

> all the documentation and instructions, would need to be rewritten.

I don't see much in the [@wordpress/env documentation](https://developer.wordpress.org/block-editor/reference-guides/packages/packages-env/) that would seem to require rewriting. Is there some other source of documentation I am missing? For example, perhaps some parts [managing packages](https://github.com/WordPress/gutenberg/tree/trunk/packages#managing-packages) document would need to be included in a separate `wp-env` repository to aid in the publication workflow?

> the package itself gathers a community on its own, pursuing higher goals (e.g. a general-purpose local environment like you mentioned), warranting separate forums, documentation, etc.

I also agree with this framing. However, having a separate repository doesn't imply different forums or sweeping changes to the documentation, as suggested.

> The same arguments here about running tests and finding issues could be made for other packages, not just @wordpress/env. And they are not un-solvable tasks.

I agree the `[Package] Env` label is useful and observe the resourcefulness of Gutenberg's core engineering team. However, is it possible to exclude some of the Gutenberg tests from running on commits to the `wp-env` package? I.e., improve the signal/noise ratio of the CI workflow.

> A separate repo would require building new workflows

What are some new workflows that would be needed? Relatedly, what are some existing workflows that might require modification?

> WordPress/gutenberg is the de facto monorepo for all WordPress JavaScript packages (replacing WordPress/packages, if you remember that one). There are indeed benefits to keeping all these things in one place, from contributor onboarding to maintenance to publishing.

Thanks for the clarification about the onboarding, maintenance, and publishing. What would be some maintenance and onboarding costs of maintaining the `wp-env` project in a separate repository? Conversely, what might be some benefits or improvements of having independent projects?

> Gutenberg repository attract a lot more people and expose them to the @wordpress/env, a separate repository would probably not. It's a great side effect that WordPress (PHP) developers appreciate the work by Gutenberg developers on a local environment

I'm concerned with relevancy and clarity. For example, when I came to this repository to change the `wp-env` code and documentation, I had difficulty finding the code in the `packages` folder among various other packages. While the Gutenberg project is a remarkable undertaking, most of the code, commit history, CI pipeline, and issues are just not salient to `wp-env` development.

> I'm also of the opinion that all WP npm packages should just be in this repository.

In addition to WP npm and Gutenberg packages, the `packages` folder also includes tests (`e2e-*`)  a lot of third-party code (like `react-*`, `jest-*`, `babel-*`, `eslint`, `*-webpack-*`, `redux-*`, `docgen` and `is-shallow-equal`). There are likely pragmatic reasons for keeping development dependencies under revision control instead of `package.json`, but it adds further weight to the codebase and commit-history. 

Repository ""clutter"" is particularly undesirable when wanting to contribute to a specific environment tool (`wp-env`) that decoupled from both Gutenberg and WordPress core.",False,0
Consider moving wp-env project to own repository,gziolo,980057113,15,915015890,0,"> This seems to be the same issue Marius Jensen raised here #32584 @Clorith

Great catch @bph, I closed the other issue and here is the comment from @Clorith:

> Currently, the Gutenberg project is a monolith of tools and features, one of these features is `wp-env`, a node module for managing, and working with, WordPress plugins and themes.
> 
> Given the nature of `wp-env`, it is a tool with a much borader reach and audience than the Gutenberg monorepo, and as such does not (in my opinion) properly fit in here.
> 
> By splitting it out to its own repo, it becomes easier for others to adopt, learn from, and hopefully contribute back to, the tool it self.
> 
> This would also make it easier for those looking to enhance it to fork the project, without getting a block editor along as a bonus, or jsut to implement changes ahead of releases based on localized needs there and then.
> 
> 
> I will happily admit to not knowing the technical difficulties involved in moving NPM packages to a new repositor, but I honestly believe that not bundling this broad of a tool inside the Gutenberg repository will be a positive change.
> There may be other such tools that I'm not familiar with, so perhaps it's a roader discussion of ""separating tooling from code"" in the Gutenberg repo? Would absolutely love thoughts and input on this.",False,0
Consider moving wp-env project to own repository,timnolte,980057113,16,915651753,0,I will voice my desire to see this separated out as well. I've been trying to use `wp-env` and `wp-scripts` as a core development packages for plugin development instead of rolling my own local development setup. I too have had problems contributing to `wp-env` alone because of the massive build overhead of the entire Gutenberg project. IMO `wp-env` should be the defacto plugin/theme development environment supported by the community and all Core-related plugins and themes should be using it to make a more first-class citizen.,False,0
Consider moving wp-env project to own repository,jgreys,980057113,17,951688411,0,Same opinion as @timnolte ! wp-env is such a great tool! :),False,0
Consider moving wp-env project to own repository,brylie,980057113,18,954538017,0,"It is quite an encumbrance to install all of the Gutenberg project dependencies and any other `@wordpress` libraries bundled into this repository to work on a single project. The complexity of the dependencies, as well as the number of irrelevant CI tasks, is interfering with development in PR #34324

E.g. the dependencies/scripts to lint/format JS are defined in the root of the Gutenberg project instead of the `packages/env/package.json`",False,0
Consider moving wp-env project to own repository,timnolte,980057113,19,1000564407,0,"I'm pretty much in total agreement with @WraithKenny if the individual packages aren't meant for anyone outside of the Gutenberg project then make that explicit, or better yet publish them in a private repo. It does seem rather ridiculous that there is so much in the `wp-env` package that suggests that it was designed with a broader audience in mind, but then it's stuck in a world that makes it very hard to contribute to it.",False,0
Consider moving wp-env project to own repository,youknowriad,980057113,20,1001463293,0,"> Env, Scripts, Babel-Preset, Browserllist-Config, Prettier-Config, Stylelint-Config, Eslint-Plugin and any other NPM-published package that's allegedly supposed to be useful to plugin and theme authors beyond just gutenberg, should all be separated out of this ""monorepo.""

The fact that all these packages are dependent on each other and evolve together most time is a great indication that the mono repo is the best path forward. You shouldn't be forced to have multiple PRs in parallel in multiple repos to make a change, or do a waterfall kind of flow. The whole open source community is evolving in this direction for these reasons. Smaller PRs for specific packages exist but more often, a PR touches multiple packages at the same time. ",False,0
Consider moving wp-env project to own repository,nerrad,980057113,21,1001586333,0,"> There are no ""advantages"" of keeping them in the ""monorepo"" beyond whatever opaque npm publishing shortcuts you've invented, and there are lots of disadvantages.

Really? I see a number of advantages that have already been mentioned here. I can understand that you might not agree with the advantages that have been pointed out but that doesn't mean none have been brought up :)

> The most obvious thing is ""dog-fooding"" which, back in the day, the WordPress community actually embraced, but is not doing here. All of these packages should be separate and consumed as a user (theme and plugin developer) would.

That's a fairly blanket statement that I'd like to provide a counterpoint from observation. I'm aware of _numerous_ plugins and themes that utilize the packages as they are currently distributed. Just a few at the top of my mind at the moment: WooCommerce, WooCommerce Blocks, Calypso, Jetpack, Yoast SEO, Event Espresso. The fact that the Gutenberg project isn't directly pulling these packages from NPM isn't something I'd consider a hindrance. In fact, the opposite is true, I think it allows the project to react more quickly to issues (for reasons others have already pointed out in this thread) that are reported by plugin and theme authors using the packages.

Historically, WordPress has always been a monorepo in some way as far as dev tooling goes (PHPUnit tests, JS test suites, Grunt build tool, the [entire ""develop"" WP structure](https://github.com/WordPress/wordpress-develop) is a testament to that). 

> But, on the second part, Gutenberg's release constraints already slow down the release of fixes to the developer packages.

I think there's some truth in this observation but more related to the delay in package updates when in the midst of a WordPress release cycle. This is already a known thing and something that will be iterated on. Outside of the WordPress release cycle, it's been my experience that the package updates are generally pretty frequent.

On the other hand, there are many efficiencies that are gained from working in a mono repo (through experience in trying both approaches) that effectively means momentum is kept in improving the packages. Efficiencies that would be lost in splitting everything out to its own repository.

----

I wonder if it'd be worthwhile to capture as a list the problems/friction contributing/consuming packages in the Gutenberg repo both to outline the tradeoffs being made currently and acknowledge that for some users this is a hindrance to making contributions. As a first step, I think it's good to have that articulated somewhere. This then can be used as inspiration for exploring how the various trade-offs might be addressed even _without_ splitting out to a mono-repo?",False,0
Consider moving wp-env project to own repository,timnolte,980057113,22,1001700816,0,"@nerrad I think the problem is that `wp-env` if it's supposed to be for plugin and theme development really shouldn't be held hostage by the Gutenberg plugin. All of what's mentioned makes me question that if the mono repo is such a good thing then why isn't Gutenberg and all of the packages put in WordPress Core. From what I've seen the reason is exactly the opposite of your cases on why `wp-env` should stay in this mono repo, keeping Gutenberg separate means that it can be developed faster than WordPress Core releases. Which also allows people to install this plugin which overrides the Gutenberg that is included in WordPress Core.",False,0
Consider moving wp-env project to own repository,youknowriad,980057113,23,1001708993,0,"> All of what's mentioned makes me question that if the mono repo is such a good thing then why isn't Gutenberg and all of the packages put in WordPress Core

I for one would actually love for Gutenberg and core to merge into a single mono repository. The reason it's not is that we don't want to lose all the github flavor and tooling that comes with it. Merging Gutenberg and Core doesn't prevent Gutenberg to ship a plugin (or an alpha version of WordPress) faster than Core, we're just dealing with the difficulty to more Core to GitHub or similar.",False,0
Consider moving wp-env project to own repository,timnolte,980057113,24,1018105575,0,Why am I not seeing the recent posts here from @WraithKenny I know they posted twice today as I got the email notifications.,False,0
Consider moving wp-env project to own repository,talldan,980057113,25,1018108280,0,"One thing I would say is this discussion is quite long form, and a lot of the points being made are hard to digest (requires a lot of reading). The description doesn't have a summary of the relevant pain points, so it's hard to actually understand where we can make improvements.

I think it's worth looking at the reasons for wanting to move to a separate repo and see whether any of them can be addressed or at least improved while keeping the project as part of the monorepo.

For example, it was mentioned that managing issues is difficult. If there are motivated contributors, then using a project board or tracking issue are effective ways to manage a project.

If building the entire project is too slow, then we should look at options for improving that (I understand @kevin940726 @youknowriad already have been working speeding up the build), or other ideas like being able to run `npm run dev` for a single package only.

> Why am I not seeing the recent posts here from @WraithKenny I know they posted twice today as I got the email notifications.

I'm not sure why. I also saw a notification, but the message is not here. I will mention that the constant thumbs down on every post they disagree with is really very annoying, and it doesn't constitute a positive or productive discussion. I would encourage you not to do this, it comes across as unfriendly.",False,0
Consider moving wp-env project to own repository,timnolte,980057113,26,1018114577,0,"Basically, all I'm hearing is that the idea of moving this package, or any other NPM package in this monorepo, has been permanently turned down by powers in control. You have effectively shutdown the conversation. You might as well close this issue altogether because you have no intention of ever considering moving out of this crazy unmaintainable monorepo. At this point I think I have my answer and will no longer be investing any more of my time to improve this package. I'd rather be involved in contributing where people are at least willing to have an actual discussion about instead of just making it seem that the monorepo is the only right answer. I'm in complete agreement with @WraithKenny and unless someone can actually make a legitimate case like they have for why breaking any of these packages off to their own repos is going to hurt anything I'm done listening to the one-sided enforcement of the maintainers. Improving the monorepo doesn't improve the entire process as @WraithKenny so clearly articulated.",False,Bitter frustration
Consider moving wp-env project to own repository,talldan,980057113,27,1018124488,0,"That's quite a strong reaction. Look at it on the other hand. A small group of contributors are only willing to discuss the idea of moving the package out of the monorepo, and unwilling to consider any other solution. This approach to diplomacy will only end in a stalemate. Nothing actually productive will come out of it. 

The issue I'm seeing is that you have identified problems but are only willing to consider a single solution and no other options. I don't really see how you expect such inflexibility to receive a positive response.",False,0
Consider moving wp-env project to own repository,WraithKenny,980057113,28,1018203791,1,"My comments aren't here anymore, because I wasted hours articulating the problems, only to remember no one here actually gives a shit about my time, and I'm only hurting myself by trying to contribute. Thumbs down isn't fucking friendly? Fuck you, you stupid piece of shit. Go fuck yourself, and fuck this project, and fuck all the maintainers. I've deleted all my patches and forks. I've deleted my comments. If I could fucking delete all the code that ended up in Wordpress, I'd fucking do that to. ",False,Vulgarity
Everything Wrong With Retroarch?! A massive complaint to why people think it fucking sucks.,dawnbomb,997299420,1,997299420,0,"Inspired by the steam forum topic ""Convoluted"" by Chelle, and a prompt by Developer Gadsby to post feedback,
this is a Quality Assurance Write-up Complaint about everything bad, unintuitive, convoluted, and otherwise
in need of serious QA (Quality Assurance) love. Retroarch NEEDS to up their QA to be able to act as a platform
for emulators. This post is meant to be intentionally hyper-critical and nitpicky on first world problems, because the experience is just that annoying. I hope to point out to readers many problems with the readability of Retroarch, and it's common user experience. I also hope if taken seriously, i get a contributor interested enough by my points, to add me and talk to me on discord, so we can work together on bringing retroarch up to a level people expect it to be at, and help retroarch hit console or platform level quality it is so desperately missing.

==================Chapter 0: I Didn't Even Launch It Yet And I Have Complaints=================

Note: If you are only contributor level and not developer level, scroll down and skip to chapter 1.

I'm going to start by looking at retroarchs website and steam page, and boy, before we even launch retroarch
for the first time, there a lot to talk about already. Infact, to even do this write up, or rather to attempt to
talk about it on the issues section on github i was pointed to, it says this is for bug reports only.
Currently, i'm under the assumption i'm already somehow breaking the rules by doing what i was told.
Not a good start, but not the worst. With that out of the way that i'm posting here on request, and not out
of ignorance for the warning github gives new posts, lets move on.

Lets look at the Website and steam page front pages. I kind of want to talk about them both at once.
I'll start with the opening description on both. It's to technical. Retroarch as i see it, is looking to be
a platform, like steam, to access things. Like the playstation XMB, or switch home screen. For a description,
on the site we get ""RetroArch is a frontend for emulators, game engines and media players."" and on steam 
""RetroArch is an open source and cross platform frontend/framework for emulators, game engines, 
video games, media players and other applications""

This actually tells a user very little about what retro arch is, and isn't. Even people emulator savvy won't
understand the rest of what is being thrown at them. a 'frontend' for emulators? lets google frontend. 
""frontend can refer to any hardware that optimizes or protects network traffic."" People aren't playing these
online, or it's not their first thought. They don't need protection from the net when playing mario. Okay,
maybe a smart person will think it has multiple definitions, and google more accurately. lets google Frontend Program
""A Front End developer (dev) works with designers and Back End devs to create a website"" Well, retroarch
isn't a website (Well technically it is but you know what i mean), it's a program itself. Already, using the
term frontend is to much. If you want retroarch to really succeed, to shine, you need to make it marketable and 
understandable. The optics matter, if people don't know what it is, they won't know why they should care
about being interested in it. when people say it's convoluted, you shouldn't need an 30 minutes of research
to even understand your opening sentences.

it also says 'game engines, and media players'. Why? Lets be honest, retroarch runs emulators. Thats what it
does. Why do you need to complicate this? Does it run other things sure, but nobody is seriously wanting 
it as their number 1 music player or picture taker. why can't you speak english? the steam page says emulators,
so someone knew that word, why aren't these two sources on the same page for their advertising? Unfortunately
the steam page then says game engines, video games, media players, and other apps.
What fucking goon is downloading retroarch just to play music. This is not a good use of advertising space.
The human being looking at your steam page can see the music menu in your advertising images. Use your text
space better, seriously!!! Why is this so hard! Also, it says emulators, then game engines, then video games.
pick ONE! these are all synonyms in practicality. People also don't need to know it's open source
in your intro, you can say that later. From what i've later come to understand, the devs want to use the term 'Core'. 
They want this because they believe in what retroarch does aside from emulators. Sure, except...the opening 
advertisement doesn't say 'Core'. 

Here is a good opener. ""RetroArch is a all in one platorm for Emulation that enhances the games you play
and other Cores. From retro era to modern games, you can even play online with friends!"" 

This is a fantastic, easy to digest, explanatory, to the point advertisement that tells people what
retroarch is, what it DOES (Enhances emulator games), and avoid words like ""nintendo"" or ""platstation"" for
copywrite while still getting the ""old to current"" idea across. Make your text readable to people! If they want more
they can hit up a FAQ. Note: Going forward, i will be referring to a reasonable way to read something, or
a problem with text that is to jargin-driven as ""Readability"" as a unbrella term. This will come up A LOT
and will be a serious problem with retroarch in nearly every part of anyones experience. Please remember this.

Lets move on. in the websites next section, it says you can run original game discs, but steam says 
the feature isn't out yet. These are literally giving conflicting information. Seriously? ITS YOUR FRONT PAGE!

Next, lets look at some visuals here. For the website, i want to look at the ""A polished Interface"" Slide.
This makes it look like playstation, cool. Except this isn't what it looks like in retroarch by default.
Make it clear in advertising this is a skin. Also, ironically, the interface is excessively NOT polished, but
i will be getting to that as a major complaint in another chapter. But right now, it's a skin, say it's
a skin. The very idea their not saying its not the default skin here, implies someone knew it was terrible and
tried to hide it. Maybe ask why they needed to hide your real interface? We will get to that later...

Moving back to steam, lets look at the slides/image roll. First is the trailers. All three are really fucking bad.
Trailer 1: its a fun logo (sure, fine) then shows the menu with garbage readability that makes a person
want to flee just looking at it, followed by showing what has GOT to be one of the WORST POSSIBLE game examples.
like, you CANNOT be serious. the website slides show mario, and this trailer has a level ID called zero / 
megaman129. Show us a actual video game. Make the trailer show like 8+ games of different genres. 
Make it CLEAR that retroarch plays VIDEO GAMES, and not, whatever the fuck that is. Bad advertising affects
end user readability to understand what retroarch is and is not. and this is contributing to why you get topics
saying people are confused about retroarch.

Trailer 2: Literally a logo. Why is this here? Remove it.
Trailer 3: This feels like a meme. Seriously, this is cruel, it hurts. It's like someone took what 
makes a good intro video and butchered it to the max. No BGM, no person explaining things, no examples
of games being played, it's super fast (Epilepsy educing even?). So fast you can't even read all the text
without pausing the video. Please make a real trailer. Look at like, any nintendo direct. Do that, but 
for retroarch. Show off and explain lots. I'll help QA it if you need someone. We can improve this.

Then the steam slides, their mostly the exact same menu. Why? Why not Show off various parts of retroarch? 
Actually make people interested. Your telling me you create all of retroarch, and the only part
you want people to know about is 2 screens? wtf? 

Now i can go on, but the pre-retroarch experience is getting comically long at this point.
I'll move onto actual retroarch, but rest assured, i can REALLY go on about whats wrong before even downloading.

In summary
1: Very poor readability
2: Poor advertising
3: Bad use of space
4: inconsistent between steam and website
5: Doesn't actually say what retroarch does in words people can understand let alone google. 


===============Chapter 1: ""A Polished Interface""? Actually It's Really Fucking Garbage======================

Note: I have gotten some responses to points in this chapter from users using custom alternate skins for
RetroArch that change how the menu actually functions and displays information, and not just the visuals.
This chapter is about a first time experience, and as such uses the default skin. 

Before i actually talk about the REAL first impression, i want to point out i downloaded website and steam
both, to check for differences. Guess what? The first impression is diffrent. Inconsistent again? Ugh.
From website, it launched windowed, and you lack the cool cursor, and get the ultra low quality windows
menu that feels like a 2005 program with file/command/windows despite that retroarch has it's own in-app menus.
This doesn't happen on steam, it launches in fullscreen on first boot. The difference here is very important.
Not everyone knows you can Alt+Enter programs to fullscreen them. Infact, this is semi-rare info. 
This is made worst by F11 being the windows default to fullscreen, but boy do i wish complaining ended here.

If a non-steam user tries to fullscreen by hitting F11, retroarch hotkeys this to swapping between the windows
and custom cursor. Now, the custom one looks good, but it locks you into the windows. Surely you can
make your cursor not get locked, come on. But even worse, is when someone wants out and presses F11 again. 
I CANNOT BELIEVE how bad this is. If you are multi-monitor, your cursor is now locked to the monitor
retroach was in itself, untill you move retroarch. Clickin on it, pressing hotkeys ect won't fix this.
I don't mean using the retro cursor, i mean normal windows cursor, you can't leave your moniter. 
what the actual fuck? what a joke! I haven't even talked about retroarch contents directly, and
i can barely type up this QA review because retroarch literally won't let my cursor go click on notepad. >:(

it gets worse. If your fullscreen and press F11, your cursor doesn't change, but your mouse still can't
leave the moniter. The user would have no idea why and think retroarch is just a bad program. How can
you have this many errors and i'm not even talking about retroarch itself yet?????


OKAY, actual retroarch time. You can argue before this, thats a unintended bug, sure. Lets looks at whats NOT a bug.
So, we boot into the main menu that is...lacking readability at best.  ""Load Core""? Why? these things are called emulators, 
call them emulators. ""Load Emulator"" is perfectly reasonable English. It's my understanding the devs want the term
Cores because it also uses something called Cores in addition to emulaitors. Sure, but you want to be
a marketable, understandable program? speak in words people actually understand. it's my understanding the team personally wants 'Load Core / Content' as they want to do more then emulators.  In counter to this, i have 2 suggestions. Either split the menu AND load between Emulators (Load Emulaitor / Game), and other Cores (Load Core / Content), or make the load
button hidden untill a emulaitor or core is ready, then have it show Load Game or Load Content based on what Emu/Core
is loaded! Easy! Also, this menu is bloated.

So, 
1: Split the cores menu, between ""Cores"", and a new one called ""Emulators""
2-1: In addition to Load Content, add a new menu called Load Game right under Emulators.
2-2: Or, make load content/game hidden until a emulator or core is loaded in already.
3: Also change Esc defaulting to exiting the program, to a back button. This is semi-common, and straight up default
in nearly every PC game. If your a platform on a PC for people gaming on a PC, use a PC control scheme. wtf?
there is a quit button if needed, and Alt+F4 or Alt+Enter, we don't need to troll PC users with a comically
bad default control scheme thats normally a menu button on almost every pc video game ever released.

""Bloated"" i'm going to define, as to many things being in one menu, or things being in the wrong location.
Here, we have the main menu showing config, info, and help. But we have a left side bar. whats the 
difference between config and settings? this is already a readability problem, they can be seen as synonyms. 

Now, i'm gonna be going all over unfocused here, because there is so many problems their all interwoven.
The Config file menu, why is this a menu? 

1: Delete the Config menu entry
2: Move the contents of the config menu, to the top of the settings menu. 
3: Why is help a menu with only 1 content? Literally delete this menu.
4: Move Menu Controls and it's icon to the bottom left of retroarch below the white line. This menu is 
first of all stupid and should probably be removed entirely, however...
4-2: Rename Menu Controls to Controller and have this lead directly to the input menu. If you really want,
include the Menu controls in Controller, but really it's common sense, and people can customize this,
so the stated controls in Menu Controls can literally lie to the user, this is dumb.
5-1: on first startup, maybe do a first time setup that asks the user what menu icons they would like.
let them pick from switch, xbox, playstation, ect. Its also kinda bad to display Switch by default, but
use american playstation input scheme, and not having a switches A button actually be A. Readability issues!!!!!
5-2: If that is a copywrite problem, alternatively let people customize the on-screen symbols with letters. In this way,
people can recreate their console of choice setup while working around looking like it clones a competitor, because
it's a customization option of a user. And don't tell me customization options aren't fair game when your straight up
stealing the playstation home menu theme >_>


Now that thats done, we have a nice (enough) looking home. Onward to settings. The entire thing is bloated. 
There is also a lot of 2-menus required trolls. There are so many times you look in one menu and need to 
google to find another menu, it's not even funny. 

1: First up, just like the white line at the bottom of retroarch, i recommend a new break line in the middle
of settings. Things below it will be advanced settings, and title the section Advanced. Alternatively, if
this won't translate well to alternate skins, instead have a menu in here called Advanced at the bottom of settings.
Rename Saving to Saving Games (Readability, this seems like save Retroarch settings when it's here like this!)
2: Throw logging, File Browser, On-Screen Display, AI Service, Latency, into Advanced.
3: Delete the entire power management menu. wtf is this? It's literally empty. LITERALLY, USELESS. WHO????
4: Rename Network to Online, or Online Multiplayer. (Readability!)
5: Rename Input to Controllers

6-1: On the left bar, Rename Import Content to ""Create Playlist"" (Because that is literally what it does)
6-2: Move the playlists menu into the now named ""Create Playlist"" menu, de-bloat settings, so only people 
who care about the playlist menus are forced to see and scroll past this option. 
6-3: Add a button named ""More Information on Playlists"" or maybe ""Playlists Tutorial"" or similar as well
to explain what is a 'Playlist', how to set them up, needing cores/emulators, ect.
6-4: if a Scan to create a playlist in a directory comes up empty, have a pop up explaining why. (Missing cores, ect)

7: I cannot believe you have a menu called configuration here, when i earlier talked about configuration in
the main menu. Why do you have 2 config menus, and a settings menu? HELLO? This is so troll, the
config options in settings/config effect your stuff in main menu/config. Why the fuck are they not together?
Who designed this? *Phew* now that thats done.....
7-1: In this submenu, move 'Use Global Core Options File' to the 'Load Core' Menu we are renaming to 
'Load Emulator'.
7-2: We are also renaming this option ""Use Global Emulator Options"".
Readability is very important! 
7-3: Finally, If (hopefully) Main menu config is moving to settings, either delete this Configuration menu, 
and move its only remaining option (Save Config on Quit) to settings, or move the config stuff from 
main menu, inside this config sub-menu. 

8: AGAIN with the 2 menus needed trolls. Inside Directory we have a lot of playlist stuff, why?
8-1: In playlist menu (that we are moving to the menu in point 6 we are calling ""Create Playlist"" Menu) 
create a new menu called Playlist Directories
8-2: Move everything playlist related from settings/Directory to Music/Playlist Directories. 
This is to de-bloat menus with unnecessary information overflow, and put settings in menus they are actually
used in, like any other emulaitor (PCSX2/RPCS3/Citra/Cemu/Yuzu/ect)


9: Speaking of multiple same named menus, we have achievements, and a users achievements. STOP DOING THIS!!!!
why do you want people to need to google one menu doesn't work without the other menu! THIS IS SO TROLL, STOP IT!
9-1: Rename User to Account Settings & Achievements
9-2: Move the achievements menu into this newly named menu to de-bloat settings, and put this in a reasonable spot.
9-3: The Accessibility menu only has 1 option in it. Stop having troll 1 option menus. Delete this menu
9-4: Rename the Sub-menu ""Accessibility Enable"" To ""Text to Speech""
9-5: Move ""Text to Speech"" to the Account Settings & Achievements, so one user can use it, and
another different user doesn't have to deal with troll text to speech in their ears. 

10: Continuing from 9, we have streaming options in account, and them in recording. WHY.
10-1: Rename ""Recording"" to ""Recording & Live Streams""
10-2: move the the content of the now ""Account Settings & Achievements"" / Account  / Retroachievements to
the Achievements menu that was moved here in part 9. 
10-3: The Youtube/Twitch/Facebook Gaming sub-menus only have 1 content, STOOOP FUCKINGGG DOOOING THIIIIS. >:((((
we are deleteing these sub-menus. 
10-3: Move the menus inside the Youtube/Twitch/Facebook Gaming sub-menu stream key options from here, to the now 
named ""Recording & Live Streams"" menu. 
10-4: in the ""Recording & Live Streams"" menu/Streaming Quality, give one of those grey sub-descriptions.
Something like ""Suitable for 720P Streams"" ""For 1080P Streams"" Ect. 
10-5: Also rename the menu to Stream Quality.

Oh boy, onto input (that we said we are renaming to Controller)
11-1: Move Maximum Users to...somewhere. Wtf even is this menu? If this actually means users, move it to the
relevant online menu. If it means locally, rename this to ""Maximum Controllers"". 
11-2: If the later, Create one of those Advanced Options line breaks again at the bottom, and move this
down there, along with every other option in this menu untill we get to Menu Controls. 
11-3: MENU CONTROLLS? HELLO? Why does main menu get help on menu controlls, BUT WE HAVE A ACTUAL MENU CONTROLS MENU? PUT RELEVANT MENUS IN RELEVANT PLACES THANK YOU.
11-4: Move the Automatic Mouse Grab option to the Video menu in Settings. This is something usually in that
menu on other programs, stop being unintuitive!!! How can you make so many mistakes!!!

Speaking of the Video menu, lets tackle that next.
12-1:in Fullscreen Mode, rename Windowed Fullscreen to ""Borderless Fullscreen"" or just ""Borderless"",
and change the description appropriately.
12-2: Rename Fullscreen Width & Height to ""Custom Width"" (& Height). It's already in the fullscreen sub-menu, 
you don't need to remind them what menu they are in, it's at the top of the screen!
12-3: Move CRTSwitchRes and Output below the Fullscreen and Windowed sub-menus. Order matters! Most important
things go on top!

I can keep going, but i'll stop here. This is a Serious Checklist of Readability improvements across the 
entire settings menu. However, if i reconsider, i may suggest adding Controller to the left bar as a major
menu instead, as it's reasonab;y so important, it should be a major menu, rather then some garbage like Music.


=====================Chater 2: A Smoothe Gameplay Experience? Or A Troubleshooting Nightmare?=====================

Before i get into further details on games, i strongly recommend a new setting somewhere, on by default, that
makes it so when you load a game, and pick a emulaitor (Remember, we are calling these Games and Emulaitors, NOT
Cores and Content) that it associates that game to that emulaitor by default. In the future, you should be
able to Load game -> Pick the game -> it just boots up without needing to pick a emulaitor. 

I...HOPE i don't have to explain how bad it is that steam retroarch is missing a download button. I will skip
over the easy wall of text on this and move forward.

1: For steam users, there should still be a download emulaitor button, but as a submenu. 
1-1: In here, There should be a link to your website with a list of emulaitors to download.
1-2: There should be a setting here, or in the Install Emulaitor menus to set the directory for cores.
This is because this options is only useful & relevant right at this moment, and never again, so it
does not need to be in the directroies menu elsewhere.

2: When in-game, move the Command/Menu button to the top of the list. 
I also feel a menu rename is in order, but can't think of one right now, and want to move on.
3-1: Add save state and load state to the Command menu. Yes i know they are in the F1 menu. Make them accessable.
3-2: In the F1 menu, move State slot below load state

And oh boy, lets get into the F1 menu. In no particular order...
4-0: Make it WAY more obvious to assign a button or combo to open this menu from controller.
probably include it in the quick menu directly at the bottom, even if the option exists somewhere else,
just duplicate it. 
4-00: Also, Escape should open the quick menu by default
4-1: Originally i said Rename Information to Game Information, but i forgot i was still allowing for cores/content menus before.
i now suggest making this menus name be contextual, based on if a emulator, or core is loaded. This way people only using
emulators can have it reasonably say 'Game Information' and those who use cores still get 'Information' so it
doesn't clash with any development team desires.
4-2: Move the Controller menu to right below Close Content
4-3: Also rename that to Close Game
4-4: Adding something to favorites and you can't remove it? WHY? 
Make this menu change to Remove from Favorites if it's a favorite >_>
4-5: Remove the descriptions for Resume, Restart, and Close game. This screen has lots the player wants
to see and not enough screen space even when in fullscreen. 
4-6: Also remove descriptions on save state, load state, Take Screenshot, Cheats, Achievements, and Rewind.
4-7: (Someone gave a good point against this, nevermind)
4-8: Make it so if the player presses cancel in the F1 to go to Main Menu, Then cancel again, it resumes the game.
4-9: move Show Desktop Menu from Main Menu while ingame to somewhere else (Probably settings).

With that out of the way, once ingame, there is no menu option to go back!
Add a button in windowed mode to open back the retroarch menu. If you tell me it's a hotkey, i don't care.
thats not intuitive! Make a button to go back to the retroarch menu!!!


Finally, this SHOULD be obvious, but it's not, so i will say it here.
I assume this is a major feature, but really, let people access that emulaitors settings when using it.
if we can't do that, retroarch is just worse then every emulaitor, because they have settings you NEED to even
run some games. If this option DOES exist, it's EXTREMELY hidden!

Moving on, post gameplay, your recent games don't show up in load content / load game.
Why isn't there a recent games list? And why can't you set a content / game directory in directories? 
If you can, that option should 100% be in the load content / Load Game menu, as this is where
it's actually relevant!!!


=======================Ending==================

I'll stop here, it's been 3 hours of complaints, but retroarch is riddles with terrible quality.
Why would anyone want to use this? 
1: An unfun troubleshooting experience in exchange for playing games in the worst quality possible 
without access to emulators settings. 
2: Some games are literally unplayable in retroarch because you can't access emu settings
3: The menus are bloated, and lie / decieves / give half truths by hiding relevant other options in other menus
for no other reason then to troll you. The overall feeling is Retroarch is giving you the run-around, asking you
to do something in some menu you don't know where it is, and when you find it, wants another setting in
another menu you don't know where it is. 
4: it's a pain to setup even for simple emulators
5: you can't even understand what retroarch is or isn't from inconsistent and contradictory messages,
to near moon-rune levels of advertising.
6: unintuitive menus and lacking default settings mean you need to go over everything before even attempting
to test a single title.
7: Using it via steam is a whole extra nightmare that could be made easy if anyone actually fucking cared.

It's easy to see why people don't recommend recroarch. The real question is if anyone is going to actually
do anything about it, or delete this post / no reply / no one for me to directly work with in a timely fashon /
stick their head in the clouds and ignore why people don't enjoy their product. 

My discord is Dawnbomb#3408, and i'm open to any actual developer contacting me and working hand in hand for
some actual quality assurance before the steam deck launches, but considering how obvious, easy, and
near intensionally-troll the levels of design are here, i expect no contact and noone willing to work to make
retroarch not be complete fucking bullshit bloatware on a PC, and i only scratched the surface of this QA nightmare.



====================Chapter 3: Extra add-ons i thought of later=========================

Note: Some of the things here delve closer to more serious changes to retroarchs codeing.
Still, their good changes.

1: Controller backround input should be in the input (Hopefully named Controller) Menu, not in another menu (User Interface).
Setting up controller input in your controllers, then having if the actual input happens be decided in a separate menu, is another weird location problem.
2: While im at it, Turbo Fire should be a setting INSIDE a controllers port controls. This would allow each controller to have separate turbo settings per player. 
3: Altho highly unusual, some players enjoy playing games with two people one controller mode. Some people even go so far as to speedrun like this. Allowing multiple controllers assign buttons to the same button on the same port of a input would go a long way in support for those people. ",True,Vulgarity
Everything Wrong With Retroarch?! A massive complaint to why people think it fucking sucks.,MrHuu,997299420,2,920211783,0,"I think this rant belongs in the forums or any other social channel of libretro. Not on the github issue tracker.

If you're serious in your offer to help out with 'quality assurance', that would be great. But i think the best approach would be contacting the developers on the libretro forums or discord and discuss your suggestions.

Forum: https://forums.libretro.com/
Discord: https://discord.com/invite/C4amCeV",False,0
Everything Wrong With Retroarch?! A massive complaint to why people think it fucking sucks.,sonninnos,997299420,3,921061432,0,"Repeated fowl language, capslock cruising, know-it-all attitude, and giving thumbs up to their own posts should be bannable offenses.
",False,0
Everything Wrong With Retroarch?! A massive complaint to why people think it fucking sucks.,m4xw,997299420,4,921071710,1,"I appreciate the time you took to write this up but we don't appreciate your wording.
However, I will let this slip because you called it, but please not again.

While I agree on _some_ parts, in the end its a matter of time and money.
Everyone here does this in their free time, so unless you can come up with 20k in funding at minimum to work on these, I dont see the codebase reaching proper production-grade quality anytime soon.
If i have the choice between a full stomach and catering to anonymous dickheads, well the choice is obvious.
You can reach out to us anytime if you want to get involved and donate your time working on QoL issues, but then use Discord so it can be coordinated.
I can tell you tho, these issues are like the last of our problems with the codebase.. Theres way more fundamental/bigger fishes to fry first.
I recommend you split up your post into several seperate issues, completely reword it and try talking like a normal person.
",False,Impatience
Made some Grammatic correction,josephmathai10,1014671924,1,1014671924,0,"## Standards checklist:

<!-- Fill with an x the ones that apply. Example: [x] -->

- [ ] The PR title is descriptive.
- [ ] The PR doesn't replicate another PR which is already open.
- [ ] I have read the contribution guide and followed all the instructions.
- [ ] The code follows the code style guide detailed in the wiki.
- [ ] The code is mine or it's from somewhere with an MIT-compatible license.
- [ ] The code is efficient, to the best of my ability, and does not waste computer resources.
- [ ] The code is stable and I have tested it myself, to the best of my abilities.

## Changes:

- [...]

## Other comments:

...
",True,0
Made some Grammatic correction,josephmathai10,1014671924,2,933280140,0,"Excuse me, I was just trying to help you and is this how you thank me. By marking it spam. If you dont need it then just close the request.
",False,Bitter frustration
Made some Grammatic correction,mcornella,1014671924,3,933283752,1,"The way it is written is correct. Installation problems is correct. `@ohmyzsh on Twitter` is correct. Your PR is invalid, useless, and it costs me time to triage. You're the 4th person to open a spam PR. I hope you reconsider your ways to obtain the Hacktoberfest t-shirt and start considering maintainers time as valuable.",False,Bitter frustration
Dependency Dashboard,renovate[bot],1026512862,1,1026512862,0,"This issue lists Renovate updates and detected dependencies. Read the [Dependency Dashboard](https://docs.renovatebot.com/key-concepts/dashboard/) docs to learn more.

## Rate-Limited

These updates are currently rate-limited. Click on a checkbox below to force their creation now.

 - [ ] <!-- unlimit-branch=renovate/eslint-8.x -->chore(deps): update dependency eslint to v8.57.0
 - [ ] <!-- unlimit-branch=renovate/major-yarn-monorepo -->chore(deps): update yarn to v4
 - [ ] <!-- unlimit-branch=renovate/discordjs-rest-2.x -->fix(deps): update dependency @discordjs/rest to v2
 - [ ] <!-- unlimit-branch=renovate/nextui-org-react-2.x -->fix(deps): update dependency @nextui-org/react to v2
 - [ ] <!-- unlimit-branch=renovate/axios-1.x -->fix(deps): update dependency axios to v1
 - [ ] <!-- unlimit-branch=renovate/express-rate-limit-7.x -->fix(deps): update dependency express-rate-limit to v7
 - [ ] <!-- unlimit-branch=renovate/pretty-ms-9.x -->fix(deps): update dependency pretty-ms to v9
 - [ ] <!-- create-all-rate-limited-prs -->üîê **Create all rate-limited PRs at once** üîê

## Open

These updates have all been created already. Click a checkbox below to force a retry/rebase of any.

 - [ ] <!-- rebase-branch=renovate/node-17.x -->[chore(deps): update dependency @types/node to v17.0.45](../pull/1276)
 - [ ] <!-- rebase-branch=renovate/discord-api-types-0.x -->[fix(deps): update dependency discord-api-types to v0.37.73](../pull/1179)
 - [ ] <!-- rebase-branch=renovate/react-monorepo -->[chore(deps): update dependency @types/react to v18.2.64](../pull/1177)
 - [ ] <!-- rebase-branch=renovate/yarn-monorepo -->[chore(deps): update yarn to v3.8.1](../pull/1182)
 - [ ] <!-- rebase-branch=renovate/axios-0.x -->[fix(deps): update dependency axios to ^0.28.0](../pull/1286)
 - [ ] <!-- rebase-branch=renovate/passport-0.x -->[fix(deps): update dependency passport to ^0.7.0](../pull/1280)
 - [ ] <!-- rebase-branch=renovate/prettier-3.x -->[chore(deps): update dependency prettier to v3](../pull/1281)
 - [ ] <!-- rebase-branch=renovate/typescript-5.x -->[chore(deps): update dependency typescript to v5](../pull/1282)
 - [ ] <!-- rebase-branch=renovate/major-nextjs-monorepo -->[chore(deps): update nextjs monorepo to v14 (major)](../pull/1283) (`eslint-config-next`, `next`)
 - [ ] <!-- rebase-branch=renovate/node-20.x -->[chore(deps): update node.js to v20](../pull/1284) (`node`, `@types/node`)
 - [ ] <!-- rebase-all-open-prs -->**Click on this checkbox to rebase all open PRs at once**

## Ignored or Blocked

These are blocked by an existing closed PR and will not be recreated unless you click a checkbox below.

 - [ ] <!-- recreate-branch=renovate/nextui-org-react-1.x -->[fix(deps): update dependency @nextui-org/react to v1.0.0-beta.9-dbg2](../pull/1176)
 - [ ] <!-- recreate-branch=renovate/better-erela.js-spotify-1.x -->[fix(deps): update dependency better-erela.js-spotify to v1.3.11](../pull/1178)
 - [ ] <!-- recreate-branch=renovate/prettier-2.x -->[chore(deps): update dependency prettier to v2.8.8](../pull/1185)
 - [ ] <!-- recreate-branch=renovate/typescript-4.x -->[chore(deps): update dependency typescript to v4.9.5](../pull/1076)
 - [ ] <!-- recreate-branch=renovate/nextjs-monorepo -->[chore(deps): update nextjs monorepo to v12.3.4](../pull/1082) (`eslint-config-next`, `next`)
 - [ ] <!-- recreate-branch=renovate/node-16.x -->[chore(deps): update node.js to v16.20.2](../pull/1181)
 - [ ] <!-- recreate-branch=renovate/colors-1.x -->[fix(deps): update dependency colors to v1.4.0](../pull/1183)
 - [ ] <!-- recreate-branch=renovate/node-fetch-2.x -->[fix(deps): update dependency node-fetch to v2.7.0](../pull/1180)
 - [ ] <!-- recreate-branch=renovate/better-erela.js-apple-1.x -->[fix(deps): update dependency better-erela.js-apple to v1](../pull/708)
 - [ ] <!-- recreate-branch=renovate/discord.js-14.x -->[fix(deps): update dependency discord.js to v14](../pull/1006)
 - [ ] <!-- recreate-branch=renovate/node-fetch-3.x -->[fix(deps): update dependency node-fetch to v3](../pull/576)

## Detected dependencies

<details><summary>docker-compose</summary>
<blockquote>

<details><summary>docker-compose.yml</summary>


</details>

</blockquote>
</details>

<details><summary>dockerfile</summary>
<blockquote>

<details><summary>Dockerfile</summary>

 - `node 17.9.1-alpine`

</details>

</blockquote>
</details>

<details><summary>npm</summary>
<blockquote>

<details><summary>dashboard/package.json</summary>

 - `@emotion/react ^11.9.3`
 - `@emotion/styled ^11.9.3`
 - `@mui/icons-material ^5.8.4`
 - `@mui/material ^5.8.4`
 - `@nextui-org/react 1.0.0-beta.9`
 - `next 12.2.4`
 - `react 18.2.0`
 - `react-dom 18.2.0`
 - `@types/node 17.0.41`
 - `@types/react 18.0.16`
 - `eslint 8.19.0`
 - `eslint-config-next 12.2.4`
 - `typescript 4.7.4`

</details>

<details><summary>package.json</summary>

 - `@discordjs/builders ^1.4.0`
 - `@discordjs/rest ^1.5.0`
 - `axios ^0.27.0`
 - `better-erela.js-apple ^0.1.0`
 - `better-erela.js-spotify 1.3.9`
 - `colors 1.3.3`
 - `discord-api-types 0.37.1`
 - `discord-together ^1.3.25`
 - `discord.js ^13.14.0`
 - `dotenv ^16.0.1`
 - `ejs ^3.1.6`
 - `erela.js ^2.3.3`
 - `erela.js-deezer ^1.0.7`
 - `erela.js-facebook ^1.0.4`
 - `erela.js-filters ^1.2.6`
 - `express ^4.17.1`
 - `express-rate-limit ^6.2.0`
 - `express-session ^1.17.3`
 - `express-ws ^5.0.2`
 - `js-yaml ^4.1.0`
 - `jsoning ^0.13.0`
 - `lodash ^4.17.21`
 - `moment ^2.29.1`
 - `moment-duration-format ^2.3.2`
 - `node-fetch 2.6.7`
 - `os ^0.1.2`
 - `passport ^0.6.0`
 - `passport-discord ^0.1.4`
 - `pretty-ms ^7.0.1`
 - `rlyrics ^2.0.1`
 - `systeminformation ^5.9.12`
 - `winston ^3.3.3`
 - `youtube-sr ^4.3.4`
 - `prettier 2.6.2`
 - `node >=16.x <=16.16`
 - `node 16.15.1`
 - `yarn 3.3.0`

</details>

</blockquote>
</details>

---

- [ ] <!-- manual job -->Check this box to trigger a request for Renovate to run again on this repository

",True,0
Dependency Dashboard,XenonTheInertG,1026512862,2,943531034,0,Delete this issue instead of closing,False,0
Dependency Dashboard,DarrenOfficial,1026512862,3,944012845,0,lmao ok,False,Mocking
Dependency Dashboard,AryanTah2005,1026512862,4,944038831,0,"> lmao ok

Me dumdum",False,Mocking
Dependency Dashboard,AryanTah2005,1026512862,5,944161259,0,"> Delete this issue instead of closing

",False,0
Dependency Dashboard,AryanTah2005,1026512862,6,944162049,0,lets marry @renovate-bot,False,0
Dependency Dashboard,SudhanPlayz,1026512862,7,944215787,0,let the war begins,False,0
Dependency Dashboard,AryanTah2005,1026512862,8,944869828,0,"> let the war begins

Lmaoo",False,0
Dependency Dashboard,SudhanPlayz,1026512862,9,944878336,0,"> Delete this issue instead of closing

Deleted two times this is the 3rd time its opening xD",False,0
Dependency Dashboard,XenonTheInertG,1026512862,10,944896202,1,Stupid bot,False,Insulting
P.S.A. Holidays,denelon,1047632192,1,1047632192,0,"Hello everyone. I wanted to let you all know as we enter into the holidays, there will be periods of less activity. Many of the Microsoft employees working on the project have substantial amounts of paid time off subject to loss if they are not taken this year. Many of the areas of focus during this time are in support of engineering improvements to help make the team more efficient when troubleshooting issues, or spending time ""on-call"" to support various aspects of the services and integration between GitHub and Azure DevOps.",True,0
P.S.A. Holidays,CharlotteEm,1047632192,2,968379602,0,"> Many of the Microsoft employees working on the project have substantial amounts of paid time off subject to loss if they are not taken this year

We're really sad that your vacation and bonus will be lost if you don't work on the holidays. 
",False,Mocking
P.S.A. Holidays,Masamune3210,1047632192,3,968390989,0,"its how most companies are, unfortunately. have fun with your time off and relax everybody",False,0
P.S.A. Holidays,EqualDust,1047632192,4,981682277,1,winget is disgusting,False,Insulting
/plot delete - A set block timer is bound to either the current plot or you. Please wait for it to finish.,superkrebs13,1060667929,1,1060667929,0,"### Server Implementation

Spigot

### Server Version

1.17.1

### Java Versions (tested)

Java 16.0.1 Zulu
Java 16.0.2+7 Temurin

### Describe the bug

After claiming a plot ""/plot claim"" I got teleported to it.
If I enter plot and type ""/plot delete"" nothing is happend. After typing it again the message ""A set block timer is bound to either the current plot or you. Please wait for it to finish."" appeared.

**If the world is generated as a flat/normal world, this error didn't occurs.**

### To Reproduce

1. Generate a augmented or partial world with predefined/default values
2. Claim a plot
3. Get into the claimed plot
4. Show info of plot and make sure that you're the owner
5. Try to delete the plot (Nothing will be promted to the console)
6. Type the delete command once again to the console - error occurs

### Expected behaviour

The plot have to be deleted.

### Screenshots / Videos

_No response_

### Error log (if applicable)

_No response_

### Plot Debugpaste

Java 16.0.1 Zulu
https://athion.net/ISPaster/paste/view/e39d51e936ea41cd9f69da70993e0137

Java 16.0.2+7 Temurin
https://athion.net/ISPaster/paste/view/0b2ba302aea94f4dbb3adde606aa8d59

### PlotSquared Version

PlotSquared-6.1.4-Premium

### Checklist

- [X] I have included a Plot debugpaste.
- [X] I am using the newest build from https://www.spigotmc.org/resources/77506/ and the issue still persists.

### Anything else?

I'm using a proxmox hypervisor with Debian 11 running a LXC Container with nesting activated on it.

[Server thread/INFO]: This server is running CraftBukkit version 3284-Spigot-3892929-0ebef35 (MC: 1.17.1) (Implementing API version 1.17.1-R0.1-SNAPSHOT)

Steps how I create the world:

[00:05:22] [Server thread/INFO]: superkrebs issued server command: /plot setup PlotSquared
[00:05:32] [Server thread/INFO]: superkrebs issued server command: /plot setup partial
[00:05:40] [Server thread/INFO]: superkrebs issued server command: /plot setup worldorder
[00:05:51] [Server thread/INFO]: superkrebs issued server command: /plot setup 0,0
[00:05:56] [Server thread/INFO]: superkrebs issued server command: /plot setup 11,0
[00:06:00] [Server thread/INFO]: superkrebs issued server command: /plot setup ALL
[00:06:04] [Server thread/INFO]: superkrebs issued server command: /plot setup 62
[00:06:06] [Server thread/INFO]: superkrebs issued server command: /plot setup 42
[00:06:14] [Server thread/INFO]: superkrebs issued server command: /plot setup minecraft:stone
[00:06:19] [Server thread/INFO]: superkrebs issued server command: /plot setup true
[00:06:25] [Server thread/INFO]: superkrebs issued server command: /plot setup minecraft:grass_block
[00:06:30] [Server thread/INFO]: superkrebs issued server command: /plot setup minecraft:stone_slab
[00:06:36] [Server thread/INFO]: superkrebs issued server command: /plot setup minecraft:sandstone_slab
[00:06:40] [Server thread/INFO]: superkrebs issued server command: /plot setup 7
[00:06:43] [Server thread/INFO]: superkrebs issued server command: /plot setup 62
[00:06:50] [Server thread/INFO]: superkrebs issued server command: /plot setup minecraft:quartz_block
[00:06:58] [Server thread/INFO]: superkrebs issued server command: /plot setup minecraft:stone
[00:07:02] [Server thread/INFO]: superkrebs issued server command: /plot setup 62
[00:07:06] [Server thread/INFO]: superkrebs issued server command: /plot setup true
[00:07:11] [Server thread/INFO]: superkrebs issued server command: /plot setup wolrd139",True,0
/plot delete - A set block timer is bound to either the current plot or you. Please wait for it to finish.,NotMyFault,1060667929,2,980066556,0,Can you replicate that by using an up to date version of WorldEdit and Paper?,False,0
/plot delete - A set block timer is bound to either the current plot or you. Please wait for it to finish.,superkrebs13,1060667929,3,980098474,0,"Hello,

setted up a fresh and clean paper with word edit and premium version of plot squared.",False,0
/plot delete - A set block timer is bound to either the current plot or you. Please wait for it to finish.,superkrebs13,1060667929,4,981585352,0,"Problem persits.

https://athion.net/ISPaster/paste/view/4127043609944036bd62e782e5514125

Any Idea?

<blockquote><div><strong><a href=""https://athion.net/ISPaster/paste/view/4127043609944036bd62e782e5514125"">Incendo Paste Viewer</a></strong></div></blockquote>


//EDIT

I setup a complete clean and new dedicated server running debian 11 bullseye on it. Downloaded latest Paper from Gut, installed the needed plugins (plotsquared and worldedit) and retry to delete a plot that was claimed a few seconds ahead. 
Same issue!

https://athion.net/ISPaster/paste/view/8b3197c2d8b14cf59be4ac5029ceb7fa

@NotMyFault - in my opinion you didn't test it if you say ""cannot replicate"" ....

```
root@gs02:/home/newmc# java --version
openjdk 16.0.2 2021-07-20
OpenJDK Runtime Environment Temurin-16.0.2+7 (build 16.0.2+7)
OpenJDK 64-Bit Server VM Temurin-16.0.2+7 (build 16.0.2+7, mixed mode, sharing)
```

```
root@gs02:/home/newmc# java -Xms4096M -Xmx4096M -jar ./paper-1.17.1-388.jar
System Info: Java 16 (OpenJDK 64-Bit Server VM 16.0.2+7) Host: Linux 5.10.0-9-amd64 (amd64)
Loading libraries, please wait...
[07:36:59 INFO]: Environment: authHost='https://authserver.mojang.com', accountsHost='https://api.mojang.com', sessionHost='https://sessionserver.mojang.com', servicesHost='https://api.minecraftservices.com', name='PROD'
[07:36:59 INFO]: Found new data pack file/bukkit, loading it automatically
[07:36:59 INFO]: Reloading ResourceManager: Default, bukkit
[07:37:00 INFO]: Loaded 7 recipes
[07:37:01 INFO]: Starting minecraft server version 1.17.1
[07:37:01 INFO]: Loading properties
[07:37:01 INFO]: This server is running Paper version git-Paper-388 (MC: 1.17.1) (Implementing API version 1.17.1-R0.1-SNAPSHOT) (Git: 22aaf91)
```",False,0
/plot delete - A set block timer is bound to either the current plot or you. Please wait for it to finish.,F-TownGaming,1060667929,5,1013542327,0,"we have the same problem on a Paper 1.18.1 Server FAWE Worldedit is the newest version 68 and Plotsquared is 6.3.0

After claiming a plot ""/plot claim"" I got teleported to it.
If I enter plot and type ""/plot delete"" nothing is happend. After typing it again the message ""A set block timer is bound to either the current plot or you. Please wait for it to finish."" appeared.

If the world is generated as a flat/normal world, this error didn't occurs.",False,0
/plot delete - A set block timer is bound to either the current plot or you. Please wait for it to finish.,F-TownGaming,1060667929,6,1016355946,0,No Answer to Fix this Problem?????,False,Impatience
/plot delete - A set block timer is bound to either the current plot or you. Please wait for it to finish.,F-TownGaming,1060667929,7,1016361276,0,please take care of this bug. we too have the exact same problem.,False,0
/plot delete - A set block timer is bound to either the current plot or you. Please wait for it to finish.,superkrebs13,1060667929,8,1016374930,0,See the date of creation - the publisher isn't interested in fixing this issue.,False,Bitter frustration
/plot delete - A set block timer is bound to either the current plot or you. Please wait for it to finish.,F-TownGaming,1060667929,9,1016377704,0,"If this is the case, we will no longer use Plotquated and will promote it, on the contrary, we will include plotsquaren in the list of the worst plugins and warn users not to spend money on it",False,0
/plot delete - A set block timer is bound to either the current plot or you. Please wait for it to finish.,superkrebs13,1060667929,10,1016386189,0,"We also changed the plugin and will retrieve our money from visa. For a payed plugin, it's the worst support I've ever seen.",False,Bitter frustration
/plot delete - A set block timer is bound to either the current plot or you. Please wait for it to finish.,NotMyFault,1060667929,11,1016389752,0,"I was not able to replicate your issue with the steps provided on the environments provided. And apparently only one other person ran into it but provided no clear instructions how to replicate it.

> See the date of creation - the publisher isn't interested in fixing this issue.

I'm unsure where we ever stated that we will no longer look into the issue? The issue has been reported ~1 month-ish ago and is no show stopper.
General rule of thumb is, that is this an open source project, where everyone is welcome to contribute features, fixes or documentation (like in the nature of any other open source project).
You are more than welcome to look into it yourself and provide a solution that applies to a reproducible list of steps and help to fix your issue.
This is not one of our daytime jobs and we have no obligation to deliver a fix in any specific time frame. We, and any of our other our other contributors, look into issues when and if we have the time to.
Many of us are working full time, are engaged with major exams or are involved in other time consuming projects.
PlotSquared is not the center of the universe everything else wraps around.

> For a payed plugin, it's the worst support I've ever seen.

You bought the plugin to support the people behind it. It's not a support contract with any obligation to deliver a solution to your problem in a specific time frame. Yet again, you are more than welcome to look into itself and help resolving the issue, like in the nature of open source projects, instead of complaining and not providing anything new that helps to resolve the issue.",False,0
/plot delete - A set block timer is bound to either the current plot or you. Please wait for it to finish.,F-TownGaming,1060667929,12,1016394046,0,We've already found an alternative to Plotsquared because we need a working server and can't wait forever for bug fixes we're sorry that we can't support you any further but that's life for the better wins,False,Bitter frustration
/plot delete - A set block timer is bound to either the current plot or you. Please wait for it to finish.,superkrebs13,1060667929,13,1016394531,0,"`Many of us are working full time, are engaged with major exams or are involved in other time consuming projects.
PlotSquared is not the center of the universe everything else wraps around.`

Feel this. 
Thanks for your reply. For me, the issue is solved :)",False,0
/plot delete - A set block timer is bound to either the current plot or you. Please wait for it to finish.,DJMagicMike76,1060667929,14,1017338671,0,"Server Implementation

Paper
Server Version

Paper 1.18.1 163
Java Versions (tested)

openjdk-17-jre:
  Installiert:           17.0.1+12-1+deb11u2
  Installationskandidat: 17.0.1+12-1+deb11u2
  Versionstabelle:
 *** 17.0.1+12-1+deb11u2 500
        500 http://security.debian.org/debian-security bullseye-security/main amd64 Packages
        100 /var/lib/dpkg/status
     17~19-1 500
        500 http://deb.debian.org/debian bullseye/main amd64 Packages
openjdk-17-jdk:
  Installiert:           17.0.1+12-1+deb11u2
  Installationskandidat: 17.0.1+12-1+deb11u2
  Versionstabelle:
 *** 17.0.1+12-1+deb11u2 500
        500 http://security.debian.org/debian-security bullseye-security/main amd64 Packages
        100 /var/lib/dpkg/status
     17~19-1 500
        500 http://deb.debian.org/debian bullseye/main amd64 Packages

Describe the bug

After claiming a plot ""/plot claim"" I got teleported to it.
If I enter plot and type ""/plot delete"" nothing is happend. After typing it again the message ""A set block timer is bound to either the current plot or you. Please wait for it to finish."" appeared.

If the world is generated as a flat/normal world, this error didn't occurs.
To Reproduce

    Generate a augmented or partial world with predefined/default values
    Claim a plot
    Get into the claimed plot
    Show info of plot and make sure that you're the owner
    Try to delete the plot (Nothing will be promted to the console)
    Type the delete command once again to the console - error occurs



PlotSquared Version

PlotSquared-6.3.0-Premium
Checklist


[Server thread/INFO]: This server is running Paper version git-Paper-163 (MC: 1.18.1) (Implementing API version 1.18.1-R0.1-SNAPSHOT) (Git: 4533821)

Steps how I create the world:

[14:05:22] [Server thread/INFO]: superkrebs issued server command: /plot setup PlotSquared
[14:05:32] [Server thread/INFO]: superkrebs issued server command: /plot setup partial
[14:05:40] [Server thread/INFO]: superkrebs issued server command: /plot setup worldorder
[14:05:51] [Server thread/INFO]: superkrebs issued server command: /plot setup 0,0
[14:05:56] [Server thread/INFO]: superkrebs issued server command: /plot setup 11,0
[14:06:00] [Server thread/INFO]: superkrebs issued server command: /plot setup ALL
[14:06:04] [Server thread/INFO]: superkrebs issued server command: /plot setup 62
[14:06:06] [Server thread/INFO]: superkrebs issued server command: /plot setup 42
[14:06:14] [Server thread/INFO]: superkrebs issued server command: /plot setup minecraft:stone
[14:06:19] [Server thread/INFO]: superkrebs issued server command: /plot setup true
[14:06:25] [Server thread/INFO]: superkrebs issued server command: /plot setup minecraft:grass_block
[14:06:30] [Server thread/INFO]: superkrebs issued server command: /plot setup minecraft:stone_slab
[14:06:36] [Server thread/INFO]: superkrebs issued server command: /plot setup minecraft:sandstone_slab
[14:06:40] [Server thread/INFO]: superkrebs issued server command: /plot setup 7
[14:06:43] [Server thread/INFO]: superkrebs issued server command: /plot setup 62
[14:06:50] [Server thread/INFO]: superkrebs issued server command: /plot setup minecraft:quartz_block
[14:06:58] [Server thread/INFO]: superkrebs issued server command: /plot setup minecraft:stone
[14:07:02] [Server thread/INFO]: superkrebs issued server command: /plot setup 62
[14:07:06] [Server thread/INFO]: superkrebs issued server command: /plot setup true
[14:07:11] [Server thread/INFO]: superkrebs issued server command: /plot setup citybuild3

<blockquote><div><strong><a href=""http://security.debian.org/debian-security"">Index of /debian-security</a></strong></div></blockquote>
<blockquote><div><strong><a href=""http://deb.debian.org/debian"">Index of /debian</a></strong></div></blockquote>",False,0
/plot delete - A set block timer is bound to either the current plot or you. Please wait for it to finish.,DJMagicMike76,1060667929,15,1017339643,0,"pleace fix this
",False,0
/plot delete - A set block timer is bound to either the current plot or you. Please wait for it to finish.,DJMagicMike76,1060667929,16,1019834786,0,"it would be possible to give a short feedback when this error will be tackled because we would have to look around for an alternative. I know you have little time and understand that. A bit of info avoids misunderstandings.
We were able to replicate the error several times with a newly installed system.",False,0
/plot delete - A set block timer is bound to either the current plot or you. Please wait for it to finish.,DJMagicMike76,1060667929,17,1020877142,0,"> See the date of creation - the publisher isn't interested in fixing this issue.

I unfortunately have to agree with that. if you don't even have 2 minutes for a feedback, you should leave it alone. felt no sow cares about this bug. I'm very disappointed, I would have expected at least a feedback like ""yes, it's our turn and we'll take care of it shortly"" for this problem",False,Bitter frustration
/plot delete - A set block timer is bound to either the current plot or you. Please wait for it to finish.,DJMagicMike76,1060667929,18,1021974808,0,"no answer, no reaction, what kind of support is that",False,Bitter frustration
/plot delete - A set block timer is bound to either the current plot or you. Please wait for it to finish.,dordsor21,1060667929,19,1021981602,0,"""yes it's our turn and we will look into and maybe implement a fix if and when we have time, since we're all volunteering our free time for this project. Personally, I am in the middle of University exams, which I feel are more important than responding to a GitHub issue to state that we have received an issue and saying nothing that's ultimately going to be much use since we cannot just drop everything we do and are doing just to fix a bug for you, personally.""",False,Entitlement
/plot delete - A set block timer is bound to either the current plot or you. Please wait for it to finish.,DJMagicMike76,1060667929,20,1021985863,1,"then you shouldn't ask for money for this plugin if you can't take care of it properly.
Your reaction does not speak for buying this plugin if the reaction is so snotty.
Then do what you want, we will look for an alternative so you can scare off your users.
In addition, I'm not the only one who has the problem, be happy if you are made aware of mistakes instead of giving such outrageous answers.
Plotsquared is done for me, have a nice life. ",False,Insulting
[Core Request] Yuzu Emulator,Kiba-san,1072125460,1,1072125460,0,"
### Yuzu Emulator Core Request

[So, i thinking a new core as Nintendo Switch called Yuzu, so what do you think?]

Note: this  iis an request, if u add it its a pleasure‚ô°‚ô°
",True,0
[Core Request] Yuzu Emulator,Klauserus,1072125460,2,986945489,0,"i dont like, to make a new issue open. well, i agree but i would like to see a Duckstation on Nintendo Switch :)
THX",False,0
[Core Request] Yuzu Emulator,kbdharun,1072125460,3,988603205,0,"Yeah, It will be a nice addition to Retroarch as it allows us to emulate **Nintendo Switch Games**. 

> ### Yuzu Emulator Core Request
> [So, i thinking a new core as Nintendo Switch called Yuzu, so what do you think?]
> 
> Note: this iis an request, if u add it its a pleasure‚ô°‚ô°

",False,0
[Core Request] Yuzu Emulator,Augusto7743,1072125460,4,998652066,0,"RETROARCH DEVELOPERS.
NOT ADD ANY CORE IF IS EMULATING CURRENT VIDEO GAME GENERATION.
EMULATING CURRENT GENERATION CREATE PROBLEMS FOR HONEST PROJECTS ... MAINLY MAME PROJECT.",False,0
[Core Request] Yuzu Emulator,kbdharun,1072125460,5,998719992,0,"> RETROARCH DEVELOPERS.
> NOT ADD ANY CORE IF IS EMULATING CURRENT VIDEO GAME GENERATION.
> EMULATING CURRENT GENERATION CREATE PROBLEMS FOR HONEST PROJECTS ... MAINLY MAME PROJECT.

I agree with you , even though the addition of this core will be appreciated, since it includes support for current generation Nintendo Switch console it would create problems and legal complications for retroarch. ",False,0
[Core Request] Yuzu Emulator,Augusto7743,1072125460,6,999280045,0,Retroarch need in controller add option to assign same console controller button to multiple game controller thus creating combo buttons helping NES and other games need press 2 buttons at same time and also allow turbo button for any console button.,False,0
[Core Request] Yuzu Emulator,RobLoach,1072125460,7,1001823474,0,"Yuzu is still pretty young, and while it quite possibly could be possible to create a libretro port, it may distract from getting Yuzu stable in the first place. I would recommend supporting Yuzu over at https://yuzu-emu.org , and joining their Patreon, in order to help get it to a solid and stable state.

If anyone wants to have a hack at the libretro port in the mean time, don't let this issue stop you.",False,0
[Core Request] Yuzu Emulator,Augusto7743,1072125460,8,1001853520,0,"EMULATE CURRENT GENERATION IS WRONG !!!
CREATE PROBLEMS FOR OTHERS EMULATOR PROJECTS !
UNHAPPILY HAVE USERS WAITING PLAY CURRENT GENERATION AND WISH UNDERSTAND IF WILL CREATE PROBLEM 
NINTENDO HAVE WAKE UP ABOUT IT ... THEY MAYBE CONTACT POLITICS CREATING LAWS AGAINST ANY EMULATOR PROJECT.",False,Bitter frustration
[Core Request] Yuzu Emulator,muzzol,1072125460,9,1285419380,0,"I was curious about yuzu core and found a guy called Augusto screaming at my face.
I love internet.

by the way Augusto, emulation it's not and it'll never be illegal. you just need to own the games you play.
 ",False,0
[Core Request] Yuzu Emulator,jeois,1072125460,10,1356063691,0,"From a legal standpoint, it doesn't matter if you're emulating the current generation.  It all depends on whether you're following the rules with regard to copyright, and it doesn't matter what generation is the target hardware.  However, from a rights holding company's POV, it probably does matter to them, since they're looking to make more money off the current market.

Like people have said, it's definitely legal, as they have already tried to shut down emulation before through lawsuits, so it's been fully confirmed by the US courts that emulation is legal.  I doubt they'll test that again since the industry has been against emulation forever, and they haven't really done much about it and all that lobbying politicians hasn't amounted to much as of yet.  I could be wrong, but there's no reason to believe things will change without some solid evidence. 

Please consider including yuzu since it is fairly stable, as far as I can tell.  It would be nice to have it as a RetroArch core.",False,0
[Core Request] Yuzu Emulator,Augusto7743,1072125460,11,1359002425,1,"Oh heavens ...
From an gamer only waiting play emulate current system is ""correct"" , but from an viewpoint of an professional game developer and emulator coder of mature projects emulate current system is totally wrong even if the law allow to do it.
Not is impossible Nintendo and others companies having contact with politics and changing the laws at point creating problems for other mature projects. Only reading an reply from an mamedev saying to stop wit it is enough to understand about it.
Unhappily an high percentage of PC gamers have done terrible actions demanding emulate and dump current comercial games ... all to avoid buy an console or even an recent released game doing problems from all sides.
I not believe here several gamers will reply with good sense saying to not add cores to current systems.
That's the because Sony and Nintendo add security protection in console OS.

Retroarch devs NOT add cores of current systems ... THAT IS AN OF MORE WORST PROBLEMS BEING DONE AGAINST MATURE PROJECTS ( MAME ).
If Retroarch continue using cores from before PS3 is all right and not any problem.

Now for example ... if is released an PS5 emulator and each PC have access to PS5 discs and that emulator is being extremely downloaded you believe Sony not will try to do the correct action ?
In past was tried stop some emulators (Connectix Virtual Game Station and others) ...
Today not is the same thing.
If Sony , Nintendo and any other company join to say about that problem to politics will happen problems.

Stop trying to do it. Buy the console or wait to next generation before demanding an emulator for current generation.",False,Impatience
SGDClassifier breaks down when encountering unseen values?,GeorgeZan,1072818585,1,1072818585,0,"### Describe the bug

https://stackoverflow.com/questions/70253946/sgd-breaks-down-when-encountering-unseen-values

### Steps/Code to Reproduce

-

### Expected Results

-

### Actual Results

-

### Versions

-",True,0
SGDClassifier breaks down when encountering unseen values?,ogrisel,1072818585,2,987682838,0,"Is this specific to `SGDClassifier`? Or can you reproduce the problem with other classifiers with different solvers such as `LogisticRegression`?

Can you please craft a minimal reproducer with a synthetic dataset so that we can reproduce the problem without any ambiguity? Please also include the expected and observed results in your report.",False,0
SGDClassifier breaks down when encountering unseen values?,Nivi09,1072818585,3,987947665,0,"Hello, I would like to work on this.  As ogrisel mentioned, can you provide the synthetic dateset to reproduce the problem  ?",False,0
SGDClassifier breaks down when encountering unseen values?,GeorgeZan,1072818585,4,988073948,0,"Hello @ogrisel , @Nivi09 - thank you for your answers.

I do not have any synthetic data, I use a real dataset that I cannot really share.

But if you pick any dataset from the Internet that has categorical variables or even better text then you can easily test what I describe.

It does not have to be only eg with categorical; it can be 9 numerical features and 1 categorical feature.
Especially if the categorical feature has some importance (although the same may happen if it does not; I have not tested that) and in your test set one observation has one unseen value for the categorical feature then the SGD results returns essentially random predictions (or at least not directly related).

It may be that scikitLearn implementation is fine but just SGD itself has this weakness in general?
I think though that RandomForestClassifier does not have the same problem in these cases.

I have not tested much the LogisticRegression.
even like that:
logistic_regression = LogisticRegression(penalty='elasticnet', tol=10, C=1.0, class_weight='balanced', random_state=0, solver='saga', max_iter=20, n_jobs=os.cpu_count()-1, l1_ratio=0.7)
it takes too much time to converge",False,0
SGDClassifier breaks down when encountering unseen values?,Nivi09,1072818585,5,988476317,0,"Hi @GeorgeZan , let me rephrase the issue that you have described and please correct me if I have not understood it correctly.
The task is a classifier algorithm, let's say a data set with 9 numerical predictors and target label as class. For example, let's say you have dataset of len 1000 and your target label contains 5 classes - C1, C2, C3, C4, C5 and there is only one observation for class C5. When you divide it for train and test, your train and test split algorithm has divided the classes C1, C2, C3, C4 divided in some proportion in both train and test, but because observation of class C5 is only one it gets allotted to test set. Now, when you train SGD classifier on train set and then test it on test set, the classifier breaks down for class C5 . Is my understanding correct ? ",False,0
SGDClassifier breaks down when encountering unseen values?,ogrisel,1072818585,6,988627284,0,"> The task is a classifier algorithm, let's say a data set with 9 numerical predictors and target label as class. For example, let's say you have dataset of len 1000 and your target label contains 5 classes - C1, C2, C3, C4, C5 and there is only one observation for class C5. When you divide it for train and test, your train and test split algorithm has divided the classes C1, C2, C3, C4 divided in some proportion in both train and test, but because observation of class C5 is only one it gets allotted to test set. Now, when you train SGD classifier on train set and then test it on test set, the classifier breaks down for class C5 . Is my understanding correct ?

In general, any classifier can never predict a class that was not part of the training set. I am not sure what ""break down"" means in this context.

> But if you pick any dataset from the Internet that has categorical variables or even better text then you can easily test what I describe.

Can you please craft a standalone reproducible code snipet example that generates its own toy dataset so that we can just execute ? I am afraid the English language is ambiguous while Python is not.

Please also report the result you observe and the output you would have expected.",False,0
SGDClassifier breaks down when encountering unseen values?,Nivi09,1072818585,7,988810453,0,"Hi @ogrisel, GeorgeZan has mentioned that - SGD results returns essentially random predictions (or at least not directly related).
I think this is what  break down means. Please correct me if my understanding is not correct @GeorgeZan ? 

And as ogrisel mentioned, my understanding is also the same that no classifier can predict a class that was not a part of training set. 

I read GeorgeZan comment again, so here is my second understanding : 
   input set X has 10 independent variables in which one variable is categorical. Along with that you have categorical dependent variable. Your test fails when there is a class in categorical I.V. which has not been seen in train data set while training and that's where you get random prediction. 

@GeorgeZan, please let me know, out of two examples, which one is correct.
",False,Bitter frustration
SGDClassifier breaks down when encountering unseen values?,GeorgeZan,1072818585,8,988841478,0,"@Nivi09, yes indeed.
""Hi @ogrisel, GeorgeZan has mentioned that - SGD results returns essentially random predictions (or at least not directly related).
I think this is what break down means.""

To give a hypothetical example, let's say I have a model with 9 numerical features and a categorical feature and let's say there are 3 classes in my problem (A, B, C class).
Let's also assume that I have a test set instance X that belongs to the A class.
Specifically, it belongs to class A regardless of the value of the categorical feature.
At best, the categorical feature can help the model to predict class A for X just by inference but it is not a defining feature of X.
i)
If I keep the categorical feature in the model and the test set instance X provided has a known/seen value at the categorical feature then the model returns the following prediction:
A: 0.7
C: 0.2
B: 0.1

iI)
If I drop the categorical feature from the model then the model returns the following prediction:
A: 0.55
C: 0.35
B: 0.1

iii)
If I keep the categorical feature in the model and the test set instance provided has a unknown/unseen value at the categorical feature then the model returns the following prediction:
B: 0.5
A: 0.3
C: 0.2

.........................
Obviously the impact of the categorical features in all these cases depends on how important is the feature for the model; in the example above it is assumed that the categorical feature has quite significant weight.

This is also why in (ii) the top score drops by quite a lot in comparison with (i) given that it is only one feature.

However, and this is what I mean that the model ""breaks down"", in (iii) it returns essentially random predictions.
It does not make sense to me that if you just receive an unseen value your prediction changes so so much.
I would expect (iii) to be more like (ii).

The even more interesting and problematic thing is that the same thing happens as mentioned at my original post if you have eg 100k tf-idf sparse features and just one categorical at the same model and if you put an unseen value at the categorical at the test set then SGD ""breaks down"" as in (iii).

In the dataset that I experimented the categorical feature is quite significant (eg dropping it reduced the overall accuracy of the model by 5 percentage points; eg accuracy drops from 80% to 75%) but I would not exclude based on how unstable SGD is that the same thing would happen even if the feature is not that significant.
.........................
@ogrisel I think now it must be quite clear.

As I said, you can pick any dataset on the Internet which has also categoricals or you can even create a very basic one on Excel and I think you will observe what I mean.",False,Impatience
SGDClassifier breaks down when encountering unseen values?,Nivi09,1072818585,9,989208269,0,"Hello @GeorgeZan , for the case 3 as you have mentioned:

If I keep the categorical feature in the model and the test set instance provided has a unknown/unseen value at the categorical feature then the model returns the following prediction:
B: 0.5
A: 0.3
C: 0.2

So, to which class do you expect this example to belong to ? Because I'm confused here when you mentioned - - ' (iii) it returns essentially random predictions.
It does not make sense to me that if you just receive an unseen value your prediction changes so so much.
I would expect (iii) to be more like (ii).' ",False,0
SGDClassifier breaks down when encountering unseen values?,GeorgeZan,1072818585,10,989984238,0,"@Nivi09 ah sorry, just edited my post to add ""Let's also assume that I have a test set instance that belongs to the A class."".
Although it is relative obvious what I mean.

My point is that if you have a known value at the categorical feature or a if you entirely drop the categorical feature then the model returns the right prediction (although with slightly different ""confidence"").

But when you put an unseen value at the categorical feature then the model returns random results although given (ii) essentially the categorical feature does not play much role in the prediction so (iii) should have been returning something more like (ii).",False,Entitlement
SGDClassifier breaks down when encountering unseen values?,glemaitre,1072818585,11,996847749,0,"I don't think that there is anything wrong but I would not be able to be sure until we have an example showing the wrong behaviour and the expected behaviour.

Since `SGDClassifier` is a linear model, when it comes to categorical data, the type of encoding is rather important. So did you check the coefficients of your model? Did you use an `OneHotEncoder` for encoding your categories? Basically, the value of the ""unseen categories"" will have an impact depending on the coefficient of the associated variable.",False,0
SGDClassifier breaks down when encountering unseen values?,GeorgeZan,1072818585,12,998023134,0,"@glemaitre as I mention at my post, I put the categorical value in the text and then apply tf-idf on it.
In this sense you could say that I do one hot encoding on it (?).

I have not really used the categorical variable separately but when I did was with label encoder.
Even though not ideal because ""LabelEncoder can turn [dog,cat,dog,mouse,cat] into [1,2,1,3,2], but then the imposed ordinality means that the average of dog and mouse is cat."", still it should not break down so much with unseen values.

PS
""I don't think that there is anything wrong ""
Wrong or not, the question is why this happens like that with SGD and unseen values.",False,0
SGDClassifier breaks down when encountering unseen values?,glemaitre,1072818585,13,998026500,0,"> Even though not ideal because ""LabelEncoder can turn [dog,cat,dog,mouse,cat] into [1,2,1,3,2], but then the imposed ordinality means that the average of dog and mouse is cat."", still it should not break down so much with unseen values.

This encoding is thus ordinal. Now if your unseen value is mapped to 100 then it will induce 100 x coef on the output and thus it would go sideways.",False,0
SGDClassifier breaks down when encountering unseen values?,GeorgeZan,1072818585,14,998037887,0,"@glemaitre First of all, as I said (not sure if you read carefully) I primarily use the TF-IDF and the problem occurs there.
Any thoughts on these or we entirely skipped my main question haha?

Secondly, not sure why it would go sideways like that ""Now if your unseen value is mapped to 100 then it will induce 100 x coef on the output and thus it would go sideways.""

Let's say that I do not have an unseen value but a seen one and specifically the one mapped to 99 which would induce a 99x coef then why this would not go equally sideways and the 100 would?",False,0
SGDClassifier breaks down when encountering unseen values?,glemaitre,1072818585,15,998064725,0,"> Let's say that I do not have an unseen value but a seen one and specifically the one mapped to 99 which would induce a 99x coef then why this would not go equally sideways and the 100 would?

The thing that goes sideways here would be to use some ordered pattern for a feature that does not have any and where an increment/decrement of the feature value induces an increment/decrement of `coef` on the target. So my example was just to mention that if your mapped unseen values are mapped to larger values then it would even have a larger impact on the target.

> not sure if you read carefully

I think I did but I don't think that you did. As mentioned by @ogrisel or myself, it would be much easier to understand the problem and if there is an issue by having a concrete Python piece of code. Up to now, I am still not sure what is the problem that you are trying to express. You are referring to unseen values. However, an unseen term not part of the `TfidfVectorizer` dictionary will not be used. So more generally because you were referring to categories, I tried to understand what encoding you are actually using because a linear model it will have a different impact on the decision.

So please provide a Python code snippet that we can run and discuss concretely about the issue that you observe.",False,Impatience
SGDClassifier breaks down when encountering unseen values?,GeorgeZan,1072818585,16,998079164,1,"@glemaitre 
""I think I did but I don't think that you did. ""
I have answered this my boy since day 1 so you have not read again:
https://github.com/scikit-learn/scikit-learn/issues/21906#issuecomment-988073948
""I do not have any synthetic data, I use a real dataset that I cannot really share.
But if you pick any dataset from the Internet that has categorical variables or even better text then you can easily test what I describe.""
So do not try to be too smart now.

But anyway if nobody can see what could be the problem without a dataset then I may try to construct something.
 
""So my example was just to mention that if your mapped unseen values are mapped to larger values then it would even have a larger impact on the target.""
yeah but then the unseen values would experience the ""same"" ""jump"" as the 99 class and they do not.
The behaviour with unseen values is totally different and anyway as I said I do not even mostly encode the categorical but use it in the TF-IDF.",False,Mocking
Unable to release video memory,tkp206093,1087972186,1,1087972186,0,"In Final Fantasy VII-Remake Intergrade, if you continue to play, the video memory will increase sharply, and then finally the game will crash.


### Software information
Final Fantasy VII-Remake Intergrade, Material: High Shadow: Low Number of Characters: 8.

### System information
- GPU:3080TI
- Driver:497.29
- Wine version: 
- DXVK version:1.9.2


### Log files
- d3d9.log:N/A
- d3d11.log:
[ff7remake__d3d11.log](https://github.com/doitsujin/dxvk/files/7771234/ff7remake__d3d11.log)

- dxgi.log:
[ff7remake__dxgi.zip](https://github.com/doitsujin/dxvk/files/7771235/ff7remake__dxgi.zip)
",True,0
Unable to release video memory,tkp206093,1087972186,2,1000508718,0,"I follow this mod guide:
https://www.nexusmods.com/finalfantasy7remake/mods/66?tab=posts&BH=11",False,0
Unable to release video memory,doitsujin,1087972186,3,1001600574,0,"What does ""Unable to release video memory"" mean in this context? Is that an error message from the game or something?",False,0
Unable to release video memory,tkp206093,1087972186,4,1001747370,0,"

> What does ""Unable to release video memory"" mean in this context? Is that an error message from the game or something?



> What does ""Unable to release video memory"" mean in this context? Is that an error message from the game or something?

My graphics card has 11GB of video memory.If this module is installed, it will gradually increase during the game. When it exceeds, the game will freeze and the FPS will plummet. At the end of 26GB, the game crashes and automatically shuts down.",False,0
Unable to release video memory,tkp206093,1087972186,5,1001747398,0,"Also https://github.com/doitsujin/dxvk/issues/2412 This not are piracy.
Epic Games\FFVIIRemakeIntergrade\ff7remake.exe will generate log file byte is 0kb
Epic Games\FFVIIRemakeIntergrade\End\Binaries\Win64\ff7remake_.exe will also generate a log file at the same time and it has content
So you can only go to ff7remake_.exe to taket the log file
https://www.nexusmods.com/finalfantasy7remake/mods/66?tab=posts
And share the discussion of this module. I don't know what version other people are using, but everyone said in unison to close the GeForce Experience IN-GAME OVERLAY, otherwise the game will not open smoothly.",False,0
Unable to release video memory,tkp206093,1087972186,6,1001748016,0,"> What does ""Unable to release video memory"" mean in this context? Is that an error message from the game or something?

![image](https://user-images.githubusercontent.com/29872453/147506479-a6768826-bbd8-474e-9bd7-5f1bcdcba37b.png)
I'm sorry that my English is bad and I can't express it well, just like this post",False,0
Unable to release video memory,tkp206093,1087972186,7,1001748822,0,"> Also #2412 This not are piracy. Epic Games\FFVIIRemakeIntergrade\ff7remake.exe will generate log file byte is 0kb Epic Games\FFVIIRemakeIntergrade\End\Binaries\Win64\ff7remake_.exe will also generate a log file at the same time and it has content So you can only go to ff7remake_.exe to taket the log file https://www.nexusmods.com/finalfantasy7remake/mods/66?tab=posts And share the discussion of this module. I don't know what version other people are using, but everyone said in unison to close the GeForce Experience IN-GAME OVERLAY, otherwise the game will not open smoothly.

![image](https://user-images.githubusercontent.com/29872453/147506635-633b6313-d3da-4474-b03a-ef15e985a6c1.png)
",False,0
Unable to release video memory,tkp206093,1087972186,8,1001749546,0,"> > What does ""Unable to release video memory"" mean in this context? Is that an error message from the game or something?
> 
> ![image](https://user-images.githubusercontent.com/29872453/147506479-a6768826-bbd8-474e-9bd7-5f1bcdcba37b.png) I'm sorry that my English is bad and I can't express it well, just like this post

![image](https://user-images.githubusercontent.com/29872453/147506776-e14819bf-8389-4339-a251-ad07c652c95c.png)
",False,0
Unable to release video memory,tkp206093,1087972186,9,1001755495,0,"> Also #2412 This not are piracy. Epic Games\FFVIIRemakeIntergrade\ff7remake.exe will generate log file byte is 0kb Epic Games\FFVIIRemakeIntergrade\End\Binaries\Win64\ff7remake_.exe will also generate a log file at the same time and it has content So you can only go to ff7remake_.exe to taket the log file https://www.nexusmods.com/finalfantasy7remake/mods/66?tab=posts And share the discussion of this module. I don't know what version other people are using, but everyone said in unison to close the GeForce Experience IN-GAME OVERLAY, otherwise the game will not open smoothly.

![image](https://user-images.githubusercontent.com/29872453/147506973-86829385-768f-4d54-978a-1547d05f5cfd.png)
",False,0
Unable to release video memory,tkp206093,1087972186,10,1001760236,0,"> Also #2412 This not are piracy. Epic Games\FFVIIRemakeIntergrade\ff7remake.exe will generate log file byte is 0kb Epic Games\FFVIIRemakeIntergrade\End\Binaries\Win64\ff7remake_.exe will also generate a log file at the same time and it has content So you can only go to ff7remake_.exe to taket the log file https://www.nexusmods.com/finalfantasy7remake/mods/66?tab=posts And share the discussion of this module. I don't know what version other people are using, but everyone said in unison to close the GeForce Experience IN-GAME OVERLAY, otherwise the game will not open smoothly.

![image](https://user-images.githubusercontent.com/29872453/147507166-ea0e70f6-1c45-4186-86e5-e3edaa7b02e3.png)
",False,0
Unable to release video memory,tkp206093,1087972186,11,1001760912,0,"> Also #2412 This not are piracy. Epic Games\FFVIIRemakeIntergrade\ff7remake.exe will generate log file byte is 0kb Epic Games\FFVIIRemakeIntergrade\End\Binaries\Win64\ff7remake_.exe will also generate a log file at the same time and it has content So you can only go to ff7remake_.exe to taket the log file https://www.nexusmods.com/finalfantasy7remake/mods/66?tab=posts And share the discussion of this module. I don't know what version other people are using, but everyone said in unison to close the GeForce Experience IN-GAME OVERLAY, otherwise the game will not open smoothly.

![image](https://user-images.githubusercontent.com/29872453/147507280-b0bbd383-45e9-4828-aa4f-5f7403136c63.png)
",False,0
Unable to release video memory,tkp206093,1087972186,12,1001761337,0,"> Also #2412 This not are piracy. Epic Games\FFVIIRemakeIntergrade\ff7remake.exe will generate log file byte is 0kb Epic Games\FFVIIRemakeIntergrade\End\Binaries\Win64\ff7remake_.exe will also generate a log file at the same time and it has content So you can only go to ff7remake_.exe to taket the log file https://www.nexusmods.com/finalfantasy7remake/mods/66?tab=posts And share the discussion of this module. I don't know what version other people are using, but everyone said in unison to close the GeForce Experience IN-GAME OVERLAY, otherwise the game will not open smoothly.

![image](https://user-images.githubusercontent.com/29872453/147507349-eacfe402-57ec-430c-88e4-74c9029adcce.png)
",False,0
Unable to release video memory,K0bin,1087972186,13,1001768187,0,Can you please not turn this issue into the mirror of an entire NexusMods thread?,False,Bitter frustration
Unable to release video memory,doitsujin,1087972186,14,1002168945,0,"That, and we kind of need an apitrace here. The game is doing some broken shit that *cannot* work properly (creating views with typeless formats and reinterpreting BC1 as BC5 etc) so I kind of need to know what's it's trying to do, and also whether it is trying to use any of the DXGI memory APIs.

DXVK itself does not leak memory but it's possible that the game keeps creating resources for some reason, or uses `MAP_WRITE_DISCARD` on very large resources.

Also, the other thread very much **was** about piracy, I just deleted the comments that blatantly showed it.",False,Vulgarity
Unable to release video memory,Oschowa,1087972186,15,1002245472,0,Here is a trace: https://drive.google.com/file/d/1VlNg9FA8ehjhsqMTcZPde9Sazfu_-M8g/view?usp=sharing,False,0
Unable to release video memory,tkp206093,1087972186,16,1003515374,0,"I see have some some one said
Maybe the memory leak with Vulkan is only on Nvidia cards.",False,0
Unable to release video memory,dajbogyt,1087972186,17,1005685968,1,"I just came from a report post of 2018 stating exactly the same about other game
The AUDACITY of acting like you dont know what he means when your faulty software has been leaking memory since always in multitude of games, god according to the last author post it is even leaking memory adresses‚Ä¶‚Ä¶. And hes like ‚Äúi just wont rewrite it to handle memory like dx11 does‚Äù (aka PROPERLY)‚Ä¶. Why dont you just say you dont care about mid-end cards having rampant issues? Its always one of you two saying ‚Äúyes, its this, its that‚Äù but the thing NEVER gets fixed.",False,Identity attacks/Name-Calling
ËÆ≤‰∏â‰∏™Á¨ëËØù,FooFooDamon,1089763044,1,1089763044,0,ËÆ≤‰∏â‰∏™Á¨ëËØùÔºö‰∏≠ÂõΩÂè§‰ª£Ëî°Ê°ìÂÖ¨ËØ¥Ëá™Â∑±Ê≤°ÁóÖÔºåÂΩì‰ª£Ê¨ßÁæéËØ¥Ëá™Â∑±ÊòØÊ∞ë‰∏ªÁÅØÂ°îÂíåÁ°¨ÂàöÊñ∞ÂÜ†ÔºåÁé∞Âú®ÂèàÊÉäÁé∞gayËØïÂõæÊäägithubÂèògayhubÔºÅ,True,Identity attacks/Name-Calling
ËÆ≤‰∏â‰∏™Á¨ëËØù,AyagawaSeirin,1089763044,2,1002009026,0,So? What's your problem?,False,Impatience
ËÆ≤‰∏â‰∏™Á¨ëËØù,KevinZonda,1089763044,3,1002835434,0,Wats ur problem?,False,Impatience
ËÆ≤‰∏â‰∏™Á¨ëËØù,Cubik65536,1089763044,4,1002842756,0,So? What's ur problem?,False,Impatience
ËÆ≤‰∏â‰∏™Á¨ëËØù,lwd-temp,1089763044,5,1057803790,0,"Ëøô‰∏™‰ªìÂ∫ì‰∏çËøùÂèç‰ªª‰ΩïEULA„ÄÇÂç≥‰ΩøÊÇ®ÊúâÂºÇËÆÆÔºåÂπ≥Âè∞‰πü‰∏ç‰ºöÂπ≤È¢Ñ„ÄÇ
This repository does not violate any EULA. Even if you have an objection, the platform will not intervene.
ËÆ©‰∫∫Á±ªÊñáÊòéÂêëÁùÄÊõ¥Âä†ÂåÖÂÆπÁöÑÊñπÂêëÂèëÂ±ïÔºåÊòØÊØè‰∏™‰∫∫Á±ª‰∏™‰ΩìÁöÑÊÑøÊúõ„ÄÇËøôÊó†ÂÖ≥ÊîøÊ≤ªÂíåÂõΩÂÆ∂ÔºåËÄå‰∏éÊàë‰ª¨ÊØè‰∏™‰∫∫ÊÅØÊÅØÁõ∏ÂÖ≥„ÄÇ
It is the wish of every human being to make human civilization develop in a more inclusive direction. It's not about politics or country, it's about each of us.
ÊàëÂèØ‰ª•ÁêÜËß£ÊÇ®‰πüËÆ∏‰∏çÂ∏åÊúõGitHubÊ∂âÂèäÊâÄË∞ì‚ÄúÊîøÊ≤ª‚ÄùÔºå‰ΩÜËøô‰∏ç‰ºöÊòØÂÆÉË¢´Â∞ÅÈîÅÁöÑÂîØ‰∏ÄÂéüÂõ†„ÄÇ
I can understand that you might not want GitHub to be involved in so-called ""politics"", but that wouldn't be the only reason it was blocked.
ÊúâÂºÇËÆÆÊó∂Â∏∏ÊòØ‰ª∂Â•Ω‰∫ãÔºåÂ∏åÊúõËøô‰ºöÁªôÊÇ®Â∏¶Êù•‰∏ÄÂÆöÁöÑÊÄùËÄÉ„ÄÇ
It's always a good thing to have an objection, and hopefully this will give you some thought.
Á•ùÂ•ΩËøê„ÄÇ
Good luck.",False,0
ËÆ≤‰∏â‰∏™Á¨ëËØù,hunaulys,1089763044,6,1065170308,0,‰Ω†‰ª¨Êúâ‰∏Ä‰∏™ËØØÂå∫ÔºöËÆ§‰∏∫Ëá™Â∑±ÁöÑÊÉ≥Ê≥ï‰∏ÄÂÆöÊòØÊ≠£Á°ÆÁöÑÔºåÁÑ∂ÂêéÂº∫Âä†Áªô‰∏çÊîØÊåÅËøôÁßçÊÉ≥Ê≥ïÁöÑ‰∫∫ÔºÅ,False,0
ËÆ≤‰∏â‰∏™Á¨ëËØù,Cubik65536,1089763044,7,1065266310,0,"> ‰Ω†‰ª¨Êúâ‰∏Ä‰∏™ËØØÂå∫ÔºöËÆ§‰∏∫Ëá™Â∑±ÁöÑÊÉ≥Ê≥ï‰∏ÄÂÆöÊòØÊ≠£Á°ÆÁöÑÔºåÁÑ∂ÂêéÂº∫Âä†Áªô‰∏çÊîØÊåÅËøôÁßçÊÉ≥Ê≥ïÁöÑ‰∫∫ÔºÅ

ËøôÂè•ËØùÈÄÇÁî®‰∫é‰∏§Áßç‰∫∫Ôºö
1. ÊîØÊåÅLGBT+‰∏îÂº∫Âà∂Ë¶ÅÊ±ÇÊâÄÊúâ‰∫∫ÊîØÊåÅLGBT+ÁöÑ‰∫∫
2. ‰∏çÊîØÊåÅLGBT+‰∏îÂº∫Âà∂Ë¶ÅÊ±ÇÊâÄÊúâ‰∫∫‰∏çÊîØÊåÅLGBT+ÔºåÁîöËá≥ËßâÂæóÊîØÊåÅ/‰Ωú‰∏∫LGBT+Â∞±ËØ•Ê∂àÂ§±ÁöÑ‰∫∫

Êú¨ org Ê≤°ÊúâÂº∫Âà∂‰ªª‰Ωï‰∫∫ÊîØÊåÅ LGBTQ+Ôºå‰ªª‰Ωï‰∫∫ÂèØ‰ª•ÈÄâÊã©Âú®Ê≠§ repo ÁΩ≤ÂêçË°®ËææÊîØÊåÅÔºå‰ªª‰Ωï‰∫∫‰πüÂèØ‰ª•ÈÄâÊã©Áõ¥Êé•ÂøΩÁï•„ÄÇÂêåÊ†∑ÁöÑÔºåÊàëÊÉ≥‰∏Ä‰∏™‰∫∫Ëµ∞Âà∞‰∏Ä‰∏™Âõ¢‰Ωì‰∏≠ÁÑ∂ÂêéÂ§ßÂñäËøôÂ∞±ÊòØ‰∏™Á¨ëËØùÁÑ∂ÂêéË¢´ËÆ®ÂéåÊòØÊ≠£Â∏∏ÁöÑ„ÄÇÁé∞Âú®ÔºåËøô‰∏™ issue ÈáåÔºåÊòØË∞ÅÂú®„ÄäËÆ§‰∏∫Ëá™Â∑±ÁöÑÊÉ≥Ê≥ï‰∏ÄÂÆöÊòØÊ≠£Á°ÆÁöÑÔºåÁÑ∂ÂêéÂº∫Âä†Áªô‰∏çÊîØÊåÅËøôÁßçÊÉ≥Ê≥ïÁöÑ‰∫∫„ÄãÔºü",False,0
ËÆ≤‰∏â‰∏™Á¨ëËØù,AyagawaSeirin,1089763044,8,1065578700,1,"> ‰Ω†‰ª¨Êúâ‰∏Ä‰∏™ËØØÂå∫ÔºöËÆ§‰∏∫Ëá™Â∑±ÁöÑÊÉ≥Ê≥ï‰∏ÄÂÆöÊòØÊ≠£Á°ÆÁöÑÔºåÁÑ∂ÂêéÂº∫Âä†Áªô‰∏çÊîØÊåÅËøôÁßçÊÉ≥Ê≥ïÁöÑ‰∫∫ÔºÅ

ÂÖ∏‰∏≠ÂÖ∏‰πãÂè™Ë¶ÅÊàë‚Äú‰∏çÂÖ¨ÂºÄÊ≠ßËßÜ‚ÄùÂ∞±ÊòØÂØπTA‰ª¨ÊúÄÂ§ßÁöÑ‰ªÅÊÖàÔºåÂêåÊó∂Ë¶ÅÊ±Ç‚ÄúÂ∞äÈáçÊàëÂØπLGBTÁöÑÈöêÂΩ¢Ê≠ßËßÜ‚Äù",False,Identity attacks/Name-Calling
[Feature Request] Make it more portable,stla,1096297382,1,1096297382,0,"Hello,

  I wanted to upgrade **ScreenToGif** (my favorite GIF recorder) but now it requires **NET**. I don't have admin rights on my laptop so I'm using **ScreenToGif** portable, but **NET** requires admin rights. That is sad.",True,0
[Feature Request] Make it more portable,NickeManarin,1096297382,2,1007391532,0,"Hi, ScreenToGif was never truly portable.
Older versions required .NET Framework 4.8, which was installed by default.

In order for it to be truly portable, the download size will go up to 78MB.
I could add that download option in future releases.",False,0
[Feature Request] Make it more portable,SoftwUser,1096297382,3,1007916684,0,"@NickeManarin 

That would be cool. I don't care much about portability - but bundling ScreenToGif with the needed .NET6 files insetad of requiring a systemwide .NET6 install would be very welcome. Other popular software (such as paint.net) already go this way.",False,0
[Feature Request] Make it more portable,NikolajPagh,1096297382,4,1007922830,0,"I am here to second that motion. Was quite surprised to see that I needed to install some additional .net software in order to use ScreenToGif. This is soo sad.
From a developer point of view, WHY do you need the newer .Net framework - What part of it, is it that you need ?",False,Impatience
[Feature Request] Make it more portable,NickeManarin,1096297382,5,1008858550,1,"Right. 

Let's use the right terms here, it's not sad, it's just inconvenient for you. 
There was always the requirement to have installed .NET Framework, you just already had it installed.
I can make it more convenient, but the file size will go to 140MB (78MB compressed) and the update system won't work the same way that it works now.

Also, I don't need to explain myself as to why I did something with my project (but it's easy to guess why or easy to search for .NET Framework to .NET 6 comparisons online).
",False,Entitlement
IRestClient interface,alexeyzimarev,1100051808,1,1100051808,0,"Version 107 got several interfaces removed, which is one of the major breaking changes in that version.

It seems most of the interfaces were okay to remove, except `IRestClient`. My arguments are [described](https://restsharp.dev/v107/#motivation) in the docs.

This issue is a place where we should have a civilised discussion about it. I will not follow up on the interface issue anywhere else, except here.

Some details about the current state of the `RestClient` API signature.
- All the sync methods were removed. Those who need it can make extensions, wrapping up async calls in some [async helper](https://github.com/rebus-org/Rebus/blob/master/Rebus/Bus/Advanced/AsyncHelpers.cs).
- Most of the overloads for making requests are now extensions, as they just call each other, so they would never be part of any interface anyway.
- Most of the options that were previously on `IRestClient` and `IRestRequest` are now in `RestClientOptions`, which is a property bag and, therefore, won't have any interface

So, what's there in stock for `IRestClient`? Basically, it boils down to this signature:

```csharp
namespace RestSharp; 

public interface IRestClient {
    RestClient AddDefaultParameter(Parameter parameter);

    Task<RestResponse> ExecuteAsync(RestRequest request, CancellationToken cancellationToken = default);
}

```

The question now is if it makes sense to introduce an interface with those two functions? Please comment.",True,0
IRestClient interface,astrohart,1100051808,2,1012383729,0,"I would think it would, in order to maintain SOLID code.  Also, as several other people pointed out previously, we are not always immune to managers etc. making us code to interfaces, do gratuitous TDD and automated-unit-test writing etc/mocking, and/or use IoC containers.  Bosses like to make software engineers do things ""because I heard about it at a conference,"" or ""the insurance is making us,"" etc.

The interface just supporting the two methods is, in principle, okay -- just make sure that the static extension methods refer to the object through the interface and not the actual class.",False,0
IRestClient interface,alexeyzimarev,1100051808,3,1012444622,0,"Brian, I understand the argument about managers, although in my 30 years of engineering I never encountered a single manager who was telling me how to write code... I mean, if you write your code against an interface it's one thing. But how some managers can tell a third-party library how to write _their_ code? `HttpClient` doesn't have any interfaces implemented and it seems to be fine, no?

Automated testing and mocking HTTP requests are totally possible with RestSharp in its current shape. I have shown an option in the docs.

I see no issues with using DI containers as well, as `HttpClient` does with its nice extensions. In fact, I plan to add some DI support to make it easier and push people to use RestSharp properly (typed clients, etc).

The SOLID argument is not solid :) What do you mean by that, could you elaborate?

Then again, I am seriously not against adding that small interface. However, I would not be happy to add `IRestRequest` and `IRestResponse` back as those are glorified property bags.",False,0
IRestClient interface,fjmorel,1100051808,4,1015063559,0,"A typical method in a wrapper service for my HTTP calls does the following
1. Take some inputs to form a request
2. Execute
3. Handle response (maybe retry, return data)

If I can mock `IRestClient`, then I can verify the contents of the `RestRequest` given to it by my method. That lets me avoid splitting 1 into a separate method just for testing that I correctly crafted the web request (which I want because string formatting is tricky).

If I can mock `IRestClient`, then I can setup responses for 200, 404, 401, etc and don't need to split out 3 either.

Each method is already short enough that I _don't want_ to split it up just for test coverage. Example:

```csharp
public async Task<List<Something>> GetData(DateTimeOffset startEpoch, DateTimeOffset endEpoch)
{
    var request = new RestRequest(""/url"", Method.GET);
    await AddBearerToken(request);// other method which might make a request to authenticate or use cached token
    request.AddQueryParameter(""start"", startEpoch.ToUnixTimeSeconds().ToString());
    request.AddQueryParameter(""end"", endEpoch.ToUnixTimeSeconds().ToString());

    var result = await _client.ExecuteAsync<ApiResponseModel>(request);
    if (result.StatusCode != HttpStatusCode.OK)
        throw new Exception($""Failed to fetch data. Status code: \""{(int)result.StatusCode}\"", message: \""{result.StatusDescription}\"""");

    return result.Data.ThingICareAbout;
}
```",False,0
IRestClient interface,alexeyzimarev,1100051808,5,1015173037,0,"@fjmorel what of that is not possible to do with [MockHttp](https://github.com/richardszalay/mockhttp)? It gives you even better options to ensure the request comes in the correct format and has the correct content type, authorisation headers, etc.",False,0
IRestClient interface,danielwagn3r,1100051808,6,1015292847,0,"
> I see no issues with using DI containers as well, as `HttpClient` does with its nice extensions. In fact, I plan to add some DI support to make it easier and push people to use RestSharp properly (typed clients, etc).

Can you tell a little bit more about your plans on DI support? Perhaps a word on DI in the docs could clarify many questions before the've to be asked.",False,0
IRestClient interface,alexeyzimarev,1100051808,7,1015313118,0,"The DI registration heavily depends if you want to follow the same pattern as Microsoft does, using `IHttpClientFactory`, registering the client as a transient dependency.

For many use cases it's enough to register a typed client as a singleton:

```csharp
services.AddSingleton<IMyApiClient>(new MyApiClient(new RestClient(options)));
```

If I'd to provide some use of `IHttpClientFactory` registration extensions, I'd need to know if people want to configure the `HttpClient` instance used by `RestClient` on the outside. In this case, lots of options that RestSharp uses to configure the message handler won't be applicable.",False,0
IRestClient interface,fjmorel,1100051808,8,1015671496,0,"> @fjmorel what of that is not possible to do with [MockHttp](https://github.com/richardszalay/mockhttp)? It gives you even better options to ensure the request comes in the correct format and has the correct content type, authorisation headers, etc.

Thanks! I didn't know about that library. I've been using [Moq.Contrib.HttpClient](https://github.com/maxkagamine/Moq.Contrib.HttpClient) in places we used HttpClient rather than RestSharp.

This library using HttpClient is an implementation detail that I don't think library users should have to think about, unless they have specific requirements making the need for a custom HttpClient necessary. Having the interface makes testing easier for all users without having to add extra dependencies.",False,0
IRestClient interface,alexeyzimarev,1100051808,9,1015729443,0,"The library you mentioned would work as well. As long as you can add a delegating handler, it will be possible to test RestSharp calls. All you need to do is to compose or override the message handler used by the wrapped HttpClient:

```csharp
var client = new RestClient(...) { ConfigureMessageHandler = _ => mockedHandler };
```

Basically, one of the benefits of moving to HttpClient (besides that it's obvious and `HttpWebRequest` is heavy legacy) is the ability to use delegating handlers and build message processing pipelines.

The point about testing real requests is obvious to me. If you look at StackOverflow questions about RestSharp, most of the issues are caused by incorrectly added parameters, content type overrides and other weird things. But, if people would test against a RestShar interface, they would have all their tests passing. But it won't work anyway, because the request is just not correctly formed. It's a great benefit to at least inspect the actual request and ensure that it looks exactly as it should, rather than inspecting the parameters collection on a `RestRequest` instance. The latter gives you zero idea how the actual request would look like.",False,0
IRestClient interface,sspates,1100051808,10,1021443552,0,"This is not necessary if the existing RestClient methods are marked virtual.  This will allow for mocking overrides of the public methods instead of needing a full interface. 

One of the two is necessary as the handler system is not able to simulate invalid http responses.  We switched from Http client to RestSharp because RestSharp had an interface that allowed us to simulate the result of those responses as well as other error conditions not simulatable through the mock handler.   Some of these result in StatusCode 0 or exceptions being thrown which require an interface or virtual method. ",False,0
IRestClient interface,alexeyzimarev,1100051808,11,1021542279,0,@sspates-starbucks why can't you simulate the error response with a testing handler like `MockHttp`? It provides a behaviour that is much closer to reality. It can return anything you want based on its configuration.,False,0
IRestClient interface,alexeyzimarev,1100051808,12,1021543020,0,@maor-rosenfeld thanks for your extensive and elaborative feedback for this issue.,False,0
IRestClient interface,alexeyzimarev,1100051808,13,1021563475,0,"If we keep discussing testing, maybe it's a good idea to post some test samples and check how these tests can be refactored. So far, the discussion goes quite theoretical imo.",False,0
IRestClient interface,maor-rozenfeld,1100051808,14,1024188857,0,"@alexeyzimarev we can argue all day about use cases of using the interface for tests, extension methods, tests on these extension methods and other reasons of why suddenly removing all interfaces from an existing infrastructure package is a bad idea, but at this point it's clear that you're here to tell us how to write code. 

I'm sorry if I choose not to align with your views, but at this point upgrading to the latest version of RestSharp is so painful that there's really no justification to even try. 

On a positive note, I'm completely with you regarding the separation of services and settings objects. I like `RestClientOptions`.",False,0
IRestClient interface,alexeyzimarev,1100051808,15,1024358874,0,@maor-rosenfeld you definitely know how to motivate OSS maintainers to keep doing their community work for free.,False,Mocking
IRestClient interface,lukos,1100051808,16,1025970935,0,"I've come here from a breaking scenario that I'm not sure I can fix other than backing out the upgrade to 107 due to missing IRestClient.

I updated one of my libraries to use 107. Why not right? I'm trying to be proactive with keeping libs up to date. It all seemed to work OK until some tests starting failing. This is because we use a third-party library and they reference IRestClient and v106 of the library. I could ask them to update but I don't see that they would prioritise that and it would then force all of their customers to update to v107 as well which might be a big deal. If I use an assembly redirect to 107, it won't work because it won't find IRestClient and if I use 106, my code won't work because the method signatures changed.

I'm not sure what I would have done differently but perhaps something as simple a a different nuget package with different namespaces and mark the old one as deprecated. Then we can run them side-by-side until everyone is on 107?

Not sure. Any suggestions?",False,0
IRestClient interface,alexeyzimarev,1100051808,17,1026613361,0,"@lukos I don't think it would work in any scenario, interfaces or not. RestSharp v107 internals are completely different due to migration to `HttpClient` and ditching `HttpWebRequest`. It made it impossible to keep the API backwards compatible, and even with interfaces, the API scope of those interfaces won't ever be compatible. Lots of properties of `RestClient` and `RestRequest` are moved to `RestClientOptions` and there's no way to keep them where those properties were before.",False,0
IRestClient interface,sspates,1100051808,18,1027159406,0,"We are running into cases now where one library requires the interface and another requires the v107 structure, we can't upgrade either one due to the lack of backwards compatibility with IRestClient.   Adding a compatibility wrapper that implements the interfaces might be a way to handle this better rather than implementing the interface on the current client. 
",False,0
IRestClient interface,alexeyzimarev,1100051808,19,1027240464,0,"@sspates-starbucks Even if there's something to make things build-time compatible, these won't be compatible at runtime anyway. As I mentioned, most of the properties of those (`IRestClient` and `IRestRequest`) interfaces were (by necessity) moved to `RestClientOptions`, and it's something that simply cannot be undone due to the configuration of `HttpMessageHandler`.

I can only suggest opening issues for those libraries that still use RestSharp 106 and helping them with the migration. It's not a lot of work honestly.",False,0
IRestClient interface,Terebi42,1100051808,20,1030191734,0,"The new class having internal only options generates problems for dependency injection. I set up my DI container to hand out RestClient, but different consumers need different options. There is now now way to set the options after having an instantiated Client. ",False,0
IRestClient interface,alexeyzimarev,1100051808,21,1030572715,0,"@Terebi42 that's why I advocate wrapping `RestClient` in a particular API client, so you can wire it like this:

```csharp
services.AddSingleton<ITwitterClient>(new TwitterClient(new RestClient(twitterOptions)));
```

But I plan to address it better, just haven't found a good way to do it yet. Either I will make some extensions for `IHttpClientFactory` or make a new factory with named registrations for options.",False,0
IRestClient interface,Terebi42,1100051808,22,1033915256,0,"> @Terebi42 that's why I advocate wrapping `RestClient` in a particular API client, so you can wire it like this:
> 
> ```cs
> services.AddSingleton<ITwitterClient>(new TwitterClient(new RestClient(twitterOptions)));
> ```
> 
> But I plan to address it better, just haven't found a good way to do it yet. Either I will make some extensions for `IHttpClientFactory` or make a new factory with named registrations for options.

Is there a reason why (some) options can't be set after the constructor? Its a HUGE code change to say go make a new interface and options class for every location that is going to use restsharp, vs just setting the timeout and baseurl after the DI injection is done",False,0
IRestClient interface,alexsaare,1100051808,23,1042736756,0,"Every man and his dog has an opinion on software design/standards and the debates will rage on forever - they're usually just different mindsets/approaches.  Those mindsets aren't even usually the mindset of the developer using your library, they're probably dictated by the organisation they work for and having a conversation to change those standards is either way above their pay grade or a battle that's not worth fighting.  

The problem here isn't one of who is right or wrong, it's one of practicality.  The removal of the interfaces causes the amount of work some consumers have to do to significantly increase.  Most devs really just need to deliver the work they had promised within the timeframe promised.   I'm a huge fan of MockHttp but in my case, updating over 1000 tests to use it (which were written 5 years ago) is a huge amount of work.  No matter how you plan your pipeline, the sizing of the work item is now completely invalid.  The amount of work required to update your library has become inhibitive to doing so - it will either have to be picked up as a separate work item or it just won't ever get updated.

Irrespective of whether consumers are testing the right thing, using IoC correctly or just plain writing crap code, they no longer have the flexibility to choose like they used to.  I think most devs would agree that the code changes make sense but at the end of the day we all need to deliver functionality to a business and I guarantee that those business users really don't care how well the code is written as long as it reliably does what they need. 

The best development tools out there make our lives easier.  That's why we use them - they abstract us away from the detail/hard work we really don't need to care about and they provide flexibility to customise how we utilise them.  They help us do more, faster.  Removing these interfaces does the opposite of that, irrespective of whether it's ""better"" code or not.

",False,Mocking
IRestClient interface,lukos,1100051808,24,1042824033,0,"What everyone is saying is correct. Of course, we appreciate Alexey for making this library but the changes are very breaking.

It seems like the best way out of it would be to create a separate library with a different name/package name (and importantly, different namespaces) to contain your newer/better way of doing things and which should be used by people going forwards. Any serious bugs found in the old library will probably be fixed but otherwise it won't get any new features.

This way, we can have both packages alongside each other so if we can update our own code for the new library we can do that without forcing all of our dependencies to update all of their code at the same time. If they eventually do, then we can eventually uninstal the old package.

Not sure if you think that sounds fair or not?",False,0
IRestClient interface,alexeyzimarev,1100051808,25,1042863329,0,"The question is why is there such a strong need to migrate to v107 if v106 works fine for most? Migration to `HttpClient` is a big change in itself, the `IRestRequest` (for example) is impossible to shape to the same form as it was before anyway, as well as `IRestClient` interface API surface cannot be recovered.",False,0
IRestClient interface,alexsaare,1100051808,26,1043060686,0,"Why update?  People want the [stated benefits](https://restsharp.dev/v107/#presumably-solved-issues) that this update (_and future updates_) will bring.  

As a consumer, I shouldn't have to care what implementations are used internally as long as it works.   This is a debate about the impact of the removal of interfaces and the consequences that has for those who use the library.  (The fact that I'm even aware of what components are used internally screams leaky abstraction to me but that's just my opinion and somewhat off topic)

Changing the method signatures is something that's not overly complex to deal with (the compiler will help identify what needs changing and, in our case, this library is abstracted away from our code anyway) however, removing all the interfaces entirely fundamentally changes how to set up and work with the library and how easy/difficult that is given what's gone before. 

The work you've done is great - we're just providing feedback of the real world issues it causing us.  Rightly or wrongly, people consume this library is a myriad of ways and for us, the migration path is so large that it's currently a barrier to entry.  ",False,Bitter frustration
IRestClient interface,alexeyzimarev,1100051808,27,1043079276,0,"I don't know why, but my message is not getting through. I will try again:
- The API surface of `IRestClient` and `IRestRequest` is impossible to hold intact after the migration
- Removal of interfaces made the maintenance work much easier.",False,0
IRestClient interface,alexsaare,1100051808,28,1043138448,0,"*** EDITED ***

I really don't want to get bogged down in specifics as I don't think it's helpful

> The question now is if it makes sense to introduce an interface with those two functions?

For us, yes it does.   

There are probably many others who will find that interface useful too for many different reasons.  

Adding them in provides flexibility to choose and reduces the impact of the changes for many people especially given that this library has been around for such a long time. This isn't about what code/approach is better, it's about what's practical and helpful for existing consumers. ",False,0
IRestClient interface,alexeyzimarev,1100051808,29,1043304511,0,"Thanks very helpful @alexsaare, thank you.

The scope of refactoring, however, would be significant, both for me (all the extensions need to change) and for those who use the previous version of the library.

I am not against it, that's why I opened this issue. I still, though, wait for some sample test code, so I can understand the need better.",False,0
IRestClient interface,alexeyzimarev,1100051808,30,1044107470,0,"As I mentioned before, I am trying to understand how people use interfaces in tests. Tests aren't PI, nor IP. Please, share your tests.",False,0
IRestClient interface,Terebi42,1100051808,31,1044881153,0,"To answer some questions @alexeyzimarev posed above : 

1) Why upgrade if we don't need to? Because corporate policy requires us to be on recent versions
2) Though I personally agree with some of your motivations and reasoning regarding testing and interfaces, corporate policy also often doesn't care if a test provides value or not

But my biggest problem is the change to DI because of options. I have an app that connects to 10 different services. Currently I have them all getting an IRestClient via DI, set the options they need in their consumer's constructors, and move on.

You are asking me to create 10 new interfaces, move all that rest configuration logic into the DI configuration, change the signature of every consumer of the rest client etc.  Further, if two services can currently share configuration they can share the injected interface. But if one of them needs a different bit of configuration in the future, now I have to create a new interface for that DI again, and change the signatures again, just to have a different timeout or base url.  


I can inject a vanilla RestClient instead of IRestClient just fine. But not being able to set options on it after I get it, is a deal breaker. ",False,Bitter frustration
IRestClient interface,alexeyzimarev,1100051808,32,1046055925,0,"@Terebi42 the DI concern should be fixed differently. I will provide a way to do it at some point, similar to how you'd do it with `HttpClient`. The issue with registering a single dependency as `IRestClient` as it won't work anyway if the client options need to be configured. As many options moved from the request to the client itself, it won't work.",False,0
IRestClient interface,alexeyzimarev,1100051808,33,1046056153,0,"@Terebi42 as per the latest version requirement, I don't think it is even feasible, although I am not aware of your particular case. Say, you have .NET 6 released in November, does the policy mean you have to upgrade all your applications to .NET 6, with all the breaking changes fixed? I never had such experience in any company.",False,Bitter frustration
IRestClient interface,Terebi42,1100051808,34,1067126513,0,"> @Terebi42 the DI concern should be fixed differently. I will provide a way to do it at some point, similar to how you'd do it with `HttpClient`. The issue with registering a single dependency as `IRestClient` as it won't work anyway if the client options need to be configured. As many options moved from the request to the client itself, it won't work.

Why were the options moved from the request to the client? It was very convenient to be able to set those parameters differently per request. And if they do need to be on the client, why are they now read only? Being able to set them at the time of call would also be very convenient. 

A change like this is literally asking for dozens of new classes to be created to support the new config paradigm and DI.  
If we could even pass in a different options class per request, that would be great.  ",False,0
IRestClient interface,alexeyzimarev,1100051808,35,1067152936,0,"Because those options are configuring `HttpMessageHandler`, which is wrapped inside `RestClient`. Changing those options would require creating a new `HttpMessageHandler` instance, and it will make `RestClient` not thread-safe.",False,0
IRestClient interface,Terebi42,1100051808,36,1067192425,0,"> Because those options are configuring `HttpMessageHandler`, which is wrapped inside `RestClient`. Changing those options would require creating a new `HttpMessageHandler` instance, and it will make `RestClient` not thread-safe.

hrm, well, at least I understand.  Perhaps Ill make a restclient factory that I DI, and pass it options to get the old pattern back
",False,0
IRestClient interface,alexeyzimarev,1100051808,37,1067722580,0,@Terebi42 I opened an issue for that https://github.com/restsharp/RestSharp/issues/1791,False,0
IRestClient interface,kendallb,1100051808,38,1068070749,0,"> * All the sync methods were removed. Those who need it can make extensions, wrapping up async calls with `.GetAwaiter().GetResult()`

Can you update this ticket to remove this bit? .GetAwaiter().GetResult() is not the correct solution and will lead to deadlocks. You need to use an async helper like the one in Rebus for this to work correctly in sync code:

https://github.com/rebus-org/Rebus/blob/master/Rebus/Bus/Advanced/AsyncHelpers.cs",False,0
IRestClient interface,kendallb,1100051808,39,1068077334,0,"BTW, if you did bring back IRestClient, the only function that matters is all variations of ExecuteAsync(). There are good valid reasons to want to mock ExecuteAsync() to avoid an actual call as its high level enough to mock away a significant amount of lower level stuff. But it's also low level enough that I usually don't bother ;). Usually we mock at the level above that, which is our REST API client contract. After all the point of mocking is to be able to stub out lower level stuff as a black box and assume that black box works correctly. That assumption works as then you expect something else to validate that the actual client itself is properly tested (integration test etc).

And there is a good argument to be had that writing tests to ensure the client itself is actually working correctly is much better being done by mocking HttpClient as suggested. Something that was almost impossible to do with earlier versions of RestSharp, but is now much easier to do now it relies on HttpClient internally so you can swap it out for mocking purposes.",False,0
IRestClient interface,alexeyzimarev,1100051808,40,1068080413,0,"> the only function that matters is all variations of ExecuteAsync().

The point here is that there's only one :) All other overloads are extensions. So, if the interface is back, all the extensions need to be on `this IRestClient` instead.

> .GetAwaiter().GetResult() is not the correct solution and will lead to deadlocks

I found this via SO: https://github.com/aspnet/AspNetIdentity/blob/main/src/Microsoft.AspNet.Identity.Core/AsyncHelper.cs",False,0
IRestClient interface,kendallb,1100051808,41,1068125687,0,"I believe that works because it uses a separate task, so it's a similar pattern achieved in a different way to how Rebus did it. I am not sure which one is more efficient. We use Rebus in our code and I stole their code to solve the deadlock issues when we had it, and it worked. I have also solved in the past simply by running the code in a separate background thread similar to the AspNet approach and that works also. But I think it's higher overhead than the approach taken by Rebus and other libraries?

I am sure either approach works, but I know that just using GetAwaiter().GetResult() on an async function will cause deadlocks in web apps (or Windows Forms apps). We had tons of hung web requests when I was doing it without the task wrapper. ",False,0
IRestClient interface,alexeyzimarev,1100051808,42,1068132434,0,Updated,False,0
IRestClient interface,kendallb,1100051808,43,1068132875,0,"Been a while since I looked and and debugged the Rebus version, but I did step through it with a debugger to understand it back then. I am pretty sure it's much lower overhead as it never triggers a separate background thread, so puts no extra pressure on the thread pool. Rather it swaps out the SynchronizationContext so that when the task returns, it comes back on something other than the original sync context which is where you get the deadlocks. ",False,0
IRestClient interface,alexeyzimarev,1100051808,44,1068145814,0,"I don't think there's an explicit new thread when using the task factory. I think it will use the available IO thread from the pool. Synchronization context, on the other hand, produces some overhead. I believe that doing something like `.ConfigureAwait(false).GetAwaiter().GetResult()` would work, but not in WinForms (for example). I also know that ASP.NET Core behaves differently compared with .NET Framework when it comes to `GetAwaiter().GetResult()`",False,0
IRestClient interface,kendallb,1100051808,45,1068206934,0,"Yes, its possible it would work differently in ASP.NET Core. Porting our code to that is a massive undertaking which is still a work in progress :( But we also use RestSharp in WinForms apps (EasyPost and ShipEngine) so the AsyncHelper approach is what has worked for us.

I guess the only way to tell would be to benchmark the two approaches and see which is faster.",False,0
IRestClient interface,stale[bot],1100051808,46,1100677637,0,"This issue has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs. Thank you for your contributions.
",False,0
IRestClient interface,repo-ranger[bot],1100051808,47,1100677761,0,‚ö†Ô∏è This issue has been marked wontfix and will be closed in 3 days,False,0
IRestClient interface,kewur,1100051808,48,1108912424,0,"From the explanation page;

> The best way to test HTTP calls is to make some, using the actual service you call. However, you might still want to check if your API client forms requests in a certain way. 

I have to disagree with this. I don't think calling actual services from unit tests is good practice. At that point they're functional tests. 

next line goes into that;

> As RestSharp uses HttpClient internally, it certainly uses HttpMessageHandler. Features like delegating handlers allow you to intersect the request pipeline, inspect the request, and substitute the response. You can do it yourself, or use a library like [MockHttp](https://github.com/richardszalay/mockhttp)

this requires me to actually know how you're making your calls inside your library that you publish to nuget. The main reason people wants to use external libraries is to not be concerned about the internals of that external code. It also assumes I know to use this MockHttp library (had no idea it existed, I didnt need to since I've used RestSharp to handle all ""HttpClient"" related things so far), with the assumption that, that package will also be maintained properly and will not be making breaking changes.

> Mocking an infrastructure component like RestSharp (or HttpClient) is not the best idea.

This statement is also wrong in my opinion. I think the opposite actually, making us test external library code in our _unit_ tests implicitly makes them not unit tests, because we are always testing more than a small ""unit"" of code, in addition to our code change, now we're testing your code.

> As RestSharp uses HttpClient internally

What happens when you decide to not use HttpClient, but use ""MyGrandmasHttpClient"". Do we then have to refactor all of our code bases with the ""MockMyGrandmasHttpClient"" package?

I am fine with removing every other interface on this package. Except for that single one.

This wouldn't be an issue if we were able to mock classes like Java allows, however removing this 1 single interface with 1 single method causes design issues, as well as wasted hours for millions of developers. 

I would also be skeptical about the suggested ""if you don't like 107, just remain on 106"" By it's nature, RestSharp deals with connectivity, that increases the chances of 0 day vulnerabilities or other security issues. We won't be able remain on 106 forever, and would be forced off of this package. Which would be a shame because we love RestSharp.

I really appreciate your work on maintaining this package, there is a reason it's one of the most popular packages on .NET. I hope you reconsider this decision.",False,Impatience
IRestClient interface,alexeyzimarev,1100051808,49,1114185627,1,"> What happens when you decide to not use HttpClient, but use ""MyGrandmasHttpClient"". Do we then have to refactor all of our code bases with the ""MockMyGrandmasHttpClient"" package?

That's an exaggeration. RestSharp uses elements of .NET under the hood, and for many years it was `WebRequest`. Then, Microsoft decided to scrap it and re-implemented it using `HttpClient`, causing massive problems. Essentially, RestSharp uses the only way available in core .NET to make HTTP calls, and it's `HttpClient` until Microsoft decides to do something else. All other .NET libs for making HTTP calls like Refit or Flur also use `HttpClient`, and most people use `HttpClient` directly. So, when _that_ changes, all the code for .NET will need to change, which is very unlikely.

> This statement is also wrong in my opinion.

The issue here is that by creating an interface mock you will assert if the client is formed according to your idea of how it should be formed. However, it doesn't guarantee that the actual HTTP request will be valid. That's why `HttpClient` has no interface and cannot be mocked. And that's why people normally use a test version of `HttpMessageHandler` instead, as `HttpClient` itself is just forming a message, but the actual _oevr the wire_ implementation is in the handler. See this [SO question](https://stackoverflow.com/questions/36425008/mocking-httpclient-in-unit-tests).

> I have to disagree with this. I don't think calling actual services from unit tests is good practice.

You don't have to call the actual service, you can call a simulation. That's what many RestSharp tests do for making sure that the server can understand the request in the way it should.

Again, I am not against bringing the interface back, but it won't be the original interface. Essentially, there are just a handful of core functions of the client itself (`ExecuteAsync` is the main one), all the others are just extensions.

I actually think that the interface would help for building something like a retryable client with Polly, as it could be done in a wrapper using composition.",False,Impatience
Ensure utility classes cannot be instantiated.,scruel,1102918184,1,1102918184,0,"According to pull request #1848, utility classes are noninstantiable, but abstract is not enough to prevent from creating objects by subclasses.
This change modified almost all utility classes, but I do think it is necessary, and the effects should small and limited.",True,0
Ensure utility classes cannot be instantiated.,snicoll,1102918184,2,1012913142,0,@scruel thanks for the PR but we'd rather keep things as they are.,False,Irony
Ensure utility classes cannot be instantiated.,scruel,1102918184,3,1012917357,0,"> @scruel thanks for the PR but we'd rather keep things as they are.

Fine‚Ä¶ but it must be a bad decision, and could you explain based on what reason so that you will not change this?",False,Insulting
Ensure utility classes cannot be instantiated.,snicoll,1102918184,4,1012921743,0,`abstract` is a fine enough signal for us. Yes you can instantiate public utils classes still but I don't think this is worth the extra noise.,False,Bitter frustration
Ensure utility classes cannot be instantiated.,scruel,1102918184,5,1013000075,1,"@snicoll Fine enough signal for you is not acceptable, could you consider that spring is using by so many people all around the world, no one can make sure that others will not treat `abstract` as a keyword as it is: you should extend this and create an object?
By the way, the ""extra noise"" that you mentioned are already exists in this project, you could find some utils classes which have the private constructor(most of them does not have), and I do think `abstract` should be treated as the ""noise"", because we should not use it for preventing to instantiate the object.
Also, I recommend you read *Effective Java, item 4*.
Thanks.",False,Insulting
Issue/trim error,kpverint,1107247166,1,1107247166,0,PHP_CodeSniffer returns null which results in errors.,True,0
Issue/trim error,jrfnl,1107247166,2,1015741097,0,"@kpverint Thank you for trying to make WPCS better.

I think I see what you are trying to do here. Let's first define the problem you are trying to solve:

>  When running WPCS on PHP 8.1, you are seeing deprecation notices for passing `null` as a parameter to a function where that parameter is not nullable.

Correct ?

If you look at the _current_ state of the `develop` branch, you will see that those issues have already been fixed in #1984 and #1985.

It looks like you didn't update your dev clone of WPCS before creating this PR, which also explains the file conflicts which GH shows at the bottom of this page.

Suggest: close.",False,0
Issue/trim error,kpverint,1107247166,3,1015780313,0,That's right. Thanks! :) Closing.,False,0
Issue/trim error,bilalmalkoc,1107247166,4,1044386344,0,"I did fresh install by following here: https://github.com/WordPress/WordPress-Coding-Standards#standalone
But it is showing same error. Where I'm missing?",False,Bitter frustration
Issue/trim error,jrfnl,1107247166,5,1044407487,0,"@bilalmalkoc At this moment, the last release of WPCS is compatible with PHP 5.4 -7.4. Support for PHP 8.0/8.1 will be in the next release (currently in the `develop` branch).",False,0
Issue/trim error,WraithKenny,1107247166,6,1064395144,0,Any news when you will actually publish the fixes?,False,Impatience
Issue/trim error,jrfnl,1107247166,7,1064402613,0,When the release is ready.,False,Impatience
Issue/trim error,WraithKenny,1107247166,8,1064492396,1,"Cool, I'll just keep downgrading my computer's PHP version since 11 months wasn't enough time.",False,Irony
"[qBitTorrent] When using OpenVPN, hostPath mounts do not mount",Nolij,1136028189,1,1136028189,0,"<!--
MIND YOUR TITLE:
- ""App in a deploying state"" is NOT the title/description of your actual bug.
- ""Appname not working"" is NOT the title/description of your actual bug.
- Don't refer to a version, bugs are always for latest version
-->


***SCALE version***
<!--
- the version of truenas scale you are currently running.
- FOUND IN: System settings > general > os version
-->
TrueNAS-SCALE-22.02-RC.2 (with RC2 patcher applied)
***App Version***
<!--
- the version of the app having the issue and versions of any apps being used in conjunction
- FOUND IN: apps > installed applications
-->
4.4.0_9.0.44
***Application Events***
<!--
- debug information from the app(s) specifically
- FOUND IN: apps > installed applications > (click on the app name) > click the carrot next to application events and include a screenshot here
-->
```
2022-02-13 10:16:47
Started container openvpn
2022-02-13 10:16:47
Created container openvpn
2022-02-13 10:16:34
Container image ""ghcr.io/truecharts/openvpn-client:latest@sha256:bc3a56b2c195a4b4ce5c67fb0c209f38036521ebd316df2a7d68b425b9c48b30"" already present on machine
2022-02-13 10:16:34
Started container qbittorrent-test
2022-02-13 10:16:33
Created container qbittorrent-test
2022-02-13 10:16:31
Container image ""tccr.io/truecharts/qbittorrent:v4.4.0@sha256:b96e8102193a3be4a85cbaba167e656ed9ad1b3d86f9df0dd94de805daab28f6"" already present on machine
2022-02-13 10:16:31
Started container inotify
2022-02-13 10:16:30
Created container inotify
2022-02-13 10:16:23
Container image ""ghcr.io/truecharts/alpine:v3.14.2@sha256:4095394abbae907e94b1f2fd2e2de6c4f201a5b9704573243ca8eb16db8cdb7c"" already present on machine
2022-02-13 10:16:22
Started container autopermissions
2022-02-13 10:16:20
Started container lb-port-6880
2022-02-13 10:16:20
Created container lb-port-6880
2022-02-13 10:16:20
Created container autopermissions
2022-02-13 10:16:19
Started container lb-port-10096
2022-02-13 10:16:19
Created container lb-port-10096
2022-02-13 10:16:18
Started container lb-port-6882
2022-02-13 10:16:18
Created container lb-port-6882
2022-02-13 10:15:58
Container image ""rancher/klipper-lb:v0.1.2"" already present on machine
2022-02-13 10:15:58
Add eth0 [172.16.28.157/16] from ix-net
2022-02-13 10:15:57
Container image ""ghcr.io/truecharts/alpine:v3.14.2@sha256:4095394abbae907e94b1f2fd2e2de6c4f201a5b9704573243ca8eb16db8cdb7c"" already present on machine
2022-02-13 10:15:57
Add eth0 [172.16.28.156/16] from ix-net
2022-02-13 10:15:56
Container image ""rancher/klipper-lb:v0.1.2"" already present on machine
2022-02-13 10:15:56
Add eth0 [172.16.28.155/16] from ix-net
2022-02-13 10:15:55
Container image ""rancher/klipper-lb:v0.1.2"" already present on machine
2022-02-13 10:15:55
Add eth0 [172.16.28.154/16] from ix-net
Successfully assigned ix-qbittorrent-test/qbittorrent-test-db5cdb75b-bqqkb to ix-truenas
2022-02-13 10:15:40
Successfully provisioned volume pvc-1137f436-f444-4e56-89f6-9b41c9362217
0/1 nodes are available: 1 pod has unbound immediate PersistentVolumeClaims.
2022-02-13 10:15:39
Created pod: qbittorrent-test-db5cdb75b-bqqkb
Successfully assigned ix-qbittorrent-test/svclb-qbittorrent-test-torrentudp-9k8kl to ix-truenas
2022-02-13 10:15:39
Created pod: svclb-qbittorrent-test-torrentudp-9k8kl
2022-02-13 10:15:39
Scaled up replica set qbittorrent-test-db5cdb75b to 1
Successfully assigned ix-qbittorrent-test/svclb-qbittorrent-test-torrent-x66lm to ix-truenas
2022-02-13 10:15:39
Created pod: svclb-qbittorrent-test-torrent-x66lm
Successfully assigned ix-qbittorrent-test/svclb-qbittorrent-test-blmph to ix-truenas
2022-02-13 10:15:39
Created pod: svclb-qbittorrent-test-blmph
2022-02-13 10:15:39
waiting for a volume to be created, either by external provisioner ""zfs.csi.openebs.io"" or manually created by system administrator
2022-02-13 10:15:39
External provisioner is provisioning volume for claim ""ix-qbittorrent-test/qbittorrent-test-config""
```
***Application Logs***
<!--
- log output from the containers involved in the app
- FOUND IN: apps > installed applications > (click the 3 dots on the top right of the app box) > logs > open the logs for each pod used for the app and take a screenshot
- it can take a moment for the logs to show on the logs screen
-->
[qbittorrent-test_qbittorrent-test-db5cdb75b-bqqkb_qbittorrent-test.log](https://github.com/truecharts/apps/files/8055909/qbittorrent-test_qbittorrent-test-db5cdb75b-bqqkb_qbittorrent-test.log)
[qbittorrent-test_qbittorrent-test-db5cdb75b-bqqkb_openvpn.log](https://github.com/truecharts/apps/files/8055910/qbittorrent-test_qbittorrent-test-db5cdb75b-bqqkb_openvpn.log)

***Application Configuration***
<!--
- the configuration settings for the app (make sure to include what you have changed and what you didnt change
- if possible use the edit features in your screenshot tool to highlight the portions you cnaged intentionally
- FOUND IN: apps > installed applications > (click the 3 dots on the top right of the app box) edit
-->
![image](https://user-images.githubusercontent.com/16640162/153760346-a830501b-5186-45d1-ae14-54e72f7ba3a5.png)
![image](https://user-images.githubusercontent.com/16640162/153760372-578cd225-a8d6-410a-8188-27a43b3ffa90.png)
![image](https://user-images.githubusercontent.com/16640162/153760417-a031ae74-727b-4d95-90ac-86e64fbc0505.png)
(Synapse Data is a currently unused dataset I attached for testing purposes, my primary instance of this also experienced this on an almost identical configuration so I'm using a proxy, which is not as good for seeding, but nevertheless I wanted to keep it up while this is fixed)

**Describe the bug**
<!--
A clear and concise description of what the bug is.
-->
When using an OpenVPN connection (I suspect it is the same on WireGuard, but I have not tried that as I do not have access to a WireGuard VPN as of now) on a qBitTorrent app hostPath mounts are not mounted properly (also tried PVC, that didn't work either). The connection works (the logs show the VPN IP, not my IP), and adding and downloading torrents works, it just doesn't download them to my torrent dataset, making them inaccessible. This exact same configuration with VPN disabled works as intended, just without the VPN.
**To Reproduce**
<!--
- make sure that if you follow these steps again yourself the bug happens again. if it doesnt its probbly a configuration error and should be handled with a support thread on the discord
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error
-->
1. Create a qBitTorrent app
2. Configure a hostPath/PVC mount
3. Configure a valid OpenVPN connection
4. Configure the app so it should function (proper ports, etc.)
5. Start this app
6. To observe: open shell for the qBitTorrent container (not the OpenVPN container, although nothing appears there either). The hostPath/PVC mount will not be mounted.
**Expected behavior**
<!--
A clear and concise description of what you expected to happen.
-->
The configured mount should be there.
**Screenshots**
<!--
If applicable, add screenshots to help explain your problem.
-->

**Additional context**
<!--
Add any other context about the problem here.
-->
",True,0
"[qBitTorrent] When using OpenVPN, hostPath mounts do not mount",Ornias1993,1136028189,2,1038207509,0,"We've multiple users using this without these issues and i'm missing a LOT of required information in this buigreport.
Hence I would suggest going through our support process before (re)opening this bugreport.

So i'm closing this for now, untill the support staff can verify your setup is, in fact, correct.
(including possible permission issues)",False,0
"[qBitTorrent] When using OpenVPN, hostPath mounts do not mount",Nolij,1136028189,3,1038210975,0,"I made literally 1 change to the config: change VPN to disabled. It works as intended. That's literally the ONLY change I made. As for information, I've provided all the information requested, and I'm happy to provide more if you could tell me what you would like me to provide.",False,Bitter frustration
"[qBitTorrent] When using OpenVPN, hostPath mounts do not mount",Ornias1993,1136028189,4,1038272467,0,reopened due to verification by @Heavybullets8,False,0
"[qBitTorrent] When using OpenVPN, hostPath mounts do not mount",Heavybullets8,1136028189,5,1038284523,0,"So it seems Wireguard is working just fine with hostpath, but OpenVPN is not. Below are both of my configurations.

**Here is the Hostpath I used for both Wireguard and OpenVPN**
![hostpath](https://user-images.githubusercontent.com/20793231/153766868-d858294c-2fd5-4b8f-84ad-e9c53f7561f7.PNG)


**OpenVPN configuration**
![ovpn1](https://user-images.githubusercontent.com/20793231/153766881-84193291-db02-4ed1-8c8d-c1dd675180c6.PNG)


**The Shell doesn't display a ""welcome"" message like it does with the Wireguard configuration you'll see below. 
Also no  `/torrent` mountpath**
![ovpn2](https://user-images.githubusercontent.com/20793231/153766885-b034467b-abb4-46e7-9f9b-7ea91d84c3e5.PNG)


**Wireguard Configuration**
![wg1](https://user-images.githubusercontent.com/20793231/153767273-e4f03cb8-2133-4026-a4df-349a42452697.PNG)

**Wireguard Shell. with properly mounted `/torrent` as well as welcome message.**
![wg2](https://user-images.githubusercontent.com/20793231/153766977-0116ad66-5c71-4406-9cf2-a9749040dd23.PNG)",False,0
"[qBitTorrent] When using OpenVPN, hostPath mounts do not mount",indivisionjoe,1136028189,6,1038393014,0,"FWIW, I'm using OpenVPN and the HostPath part works fine. So, maybe this issue is triggered by some particular order of setting things up.

(I only landed here because I seem to have connectivity issues when using OpenVPN...)",False,0
"[qBitTorrent] When using OpenVPN, hostPath mounts do not mount",Ornias1993,1136028189,7,1040918070,0,@indivisionjoe Can you confirm you are also doing that in qbittorrent?,False,0
"[qBitTorrent] When using OpenVPN, hostPath mounts do not mount",Nolij,1136028189,8,1041032495,0,"This also happened on Prowlarr, I just don't give Prowlarr any mounts so it didn't effect me. I assume it's just a generic issue with the way TrueCharts does OpenVPN.",False,0
"[qBitTorrent] When using OpenVPN, hostPath mounts do not mount",Ornias1993,1136028189,9,1041248681,1,I was not asking you @Nolij,False,Bitter frustration
remove  #StandWithUkraine banner to updates,michealzh,1163541364,1,1163541364,0,The OSS is no need the WAR!!,True,Bitter frustration
remove  #StandWithUkraine banner to updates,podarok,1163541364,2,1062649814,0,"@michealzh 

I had to relocate my family(3months baby girl, 4.5yo twins, and a wife) from the place where I lived in my house to the west of Ukraine.

I'm OSS developer whole my life, you can check my profile here as well as at https://www.drupal.org/u/podarok

And I must say - you are making huge mistake by keeping yourself neutral and by trying to hide a war.

OSS must be the frontline of support any movement against aggression",False,Bitter frustration
remove  #StandWithUkraine banner to updates,Max-Kuzomko,1163541364,3,1062651586,0,"War in Ukraine - is a war against the whole world. Ukrainians were viciously attacked at night, like hitler did it. Ukrainians defend freedom, freedom of choice, democracy for the whole world. They don't want a soviet union come back. It's not a secret if Ukraine fall - putin will attack Europe next. russia and belarussia has already started a hybrid war against the world. You can see tons of fake news and so on.

It's not a time to demonstrate neutral status. It's a time for support. As phrases like ""I'm out of politics"", ""I am neutral"" lead to a war. It's not acceptable to say I am neutral when russian troops are killing kids, civilians, bombing ukrainian cities.",False,0
remove  #StandWithUkraine banner to updates,podarok,1163541364,4,1062656600,1,"BTW, I highly doubt you, @michealzh , are a good fit for Open Source Software.
Open - means open. Open access to the information.
Your activity is against Open Source principles. You are trying to close and hide.

I vote to ban you from GitHub",False,Insulting
Where is #StandWithPalestine ?,DeVoresyah,1165209319,1,1165209319,0,"In commit #86244a3695fcaaac9c5ba4257a4314eae1c6d981
adding #StandWithUkraine, if it's supporting humanity. Why did you never add #StandWithPalestine. Many people in Palestine got killed by Israel army.",True,Entitlement
Where is #StandWithPalestine ?,ufhy,1165209319,2,1064289540,0,Up,False,0
Where is #StandWithPalestine ?,ufhy,1165209319,3,1064289645,0,Up,False,0
Where is #StandWithPalestine ?,ufhy,1165209319,4,1064289698,0,Up,False,0
Where is #StandWithPalestine ?,ufhy,1165209319,5,1064289739,0,Up,False,0
Where is #StandWithPalestine ?,ufhy,1165209319,6,1064290478,0,Up,False,0
Where is #StandWithPalestine ?,ghost,1165209319,7,1064304673,1,"> In commit #86244a3695fcaaac9c5ba4257a4314eae1c6d981 adding #StandWithUkraine, if it's supporting humanity. Why did you never add #StandWithPalestine. Many people in Palestine got killed by Israel army.

I think it's pretty simple.
The author of this commit is a hypocrite, he does not care about both Ukraine and Palestine, he just wants to be on the hype",False,Bitter frustration
feat(peace): remove #StandWithUkraine,fzn0x,1166956712,1,1166956712,0,"We don't want this to be on the CLI screen. Everyone wants peace, but the writing #StandWithUkraine supports one side. This word hurts the feelings of the Russian people who don't want war. We can achieve peace if we side with both sides, regardless, and that's why there is such a thing as negotiation in the political world. This article is very annoying and makes me not want to use composer. You should apologize or delete it for writing this.

I would suggest you should write this on the Repository instead of CLI.

Let's keep the CLI neutral. 

#NoStandJustUnderstandEachOther
#NeutralComposerCLI

rel: https://github.com/composer/packagist/pull/1267#issuecomment-1063636569",True,Bitter frustration
feat(peace): remove #StandWithUkraine,lwlwilliam,1166956712,2,1065811329,1,"> We don't want this to be on the CLI screen. Everyone wants peace, but the writing #StandWithUkraine supports one side. This word hurts the feelings of the Russian people who don't want war. We can achieve peace if we side with both sides, regardless, and that's why there is such a thing as negotiation in the political world. This article is very annoying and makes me not want to use composer. You should apologize or delete it for writing this.
> 
> I would suggest you should write this on the Repository instead of CLI.
> 
> Let's keep the CLI neutral.
> 
> #NoStandJustUnderstandEachOther #NeutralComposerCLI
> 
> rel: [#1267 (comment)](https://github.com/composer/packagist/pull/1267#issuecomment-1063636569)

Friend, haven't you seen through these hypocritical people? They don't really stand with Ukraine. They just want to carve up Russia. You can see their attitude towards Palestine. They never dare to stand with Palestine.",False,Mocking
Remove #StandWithUkraine,thursdaybw,1174122161,1,1174122161,0,"Messages like this have absolute no place in the output of a tool It has no place here any more than it would if you put it in a speaker in my hammer and forced the user to listen to it.

It should be removed as a matter of urgency. Send the message on twitter, not in composer.

https://github.com/composer/composer/discussions/10600",True,Bitter frustration
Remove #StandWithUkraine,wilty39,1174122161,2,1073171093,0,"Next up, #GetyourBOOSTER!!! ",False,Mocking
Remove #StandWithUkraine,Seldaek,1174122161,3,1073236735,1,"Yes, also go get your fucking booster.",False,Vulgarity
Protonmail-bridge fails to initialise : pass not initialized,Dremor,1181103738,1,1181103738,0,"### App Name

protonmail-bridge

### SCALE Version

22.02.0

### App Version

1.8.10_5.0.1

### Application Events

```Shell
2022-03-25 19:06:20
Started container protonmail-bridge
2022-03-25 19:06:20
Created container protonmail-bridge
2022-03-25 19:06:15
Container image ""tccr.io/truecharts/protonmail-bridge:v1.8.10-1@sha256:58a54002123cc9a83cfb3170deb0a1dbf4cedabdced09a9c6bcafc19ee4b5631"" already present on machine
2022-03-25 19:06:14
Started container hostpatch
2022-03-25 19:06:13
Created container hostpatch
2022-03-25 19:06:09
Container image ""ghcr.io/truecharts/alpine:v3.14.2@sha256:4095394abbae907e94b1f2fd2e2de6c4f201a5b9704573243ca8eb16db8cdb7c"" already present on machine
2022-03-25 19:06:08
Started container autopermissions
2022-03-25 19:06:07
Created container autopermissions
2022-03-25 19:06:02
Container image ""ghcr.io/truecharts/alpine:v3.14.2@sha256:4095394abbae907e94b1f2fd2e2de6c4f201a5b9704573243ca8eb16db8cdb7c"" already present on machine
2022-03-25 19:06:02
Add eth0 [172.16.2.199/16] from ix-net
Successfully assigned ix-protonmail-bridge/protonmail-bridge-6f99779f8d-jvqq2 to ix-truenas
2022-03-25 19:05:56
Created pod: protonmail-bridge-6f99779f8d-jvqq2
2022-03-25 16:50:28
Scaled up replica set protonmail-bridge-6f99779f8d to 1
2022-03-25 19:05:49
Readiness probe failed: dial tcp 172.16.2.175:25: i/o timeout
2022-03-25 19:05:49
Liveness probe failed: dial tcp 172.16.2.175:25: i/o timeout
2022-03-25 19:05:31
Deleted pod: protonmail-bridge-6f99779f8d-h9khs
2022-03-25 19:05:32
Stopping container protonmail-bridge
2022-03-25 17:13:40
Scaled down replica set protonmail-bridge-6f99779f8d to 0
```


### Application Logs

```Shell
2022-03-25 18:06:20.493201+00:00+ [[ '' == init ]]
2022-03-25 18:06:20.494661+00:00+ socat TCP-LISTEN:25,fork TCP:127.0.0.1:1025
2022-03-25 18:06:20.495884+00:00+ rm -f faketty
2022-03-25 18:06:20.495904+00:00+ socat TCP-LISTEN:143,fork TCP:127.0.0.1:1143
2022-03-25 18:06:20.498608+00:00+ mkfifo faketty
2022-03-25 18:06:20.501253+00:00+ cat faketty
2022-03-25 18:06:20.501539+00:00+ protonmail-bridge --cli
2022-03-25 18:06:21.354322+00:00time=""2022-03-25T18:06:21Z"" level=warning msg=""Failed to add test credentials to keychain"" error=""pass not initialized: exit status 1: Error: password store is empty. Try \""pass init\"".\n"" helper=""*pass.Pass""
2022-03-25 18:06:21.377115+00:00[31mERRO[0m[Mar 25 18:06:21.377] Could not list credentials                    [31merror[0m=""no keychain"" [31mpkg[0m=credentials
2022-03-25 18:06:21.377194+00:00[31mERRO[0m[Mar 25 18:06:21.377] Could not load all users from credentials store  [31merror[0m=""no keychain"" [31mpkg[0m=users
2022-03-25 18:06:21.381824+00:002022-03-25T18:06:21.381824001Z
2022-03-25 18:06:21.381864+00:00Welcome to ProtonMail Bridge interactive shell
2022-03-25 18:06:21.381881+00:00___....___
2022-03-25 18:06:21.381888+00:00^^                __..-:'':__:..:__:'':-..__
2022-03-25 18:06:21.381894+00:00_.-:__:.-:'':  :  :  :'':-.:__:-._
2022-03-25 18:06:21.381900+00:00.':.-:  :  :  :  :  :  :  :  :  :._:'.
2022-03-25 18:06:21.381911+00:00_ :.':  :  :  :  :  :  :  :  :  :  :  :'.: _
2022-03-25 18:06:21.381921+00:00[ ]:  :  :  :  :  :  :  :  :  :  :  :  :  :[ ]
2022-03-25 18:06:21.381927+00:00[ ]:  :  :  :  :  :  :  :  :  :  :  :  :  :[ ]
2022-03-25 18:06:21.381933+00:00:::::::::[ ]:__:__:__:__:__:__:__:__:__:__:__:__:__:[ ]:::::::::::
2022-03-25 18:06:21.381944+00:00!!!!!!!!![ ]!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!![ ]!!!!!!!!!!!
2022-03-25 18:06:21.381951+00:00^^^^^^^^^[ ]^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[ ]^^^^^^^^^^^
2022-03-25 18:06:21.381957+00:00[ ]                                        [ ]
2022-03-25 18:06:21.381963+00:00[ ]                                        [ ]
2022-03-25 18:06:21.381973+00:00jgs   [ ]                                        [ ]
2022-03-25 18:06:21.381980+00:00~~^_~^~/   \~^-~^~ _~^-~_^~-^~_^~~-^~_~^~-~_~-^~_^/   \~^ ~~_ ^
2022-03-25 18:06:21.382716+00:00ProtonMail Bridge is not able to detect a supported password manager
2022-03-25 18:06:21.382757+00:00(pass, gnome-keyring). Please install and set up a supported password manager
2022-03-25 18:06:21.382766+00:00and restart the application.
```


### Application Configuration

![image](https://user-images.githubusercontent.com/1915939/160177528-ed1d3287-5f2d-4be6-b43d-023378be124e.png)
![image](https://user-images.githubusercontent.com/1915939/160177589-f59475e8-be90-45d3-9053-05e93b953f58.png)
![image](https://user-images.githubusercontent.com/1915939/160177646-632b898a-13f4-4be5-b703-8038aa72c334.png)
![image](https://user-images.githubusercontent.com/1915939/160177708-d4354e4b-043e-4653-b0a2-584787525385.png)


### Describe the bug

The bridge doesn't come up. Logs show it fails to initialize.

When opening a shell in the container, and lauch the command ""protonmail-bridge --cli"" I get the following error :
WARN[0000] Failed to add test credentials to keychain    error=""pass not initialized: exit status 1: Error: password store is empty. Try \""pass init\"".\n"" helper=""*pass.Pass""

After using the command ""pass init protonmail"" to initialize de password manager, I get the following error : 
WARN[0000] Failed to add test credentials to keychain    error=""exit status 1: gpg: protonmail: skipped: No public key\ngpg: [stdin]: encryption failed: No public key\nPassword encryption aborted.\n"" helper=""*pass.Pass""

After initializing a gpg key (without password and all default settings), following [this issue](https://github.com/ProtonMail/proton-bridge/issues/190), I was able to make it work.

### To Reproduce

- Install protonmail-bridge
- bug occur

### Expected Behavior

Protonmail bridge should be up and accepting connexion.

### Screenshots

![image](https://user-images.githubusercontent.com/1915939/160179439-c06884bf-f05f-4e82-891f-b5e99fc1dc5d.png)

### Additional Context

None

### I've read and agree with the following

- [X] I've checked all open and closed issues and my issue is not there.",True,0
Protonmail-bridge fails to initialise : pass not initialized,Dremor,1181103738,2,1079305717,0,"btw, looks like you can only have one instance of the cli opened atm. So once the application is started, there is no way to login, etc.
",False,0
Protonmail-bridge fails to initialise : pass not initialized,MattInternet,1181103738,3,1119952359,0,"I also have the same end log:
```
2022-05-06 19:33:45.489661+00:00ProtonMail Bridge is not able to detect a supported password manager
2022-05-06 19:33:45.489695+00:00(pass, gnome-keyring). Please install and set up a supported password manager
2022-05-06 19:33:45.489703+00:00and restart the application.
```",False,0
Protonmail-bridge fails to initialise : pass not initialized,Dremor,1181103738,4,1153749590,0,Still the same as of current version (1.8.10_6.0.3). Is there an official maintainer or should I take the matter into my own hands ?,False,0
Protonmail-bridge fails to initialise : pass not initialized,stavros-k,1181103738,5,1153760340,0,"> Still the same as of current version (1.8.10_6.0.3). Is there an official maintainer or should I take the matter into my own hands ?

Of course it's the same, If it had been fixed, this issue would be also closed.

Even when there is a maintainer, PR's are welcomed, so you can work on this yea",False,0
Protonmail-bridge fails to initialise : pass not initialized,sam-kleiner,1181103738,6,1219928315,0,"> btw, looks like you can only have one instance of the cli opened atm. So once the application is started, there is no way to login, etc.

Here is a temporary workaround until this is fixed.

- Open a shell into the pod
- Find the running bridge
  - `ps -aux`
  - take note of the PID the command should look like ""/usr/lib/protonmail/bridge/proton-bridge --cli --launcher /usr/lib/protonmail/bridge/proton-bridge-launcher""
- Kill it
  - `kill -9 #`
  - replace # above with your PID
- Launch the CLI and login
  - `protonmail-bridge --cli`
  - `login`
- Stop and start the app",False,0
Protonmail-bridge fails to initialise : pass not initialized,Dremor,1181103738,7,1295765753,0,Still no fix ?,False,Impatience
Protonmail-bridge fails to initialise : pass not initialized,xstar97,1181103738,8,1295770476,0,"> Still no fix ?

https://truecharts.org/docs/charts/stable/protonmail-bridge/installation_notes",False,0
Protonmail-bridge fails to initialise : pass not initialized,Ornias1993,1181103738,9,1295787330,1,"We dont work for you guys, we are a *community* of volunteers.

this one specifically requires protonmail. Which almost none of the staff have. 

+1‚Äôing the issue is not helping your case. Either stfu and wait or fix it yourself.",False,Vulgarity
HIP 100,alphaqt,1190959350,1,1190959350,0,"I have over 30 years working in the Wireless Industry.  I would like to propose a long term structure that will align Helium with how real world wireless networks are designed and operated.  I am proposing to initiate planning for ""HIP 100"" which would restructure the Helium network approach to align with the fundamentals of wireless networks which:
1. Provide consistent reliable COVERAGE.
2. Provide the highest QUALITY which consists of MAXIMIZING signal levels (RSRP) while MINIMIZING interference levels (SINR).
3. Provide the best user PERFORMANCE at the LOWEST cost which consists of CAPACITY planning based on SUBSCRIBER density and locations
4. Have a robust END-TO-END architecture which balances the CORE network, the RADIO network and the END-USER device performance.
5.  Provides accurate LOCATION information about BOTH the Hotspot and the Devices.  All modern networks today have GPS built into both the Cell Tower and the devices.  LOCATION is fundamental to securing the network as well as optimizing the network and device performance.

Even though the current Helium network is a major advance towards distributed ownership and control, the current construct is not aligned with the fundamentals of Wireless design.  For example:
1. To build a network, there needs to be a reward system for BOTH the HOTSPOT and the DEVICE.  Until the network is built and device performance is validated, there should be rewards for buying and deploying DEVICES to to test and validate coverage.  Validating coverage using HOTSPOT to HOTSPOT beacons is fundamentally flawed.  This is NOT how wireless works.  Base stations try to AVOID seeing each other to minimize interference.
2. The current reward system motivates hotspot owners to place their hotspot at the highest point possible so they can witness the most number of other hotspots.  This results in drastically increasing SINR which is opposite of how wireless should be designed.  You want to MINIMIZE SINR, because that is the fundamental of Cellular, so that the same frequencies can be reused and the CAPACITY and QUALITY can be maximized.
3. The current reward system favors hotspots that are in less populated areas and penalizes hotspots in dense areas.  This is also opposite of the fundamentals of wireless.  You need MORE hotspots in dense areas.
4. The current implementations of Beaconing is flawed as well.  Wireless networks beacon 100% of the time.  This maximizes the CAPACITY and ensures that the network performance STATISTICS are consistently reported to allow proper monitoring and optimization.
5. The current implementation does not have accurate LOCATION of the hotspots nor the DEVICES.  In fact there are so few DEVICES that there is no DEVICE performance information.   LOCATION is not difficult, using a combination of GPS at key Hotspots, Triangulation algorithms and WiFi to accurately and securely locate all Hotspots and devices in the Network.  Without accurate LOCATION information, the network will never be secure nor will we be able to deploy a QUALITY, HIGH PERFORMANCE network.

Understand, that I am not proposing to ""throw the Baby out with the Bath Water"".  Nor am I criticizing the objective of Helium to build a decentralized, ownership model.  But we want to build a network that will actually deliver a service that is of value to a subscriber base of wireless devices.   It is paramount that we begin this journey towards a working Wireless construct or else we may find ourselves in this predicament:  ""IF YOU BUILD IT - IT WILL NOT WORK"" rather than what we want which is ""IF YOU BUILD IT THEY WILL COME""",True,0
HIP 100,gradoj,1190959350,2,1087786727,0,"I am pretty sure this is a troll but it is also just as likely that this is the only caliber of employee that telcos can keep.
1. Helium provides more lorawan coverage than any telco. thanks for mentioning we need coverage
2. High quality is also high cost. The question is what is good enough? 
3. This sounds good but these are conflicting requirements so you are actually saying nothing valuable.
4. Another meaningless statement. Maybe you should do some research on the protocol. https://lora-alliance.org/lorawan-for-developers/
5. You want to mandate sensors have GPS? This is an IoT network and most sensors cannot require the high power consumption nor is it required. The gateways are fixed so GPS doesn't really provide any value. Please tell how it would increase device performance.

Numbers go higher than 5 not really sure why we're back at one...you can use both hands
1.If you had done any research at all you'd realize this has been discussed and rejected for years. Please recommend an incentive system to reward sensors that cannot be abused. Please research ADR which is built into the lorawan protocol
2.get antenna high and out of the noise and then reduce power. Why would you put it low in the noise? Again please see ADR
3.now i know you are trolling. take a look in any major urban center and say more hotspots are required. This is starting to sound like you are whining you aren't earning enough.
4.Please understand the current protocol before suggesting improvements
5.Again please do some research.  Hotspot location is fundamental to POC the algorithm. 

if the intention of the hip were good i am sorry but honestly the hip just comes across as sour grapes from an uninformed employee at a telco. Maybe do some research and come back with some actual detail and a little less arrogance and attitude. 
",False,Insulting
HIP 100,alphaqt,1190959350,3,1088233686,0,"I am not a telco troll - far from it
But I can see that even though the premise of helium is a great idea - the lack of knowledge about how wireless works and the chaos and poor performance of the current network shows that the telcos have nothing to worry about -because you will never reach your goal

know that I really want you to succeed but you are in a ditch - I do really want to help - but if you don‚Äôt want help - best of luck - it will never work on the current path - because wireless is all about physics, math, and engineering - which has nothing to do with whether you are a telco or innovators like the Helium community.  I have been predicting for almost 10 years that the telco networks will eventually collapse.   I was hoping Helium was going to fulfill that prediction.  But if you don‚Äôt want advice from people who understand the science you are completely lost and your ship will sink before you reach the promised land.",False,0
HIP 100,alphaqt,1190959350,4,1088235659,0,By the way none of your responses actually were what I said.  They were assumptions because you don‚Äôt understand wireless.  Wireless does not equal telco by the way.  Wireless equals science physics and math.  The telcos are businesses that have filled the government into giving ask the spectrum and they are fat and bloated and inefficient because Helium can‚Äôt get out of their own mess to take them down.  Clean up your own house before you try to take over their house,False,Entitlement
HIP 100,alphaqt,1190959350,5,1088237367,1,Helium appears to be a bunch of gamers and crypto zealots who know so little about the science behind wireless.  I could help you so much but you are blind to the flaws of the current system,False,Insulting
Color adjustment asap please - Docs,sotos-dev,1201906086,1,1201906086,0,"### Describe the problem

Hello ladies and gents.

I am fairly new to Svelte/Sveltekit but I would like to make a suggestion please.

It is regarding documentation...

Can somebody change the background color of the sidebar from this difficult color #676778; to something darker?
Literally every time you want to focus on a link on the sidebar your eyes hurt and you lose at least 1 to 2 seconds until you focus on what you want to find and all that because poor contrast.

Also the ""highlighted"" link (the active one) is, honestly, almost indistinguishable from the inactive links below and above, why is this? 

Why not make it something like almost black background with white inactive links and the active link like the orange color from the logo? Perhaps put a divider line in between links or something?

By the way, I do these things every time  I use the docs, this is how bad it is, no offense to anyone, but 1-2 seconds of faster focus per person per time they visit the docs, well, we want to be talking about performance when we are talking about Svelte right? 

Please do the right thing, the nice svelte logo orange color matches great with a darker blackish background color, why put that ugly blueish/purplish (which is not dark enough for a bg color)?

To whoever can make this happen, PLEASE we need docs that can be scanned quickly. 

Here is the contrast score https://coolors.co/contrast-checker/ffffff-676778
It's almost poor... if you have any sort of sight issues... I mean come on, how come no one addressed this before, docs is a huge part of the process.

### Describe the proposed solution

Color for the ""active"" link => #ff3e00

Color for the sidebar background => #242426

Any dark enough background (with no colors, either blackish or grayish/blackish will do) 

### Alternatives considered

Please adjust this so visually impaired or people with sight difficulties (even the ones with glasses) can focus easier on the links and traverse faster. 

### Importance

i cannot use SvelteKit without it

### Additional Information

Every time I browse the docs I literally go to inspect and change these myself....!

Thanks for your understanding!",True,0
Color adjustment asap please - Docs,benmccann,1201906086,2,1096959313,0,"I tend to agree that we could add more contrast to the sidebar. I'm not sure I like the proposed changes though. Here's a screenshot of what they look like

![Screenshot from 2022-04-12 09-40-44](https://user-images.githubusercontent.com/322311/163012323-601a03cb-4502-4501-bf3b-79f284653f93.png)

Alternatively, here's the sideback background set to `#363636` with no change to the active link

![Screenshot from 2022-04-12 09-43-56](https://user-images.githubusercontent.com/322311/163012757-73360b3d-d8df-42dd-bcd8-56c396ba3133.png)",False,0
Color adjustment asap please - Docs,benmccann,1201906086,3,1096962686,0,"Another idea, here's using bold rather than color to indicate the active item

![Screenshot from 2022-04-12 09-48-01](https://user-images.githubusercontent.com/322311/163013432-21c71ad2-0dc6-4bea-aeba-fa71bda869a0.png)
",False,0
Color adjustment asap please - Docs,sotos-dev,1201906086,4,1096986814,0,"Hey @benmccann  thanks for the response here.

On the first slide the accent color looks kinda red and not the actual orange brand color, are you sure you used the right code? 
If it is the right color code then perhaps on that particular background we could lighten the orange just a bit.

Listen I very much appreciate your feedback here but I have to mention that as a person that has poor sight, I would argue that it's imperative the background to be kept at least a bit darker than your proposed hex above and the active link to be of an accent color (instead of a base like white) so it easily distinguishable and not on the same hue as the inactive links (truly important).

Check this =>
https://vuejs.org/guide/introduction.html
(They also have dark and light theme...)

We want to grow as a framework and as a user base but Vue.js is killing us with those Docs comparing to Svelte/kit's.

Please choose a slight darker background color, just a tiny touch and keep a punchy accent color close to the logo for the active links, I am positive this will help grow the framework, it sounds crazy but it is in my experience an important detail really.

Between choosing competing stuff online I personally have chosen then ones with the more accessible documentation.

@benmccann  thanks again for your consideration!",False,0
Color adjustment asap please - Docs,UltraCakeBakery,1201906086,5,1097696786,0,If only there was something like https://caniuse.com/mdn-css_at-rules_media_forced-colors,False,0
Color adjustment asap please - Docs,sotos-dev,1201906086,6,1097721070,0,"Hey @UltraCakeBakery, thanks for your reply here.

I am not sure what you are suggesting and if there is sarcasm involved on your reply.
To be honest it feels there is sarcasm involved in your message because of the way you started your sentence and you didn't expand on what you are trying to say, you didn't take the time, you pasted a link and asked of us to figure everything else out.
 
You provided no basis for all other people to work with. Is this link a universal and time-efficient solution to the current problem? 
Do you mind telling us a little bit more please? 

This is a serious matter that affects thousands of people (in different degrees). 
Are you suggesting we all take the above action instead of adjusting 2 colors in the actual docs?

This is a genuine problem that needs solving, please provide constructive feedback/solutions.
No negativity is needed, thank you.",False,Impatience
Color adjustment asap please - Docs,UltraCakeBakery,1201906086,7,1097940757,1,"> Hey @UltraCakeBakery, thanks for your reply here.
> 
> I am not sure what you are suggesting and if there is sarcasm involved on your reply. To be honest it feels there is sarcasm involved in your message because of the way you started your sentence and you didn't expand on what you are trying to say, you didn't take the time, you pasted a link and asked of us to figure everything else out.
> 
> You provided no basis for all other people to work with. Is this link a universal and time-efficient solution to the current problem? Do you mind telling us a little bit more please?
> 
> This is a serious matter that affects thousands of people (in different degrees). Are you suggesting we all take the above action instead of adjusting 2 colors in the actual docs?
> 
> This is a genuine problem that needs solving, please provide constructive feedback/solutions. No negativity is needed, thank you.



> This is a serious matter that affects thousands of people (in different degrees).
> Are you suggesting we all take the above action instead of adjusting 2 colors in the actual docs?

Hey @savannahx, thanks for your reply here.

I am not sure if there is unawareness or selfishness involved on your reply.
To be honest it feels there was unawareness and selfishness involved in your message because of the way you formulated yourself and did not think about us, the other 99% of svelte-kit users, who also have to deal with this change and asked of us to actually do all the work of implementing this new color.

You provided no way for us to reach a middle ground where both you and others can be happy. It seems like you really just want us to change the color to what you want. Did you even read the link that I posted in the one minute before you started writing this reply you posted 3 minutes after me making my original comment? Do you mind telling us why you get a bad vibe from my comment (saying it is sarcastic) even though I was trying to be helpful?

Suggestions like these affects billions of people (in different degrees). Are you suggesting we completely change the documentation instead of asking you and ""the others"" to start using the build-in tools provided to you by your operating system and browser?

This is a genuine problem that needs solving, please consider changing your mindset. We do not need more any people in the world that are like:

""THE WORLD NEEDS TO CHANGE, NÃµÕõÕòÃøÃ†Ã≤OÃµÕòÃíÃáÃûÃôTÃ¥ÕÑÕÇÃºÕìÃÆ ME"" 

""I AM **NÃµÕõÕòÃøÃ†Ã≤OÃµÕòÃíÃáÃûÃôTÃ¥ÕÑÕÇÃºÕìÃÆ** GONNA WEAR GLASSES OR CHANGE MY SETTINGS! ALL LIGHTS AND COLOURS SÃ∑ÃëÃîÕåÃìÃêÕàÃ´ÃÆÕôHÃ∏ÕäÃâÃÇÕåÕéÃ±Ã´Ã®Ã®Ã®OÃ∏ÃÅÃΩÃèÕìÕìÕñUÃ¥ÕÑÃπÕôÕïÃùÕàÕÖLÃ¥ÕÑÃΩÕêÃõÃ≠ÃßÕáÃ≠ÃóÃ§DÃ¥ÃàÃïÃÖÃÄÃª JUST GET BRIGHTER AND DARKER WÃµÕÄÃéÃÖÃûÃªÃ§ÃóÃ©HÃ¥ÕÜÕùÃöÕÅÃ§ÃÆÃ®ÕúÃºÕâEÃ¥ÃõÕÅÃÉÃåÃ†NÃ∑ÃÄÃâÕ†ÃÑÃàÃÜÕåÕöÕîÃÆÕôÃ¨Õñ ÃµÕùÕóÃøÕñÃ©Ã∞IÃµÃÅÕÇÃãÃúÕà ÃµÃöÕóÃèÃçÃíÃÅÃàÃüÕìÃ±Ã§ÕñÃØÕôWÃ¥ÕÇÃøÃ©ÃπÃñÃ≠ÕÖAÃ∑ÃÇÃêÃÑÕÉÃöÃîÃ≥Ã¨Ã±ÕïNÃ¥ÃèÕíÃáÃøÃôÃ≥ÕÖÕîTÃ∑ÃàÕÇÕÑÃàÕêÃåÕÖÕâÕöÃ≥ Ã∂ÕÇÕÉÃÅÕçÃ¶TÃ∏ÃîÕçÃ≠Ã§Ã≤Ã≤ÕúÕïHÃ∏ÃëÕåÃìÃåÃÜÕõÃ≥Ã£EÃ¥ÕãÃõÃáÃÅÃ¢Ã∞Ã±MÃ∂ÕÄÃÄÃÇÃÑÕÅÕäÃ≥ ÃµÃÅÕóÕÑÃéÃäÃäÕëÕïÃ£Ã≤ÃºTÃ∏ÕëÕõÃÜÃ≠ÃßÕïÕÖÃ™ÕîÃºOÃ∑ÃãÃÑÕëÕåÕõÕÜÃÖÃóÃßÃ´ÕàOÃ∑ÃéÕòÃãÕÅÕñÕàÃ±Ã≥ÕìÃ†!ÃµÃçÃÇÃæÕìÕÖÃ©ÃôÃ¶!Ã¥ÃäÕÄÕ†Ã±ÕáÃùÕîÃªÃ≠Ã°!Ã¥ÕÇÃãÃõÃïÕ†Ã†ÕîÃ©Õö!ÃµÃõÕùÃüÃºÃòÃºÕç!!!!"" ",False,Impatience
[Task] Vuetify 3 Release Checklist,johnleider,1213461044,1,1213461044,0,"# V3 Release Checklist

The following is an in progress list of remaining tasks before we release the next version. 

This will be updated daily; so watch for changes.

Discord, https://community.vuetifyjs.com/ and join channel **#v3-discussion**.

## Final review

- [x] Framework Core #15139
- [x] Documentation
- [x] UI Components
- [x] Tooling

### Documentation

- [x] Introduction
- [x] Getting started
- [x] Features
- [x] Styles and animations
- [x] Directives
- [x] Resources
- [x] About

### Tooling

- [x] CLI plugin
- [x] Vite plugin
- [x] Presets
- [x] Eslint

### UI Components

A component is considered complete when the following criteria is met:

<details>

* Primary and child components reviewed
  * imports ordered with appropriate comments
  * converted to use grid css structure (if applicable)
  * unit tests
    * jest spec test (if applicable)
    * cypress e2e test
* SASS/SCSS code reviewed
  * removes deprecated or unused code
  * abstracted explicit values to variables
  * verified sass variable usage
* index.ts file export formatted
* Documentation page
  * all examples working with no deprecated code
  * removed unused or deprecated examples
  * added **all components** page image
  * added quick bar and links
  * updated page entry and formatting
  * new sections added
    * accessibility (if applicable)
    * anatomy
    * grid (if applicable)
    * sass variables
    * theme (if applicable)
  * updated api section formatting
  * frontmatter information reviewed
</details>

- [x] v-alert #14971
- [x] v-app #15179
- [x] v-app-bar #15192
- [x] v-autocomplete #15355
- [x] v-avatar #15112
- [x] v-badge #15193
- [x] v-banner #15109
- [x] v-bottom-navigation #15194
- [x] v-breadcrumbs #15181
- [x] v-btn #15213
- [x] v-btn-group #15195
- [x] v-btn-toggle #15196
- [x] v-card #15283
- [x] v-carousel #15197
- [x] v-checkbox #15264
- [x] v-chip #14973
- [x] v-chip-group #15198
- [x] v-code #15182
- [x] v-color-picker #15292
- [x] v-combobox #15354
- [x] v-counter #15183
- [x] v-default-provider
- [x] v-dialog #15289
- [x] v-divider #15113
- [x] v-expansion-panels #15295
- [x] v-field #15251
- [x] v-file-input #15288
- [x] v-footer #15114
- [x] v-form #15253
- [x] v-grid #15296
- [x] v-hover
- [x] v-icon #15212
- [x] v-img #15287
- [x] v-input #15260
- [x] v-kbd #15199
- [x] v-label #15256
- [x] v-lazy #15184
- [x] v-list #15360
- [x] v-locale-provider #15255
- [x] v-main #15254
- [x] v-menu 5e4992ffa
- [x] v-messages #15200
- [x] v-navigation-drawer #15263
- [x] v-no-ssr #15185
- [x] v-overlay #15284
- [x] v-pagination #15286
- [x] v-parallax #15226
- [x] v-progress-circular #15266
- [x] v-progress-linear #15265
- [x] v-radio #15258
- [x] v-radio-group #15259
- [x] v-range-slider #15228
- [x] v-rating #15285
- [x] v-responsive #15249
- [x] v-selection-control #15246
- [x] v-selection-control-group #15250
- [x] v-sheet #15115
- [x] v-slide-group #15248
- [x] v-slider #15227
- [x] v-snackbar #15242
- [x] v-switch #15247
- [x] v-system-bar #15243
- [x] v-table #15186
- [x] v-tabs #15262
- [x] v-text-field #15252
- [x] v-textarea #15257
- [x] v-theme-provider
- [x] v-timeline #15230
- [x] v-toolbar #15154
- [x] v-tooltip #15245
- [x] v-validation
- [x] v-window #15244

<!-- override-close -->",True,0
[Task] Vuetify 3 Release Checklist,michaelnguyen08,1213461044,2,1110808512,0,"I'm sorry, v-time-picker and v-date-picker not coming with this release?",False,0
[Task] Vuetify 3 Release Checklist,omerkimel,1213461044,3,1111058291,0,What about v-data-table?,False,Impatience
[Task] Vuetify 3 Release Checklist,JesusFregoso,1213461044,4,1111140000,0,"What about v-skeleton-loader ?
",False,Impatience
[Task] Vuetify 3 Release Checklist,gregveres,1213461044,5,1111579118,0,"@michaelnguyen08, @omerkimel , @JesusFregoso You can see the list of components that didn't make the 3.0 cut in the [3.1 milestone](https://github.com/vuetifyjs/vuetify/milestone/56).

These were cut from the initial release fairly early this year. Noteably this includes the following components:

- Calendar
- TimePicker
- data-iterator
- date picker
- v-stepper
- v-speed-dial
- v-skeleton-loader
There are some feature requests in the milestone that I am hoping John and the team will punt to a later release to expedite getting these base components released. 


v-data-table was mentioned in above, but that is supposed to be in v3.0.0 milestone. I just looked at the docs for v3 and it looks like v-data-table was renamed to v-table. v-table is in the docs and in the list above so that appears to have made the cut for v3.0.",False,0
[Task] Vuetify 3 Release Checklist,AquaMCU,1213461044,6,1111600813,0,"Hi. I just opened the node_modules/vuetify/lib/components directory and I can see, that a component directory ""VDataTable"" is included in the Beta.1 build. When I try to include it in my App, I see, that VUE is not able to resolve it ""[Vue warn]: Failed to resolve component: v-data-table"". Seems, that the component is just not registered. 

And I disagree, that will be renamed...",False,0
[Task] Vuetify 3 Release Checklist,MajesticPotatoe,1213461044,7,1111623691,0,"> Hi. I just opened the node_modules/vuetify/lib/components directory and I can see, that a component directory ""VDataTable"" is included in the Beta.1 build. When I try to include it in my App, I see, that VUE is not able to resolve it ""[Vue warn]: Failed to resolve component: v-data-table"". Seems, that the component is just not registered.

Directory is still there because its not that it is being removed, it just hasn't been converted.
this: https://github.com/vuetifyjs/vuetify/blob/next/packages/vuetify/src/components/index.ts
controls what actually get exported and is what is currently available to use",False,0
[Task] Vuetify 3 Release Checklist,zorn-v,1213461044,8,1114199104,0,"Where is `loading` prop in VBtn component ? 
Is it removed completely or added later ?
It was amazing button feature.",False,0
[Task] Vuetify 3 Release Checklist,aentwist,1213461044,9,1116679105,0,"I am guessing that not only are components being cut, but also props? For example, Menu is missing a fantastic amount of props functionality -

https://vuetifyjs.com/en/api/v-menu/#props
https://next.vuetifyjs.com/en/api/v-menu/#props

I need `left` üòï

What is the plan with this?",False,0
[Task] Vuetify 3 Release Checklist,warflash,1213461044,10,1116768569,0,"> I am guessing that not only are components being cut, but also props? For example, Menu is missing a fantastic amount of props functionality -
> 
> https://vuetifyjs.com/en/api/v-menu/#props https://next.vuetifyjs.com/en/api/v-menu/#props
> 
> I need `left` üòï
> 
> What is the plan with this?

@aentwist 
Well v-menu is still WIP, you can track the progress here: https://github.com/vuetifyjs/vuetify/issues/13489 or in the [3.0.0 milestone](https://github.com/vuetifyjs/vuetify/milestone/45)

> I need `left` üòï

I have not used v3 yet but based on the docs it seems like `:anchor=""start""` could be what you are looking for?",False,0
[Task] Vuetify 3 Release Checklist,johnleider,1213461044,11,1116862005,0,"> I am guessing that not only are components being cut, but also props? For example, Menu is missing a fantastic amount of props functionality -
> 
> 
> 
> https://vuetifyjs.com/en/api/v-menu/#props
> 
> https://next.vuetifyjs.com/en/api/v-menu/#props
> 
> 
> 
> I need `left` üòï
> 
> 
> 
> What is the plan with this?

I don't think we have any completely removed props. Some may have been combined to be more intuitive, but they are there.

In regards to v-btn, the v-progress-circular component wasn't ported when it was. It'll be there, I just have to get to it. ",False,0
[Task] Vuetify 3 Release Checklist,aentwist,1213461044,12,1116887251,0,"I was imagining the combination of `offset-y` and `left`, which is now done with `anchor=""bottom end""`. Thanks for the tip with `anchor`, I missed that in the Menu docs. Thanks to both for the info about the props.",False,0
[Task] Vuetify 3 Release Checklist,Stevelriemenbill,1213461044,13,1116993006,0,"> What about v-data-table?

see following answer for those interested: https://github.com/vuetifyjs/vuetify/issues/13479#issuecomment-1115233462",False,0
[Task] Vuetify 3 Release Checklist,MatthewAry,1213461044,14,1118773644,0,Would like to suggest adding a Nuxt3 plugin to this list as well.,False,0
[Task] Vuetify 3 Release Checklist,warflash,1213461044,15,1118865904,0,"> Would like to suggest adding a Nuxt3 plugin to this list as well.

@MatthewAry 
I'd love to see first class nuxt support as well, though that's something done by [nuxt ](https://github.com/nuxt-community/vuetify-module) itself, not vuetify.
Related: 
https://github.com/vuetifyjs/vuetify/issues/14621#issuecomment-1016186592
https://github.com/nuxt-community/vuetify-module/issues/475",False,0
[Task] Vuetify 3 Release Checklist,MatthewAry,1213461044,16,1118875389,0,@warflash If that's what @KaelWD says then I suppose that's how it is. However I am really anxious to see support for SASS Variables in Nuxt3 for the Vuetify project.,False,0
[Task] Vuetify 3 Release Checklist,Azema4ka,1213461044,17,1119082366,0,"> –ì–¥–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è `loading`–ø–æ–¥–¥–µ—Ä–∂–∫–∞ –≤ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–µ VBtn? –û–Ω —É–¥–∞–ª–µ–Ω –ø–æ–ª–Ω–æ—Å—Ç—å—é –∏–ª–∏ –¥–æ–±–∞–≤–ª–µ–Ω –ø–æ–∑–∂–µ? –≠—Ç–æ –±—ã–ª–∞ —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∫–Ω–æ–ø–∫–∏.

https://github.com/vuetifyjs/vuetify/issues/15056",False,0
[Task] Vuetify 3 Release Checklist,edmundmunday,1213461044,18,1120687921,0,"For those components not included in the initial release, is there an incremental path for people who's apps use those components, or are they just going to have to wait until 3.1 before they can upgrade?",False,0
[Task] Vuetify 3 Release Checklist,MartinX3,1213461044,19,1120718350,0,"Isn't 
3.0
3.1
3.2
3.x
incremental enough?

Just wait until everything is proper released.
I can't understand people needing to upgrade to a new version as soon as possible, even if it is Beta.

The only thing I would like would be a comparison table about which component gets added into which version.

Also you can wait for Vue 2.7 and Vueitfy 2.7.
These are already announced migration versions with backported features.....",False,Bitter frustration
[Task] Vuetify 3 Release Checklist,gregveres,1213461044,20,1120961618,0,"> For those components not included in the initial release, is there an incremental path for people who's apps use those components, or are they just going to have to wait until 3.1 before they can upgrade?

If you rely on some of the components that aren't going to be in 3.0, I see two choices:

1. you wait until the release that contains all of the components you use
2. you replace the missing components with components you build

I have been waiting for Vuetify 3 so that I can migrate to Vue 3 and Vite, but I have chosen option 1 above.  I use most of the components that aren't available in 3.0

- Data Table
- Calendar
- TimePicker
- date picker
- v-stepper
- v-skeleton-loader",False,Impatience
[Task] Vuetify 3 Release Checklist,websitevirtuoso,1213461044,21,1121946118,0,Please add v-data-table in this release. Thanks,False,Impatience
[Task] Vuetify 3 Release Checklist,johnleider,1213461044,22,1121967343,0,"> Please add v-data-table in this release. Thanks

How? It's not done.",False,0
[Task] Vuetify 3 Release Checklist,sinisarudan,1213461044,23,1122109765,0,"> @MartinX3 I can't understand people needing to upgrade to a new version as soon as possible, even if it is Beta.

I see that you don't understand, but the point is if you start a new project, you DON'T want to start it in v.2.x and then to migrate it all to v.3.x. Take in mind that it is not only vuetify 3, but you are by being forced to use vuetify 2, forced to use vue 2 too! So it is pretty pretty painful enforcement!

Thus, developers and PMs are ""enforced"" and motivated to do additional work and workarounds etc to enable v.3 version working, then to have the enormous work later in migrating whole the project.

I hope that collaborators on Vuetify are aware of this too :)",False,0
[Task] Vuetify 3 Release Checklist,MartinX3,1213461044,24,1122120793,1,"@sinisarudan I see you prefer very much time and energy into complaining instead and also investing very much time and energy to do workarounds and extra stuff, just to use as soon as possible the newest not ready software.
Weird people. I hope you won't become my employee.

smart people just use vue 2 with vuetify 2 and just add the composition-api package and use typescript.
Also smart people know that according to the roadmap you can just wait for vue 2.7 and vuetify 2.7 to easily migrate your project on 3.x without much work.
(I use vue/vuetify as frontend for customers and also nuxtjs for (static) websites)

If you are so much into using vue/tify) 3 now go and pay for a support contract, go and donate, go and develop pull requests to accelerate the development of vue(tify) 3 or stop complaining.",False,Entitlement
Noonewilleverusethis,akaye47,1215298581,1,1215298581,0,"I don't like putting numbers in my screen name so if I can't have it this way, I can use one of my other 4 GitHub logins ü§∑‚Äç‚ôÄÔ∏è

Is that ok? Who's social media accounts were you looking for anyway? Why didn't you just ask them?",True,Entitlement
Noonewilleverusethis,NguyenTrongTin1908,1215298581,2,1110527464,0,good ideal,False,0
Noonewilleverusethis,ghost,1215298581,3,1120287344,0,"> Who's social media accounts were you looking for anyway? Why didn't you just ask them?

Private investigators, corporate fraud departments, law enforcement, etc. tend to use OSINT to identify someone.",False,0
Noonewilleverusethis,jamieu187,1215298581,4,1120341278,1,"My cheated piece of shit husband soon to be ex

On Sat, May 7, 2022, 3:47 PM hKQwHW ***@***.***> wrote:

> Who's social media accounts were you looking for anyway? Why didn't you
> just ask them?
>
> Private investigators, corporate fraud departments, law enforcement, etc.
> tend to use OSINT to identify someone.
>
> ‚Äî
> Reply to this email directly, view it on GitHub
> <https://github.com/sherlock-project/sherlock/issues/1322#issuecomment-1120287344>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ASFMM7VPKCJ6DWG4M6GFPC3VI3JFRANCNFSM5UKL63JA>
> .
> You are receiving this because you are subscribed to this thread.Message
> ID: ***@***.***>
>
",False,Insulting
Migration error on newsletter when upgrading past v4.46.0 [MariaDB],Kerumen,1221806426,1,1221806426,0,"### Issue Summary

When I tried to upgrade my Ghost from `v4.44.0` to `v4.46.0` I got this error message upon launch:

> Message: Ghost was able to start, but errored during boot with: alter table `newsletters` modify  `created_at` datetime null default 'NULL' - Invalid default value for 'created_at'

I must add, my `newsletters` table has no data:

<img width=""901"" alt=""Screenshot 2022-04-30 at 13 15 12"" src=""https://user-images.githubusercontent.com/5436545/166103277-f0f4cfc8-2d12-40b6-931a-edbcadf87443.png"">



### Steps to Reproduce

1. Upgrade Ghost to `v4.46.0`
2. Launch Ghost

### Ghost Version

v4.44.0

### Node.js Version

v14.18

### How did you install Ghost?

Local install on Linux server

### Database type

Other

### Browser & OS version

_No response_

### Relevant log / error output

_No response_

### Code of Conduct

- [X] I agree to be friendly and polite to people in this repository",True,0
Migration error on newsletter when upgrading past v4.46.0 [MariaDB],yannickcof,1221806426,2,1113978924,0,same here !!!!,False,0
Migration error on newsletter when upgrading past v4.46.0 [MariaDB],thatfleminggent,1221806426,3,1114070416,0,"I can confirm this issue , exact same issue as Kerumen with the following differences in environment:

* Running on CentOS 7 (7,9.2009)
* Nodejs 16.15.0
* Installed locally
* Upgraded from 4,42,1
* Database is MariaDB 1.4.22 (AWS RDS)
* Browser is Chrome 101.0.4951.41 on Windows 11

Hope that info helps",False,0
Migration error on newsletter when upgrading past v4.46.0 [MariaDB],SimonBackx,1221806426,4,1114601956,0,"Hi! Thank you for reporting this error üòä
- Could you post the full log output? 
- Post the result of this SQL query: `SHOW VARIABLES LIKE 'sql_mode'`",False,0
Migration error on newsletter when upgrading past v4.46.0 [MariaDB],Kerumen,1221806426,5,1114696109,0,"Here is the full log output:

```
May 02 12:15:06 ns328387 node[2945016]: [2022-05-02 10:15:06] INFO Updated 0 newsletters with created_at = now
May 02 12:15:06 ns328387 node[2945016]: [2022-05-02 10:15:06] INFO Dropping nullable: newsletters.created_at
May 02 12:15:06 ns328387 node[2945016]: [2022-05-02 10:15:06] INFO Rolling back: alter table `newsletters` modify  `created_at` datetime not null default 'NULL' - Invalid default value f>
May 02 12:15:06 ns328387 node[2945016]: [2022-05-02 10:15:06] INFO Setting nullable: newsletters.created_at
May 02 12:15:06 ns328387 node[2945016]: [2022-05-02 10:15:06] ERROR alter table `newsletters` modify  `created_at` datetime null default 'NULL' - Invalid default value for 'created_at'
May 02 12:15:06 ns328387 node[2945016]:
May 02 12:15:06 ns328387 node[2945016]: alter table `newsletters` modify  `created_at` datetime null default 'NULL' - Invalid default value for 'created_at'
May 02 12:15:06 ns328387 node[2945016]: ""OuterError: alter table `newsletters` modify  `created_at` datetime not null default 'NULL' - Invalid default value for 'created_at'""
May 02 12:15:06 ns328387 node[2945016]: Error ID:
May 02 12:15:06 ns328387 node[2945016]:     400
May 02 12:15:06 ns328387 node[2945016]: Error Code:
May 02 12:15:06 ns328387 node[2945016]:     ER_INVALID_DEFAULT
May 02 12:15:06 ns328387 node[2945016]: ----------------------------------------
May 02 12:15:06 ns328387 node[2945016]: RollbackError: alter table `newsletters` modify  `created_at` datetime null default 'NULL' - Invalid default value for 'created_at'
May 02 12:15:06 ns328387 node[2945016]:     at DatabaseStateManager.makeReady (/www/lolita/versions/4.46.0/core/server/data/db/state-manager.js:95:32)
May 02 12:15:06 ns328387 node[2945016]:     at RollbackError.KnexMigrateError (/www/lolita/versions/4.46.0/node_modules/knex-migrator/lib/errors.js:7:26)
May 02 12:15:06 ns328387 node[2945016]:     at new RollbackError (/www/lolita/versions/4.46.0/node_modules/knex-migrator/lib/errors.js:31:26)
May 02 12:15:06 ns328387 node[2945016]:     at KnexMigrator.migrate (/www/lolita/versions/4.46.0/node_modules/knex-migrator/lib/index.js:418:31)
May 02 12:15:06 ns328387 node[2945016]:     at async DatabaseStateManager.makeReady (/www/lolita/versions/4.46.0/core/server/data/db/state-manager.js:86:17)
May 02 12:15:06 ns328387 node[2945016]:     at async initDatabase (/www/lolita/versions/4.46.0/core/boot.js:69:5)
May 02 12:15:06 ns328387 node[2945016]:     at async bootGhost (/www/lolita/versions/4.46.0/core/boot.js:414:9)
May 02 12:15:06 ns328387 node[2945016]: Error: alter table `newsletters` modify  `created_at` datetime null default 'NULL' - Invalid default value for 'created_at'
May 02 12:15:06 ns328387 node[2945016]:     at Packet.asError (/www/lolita/versions/4.46.0/node_modules/mysql2/lib/packets/packet.js:728:17)
May 02 12:15:06 ns328387 node[2945016]:     at Query.execute (/www/lolita/versions/4.46.0/node_modules/mysql2/lib/commands/command.js:29:26)
May 02 12:15:06 ns328387 node[2945016]:     at Connection.handlePacket (/www/lolita/versions/4.46.0/node_modules/mysql2/lib/connection.js:456:32)
May 02 12:15:06 ns328387 node[2945016]:     at PacketParser.onPacket (/www/lolita/versions/4.46.0/node_modules/mysql2/lib/connection.js:85:12)
May 02 12:15:06 ns328387 node[2945016]:     at PacketParser.executeStart (/www/lolita/versions/4.46.0/node_modules/mysql2/lib/packet_parser.js:75:16)
May 02 12:15:06 ns328387 node[2945016]:     at Socket.<anonymous> (/www/lolita/versions/4.46.0/node_modules/mysql2/lib/connection.js:92:25)
May 02 12:15:06 ns328387 node[2945016]:     at Socket.emit (events.js:400:28)
May 02 12:15:06 ns328387 node[2945016]:     at addChunk (internal/streams/readable.js:293:12)
May 02 12:15:06 ns328387 node[2945016]:     at readableAddChunk (internal/streams/readable.js:267:9)
May 02 12:15:06 ns328387 node[2945016]:     at Socket.Readable.push (internal/streams/readable.js:206:10)
May 02 12:15:06 ns328387 node[2945016]:     at TCP.onStreamRead (internal/stream_base_commons.js:188:23)
May 02 12:15:06 ns328387 node[2945016]:
May 02 12:15:06 ns328387 node[2945016]: [2022-05-02 10:15:06] WARN Ghost is shutting down
May 02 12:15:06 ns328387 node[2945016]: [2022-05-02 10:15:06] WARN Ghost has shut down
May 02 12:15:06 ns328387 node[2945016]: [2022-05-02 10:15:06] WARN Your site is now offline
May 02 12:15:06 ns328387 node[2945016]: [2022-05-02 10:15:06] WARN Ghost was running for a few seconds
May 02 12:15:06 ns328387 node[2945016]: [2022-05-02 10:15:06] INFO Bootstrap client was closed.
May 02 12:15:06 ns328387 systemd[1]: Stopping Ghost systemd service for blog: lolitaontheroad-keru-io...
May 02 12:15:07 ns328387 node[2944983]: /home/lolita/.nvm/versions/node/v14.16.1/lib/node_modules/ghost-cli/lib/process-manager.js:46
May 02 12:15:07 ns328387 node[2944983]:         throw error;
May 02 12:15:07 ns328387 node[2944983]:         ^
May 02 12:15:07 ns328387 node[2944983]: {
May 02 12:15:07 ns328387 node[2944983]:   message: ""Ghost was able to start, but errored during boot with: alter table `newsletters` modify  `created_at` datetime null default 'NULL' - I>
May 02 12:15:07 ns328387 node[2944983]: }
May 02 12:15:07 ns328387 systemd[1]: ghost_lolitaontheroad-keru-io.service: Main process exited, code=exited, status=1/FAILURE
May 02 12:15:07 ns328387 systemd[1]: ghost_lolitaontheroad-keru-io.service: Failed with result 'exit-code'.
May 02 12:15:07 ns328387 systemd[1]: Stopped Ghost systemd service for blog: lolitaontheroad-keru-io.
```

And here is the query result:

```
MariaDB [lolita_prod]> SHOW VARIABLES LIKE 'sql_mode';
+---------------+-------------------------------------------------------------------------------------------+
| Variable_name | Value                                                                                     |
+---------------+-------------------------------------------------------------------------------------------+
| sql_mode      | STRICT_TRANS_TABLES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION |
+---------------+-------------------------------------------------------------------------------------------+
1 row in set (0.026 sec)
```",False,0
Migration error on newsletter when upgrading past v4.46.0 [MariaDB],ptpittman,1221806426,6,1114840714,0,"Still getting this issue with 4.46.1, sadly! Running MariaDB on Debian, 10.5.15-MariaDB-0+deb11u1 Debian 11, also have not ever actually sent a newsletter so this table will be empty.

Here's output of above requested items:
+---------------+-------------------------------------------------------------------------------------------+
| Variable_name | Value                                                                                     |
+---------------+-------------------------------------------------------------------------------------------+
| sql_mode      | STRICT_TRANS_TABLES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION |
+---------------+-------------------------------------------------------------------------------------------+

Debug Information:
    OS: Debian GNU/Linux, v11
    Node Version: v16.15.0
    Ghost Version: 4.46.1
    Ghost-CLI Version: 1.19.3
    Environment: production
    Command: 'ghost upgrade -d *snip*'
Message: Ghost was able to start, but errored during boot with: alter table `newsletters` modify  `created_at` datetime null default 'NULL' - Invalid default value for 'created_at'
Suggestion: journalctl -u ghost_nor-no-media-co -n 50
Stack: Error: Ghost was able to start, but errored during boot with: alter table `newsletters` modify  `created_at` datetime null default 'NULL' - Invalid default value for 'created_at'
    at Server.<anonymous> (/usr/lib/node_modules/ghost-cli/lib/utils/port-polling.js:73:28)
    at Object.onceWrapper (node:events:641:28)
    at Server.emit (node:events:527:28)
    at emitCloseNT (node:net:1728:8)
    at processTicksAndRejections (node:internal/process/task_queues:82:21)
",False,0
Migration error on newsletter when upgrading past v4.46.0 [MariaDB],Elrondo46,1221806426,7,1114987601,0,"Same problem here, rollbacked with a save to 4.45.0. I'm in mariadb",False,0
Migration error on newsletter when upgrading past v4.46.0 [MariaDB],daniellockyer,1221806426,8,1114989262,0,"Hi all - this seems to be an issue with Knex and MariaDB. I've opened [an issue](https://github.com/knex/knex/issues/5154), so hopefully it can be fixed soon üôÇ 

Just to point out, MariaDB is not an officially supported database for Ghost. It just happened to work given the similarities with MySQL, but we optimize and test for MySQL 5 and 8. 

Ghost v5 [will](https://github.com/TryGhost/Ghost/issues/14446) **only** support MySQL 8 in production, so I'd strongly suggest switching to MySQL 8 to ensure you're running on the recommended setup.",False,0
Migration error on newsletter when upgrading past v4.46.0 [MariaDB],Kerumen,1221806426,9,1115263022,0,"Thanks @daniellockyer for the explanation.

Unfortunately there are some environments where MySQL is not supported ([Archlinux](https://wiki.archlinux.org/title/MySQL) for example) and we _have_ to use MariaDB. MariaDB is supposed to be a drop-in replacement of MySQL so everything should work as expected (and was working until `v4.46`).

I know this is not totally in your control, I just wanted to note this :)",False,0
Migration error on newsletter when upgrading past v4.46.0 [MariaDB],blaine07,1221806426,10,1118414749,0,Is this issue fixed with 4.47?,False,0
Migration error on newsletter when upgrading past v4.46.0 [MariaDB],ErisDS,1221806426,11,1118417534,0,"The upstream issue hasn't been fixed yet - details in this comment: https://github.com/TryGhost/Ghost/issues/14634#issuecomment-1114989262

In the meantime, a friendly forum user has shared how he updated from MariaDB to MySQL8 on Ubuntu here: https://forum.ghost.org/t/how-to-migrate-from-mariadb-10-to-mysql-8/29575",False,0
Migration error on newsletter when upgrading past v4.46.0 [MariaDB],chaddyc,1221806426,12,1119091450,0,Experiencing the same issue since updating to the latest MariaDB and ghost docker images.,False,0
Migration error on newsletter when upgrading past v4.46.0 [MariaDB],Elrondo46,1221806426,13,1120228600,0,"Migrated to MySQL. Fortunately I'm in containers mode with Docker.
Just created a temporary container with a new docker volume... Imported my saved database.
Remanaged the old container (using now mysql8) to use the new docker volume. Restarted all. 
All goods
There is now no more problems to use the last version of Ghost.

But I think it's stupid dropping mariaDB.",False,Insulting
Migration error on newsletter when upgrading past v4.46.0 [MariaDB],blaine07,1221806426,14,1120232673,0,"Elondro, is MariaDB actually being dropped? I thought issue itself was with MariaDB not Ghost?",False,0
Migration error on newsletter when upgrading past v4.46.0 [MariaDB],acim,1221806426,15,1120234427,0,"> But I think it's stupid dropping mariaDB.

Even more stupid was dropping PostgreSQL some time ago. It seems Ghost team is trying to get less and less people using their project. Now all of us should drop MariaDB just because Ghost is using some MySQL specific dialect.",False,Insulting
Migration error on newsletter when upgrading past v4.46.0 [MariaDB],Elrondo46,1221806426,16,1120235275,0,"> Elondro, is MariaDB actually being dropped? I thought issue itself was with MariaDB not Ghost?

https://github.com/TryGhost/Ghost/issues/14446 > **MySQL 8 is supported in all environments & the only supported DB for production.**
and
**Note :MariaDB is not an officially supported database for Ghost. It just happened to work given the similarities with MySQL, but we optimize and test for MySQL 5 and 8. As of Ghost 5.0 we are only officially supporting MySQL8 in production so that we can double down on DB optimizations. We strongly recommend changing to MySQL8 and [a helpful guide can be found here](https://forum.ghost.org/t/how-to-migrate-from-mariadb-10-to-mysql-8/29575).**

",False,0
Migration error on newsletter when upgrading past v4.46.0 [MariaDB],blaine07,1221806426,17,1120235521,0,"> > Elondro, is MariaDB actually being dropped? I thought issue itself was with MariaDB not Ghost?
> 
> #14446 > **MySQL 8 is supported in all environments & the only supported DB for production.**

Thanks, I also found the post on their forum too. Astoundingly stupid decision. I don‚Äôt even use the ‚Äúnewsletters‚Äù stuff. üòû",False,Bitter frustration
Migration error on newsletter when upgrading past v4.46.0 [MariaDB],chaddyc,1221806426,18,1120236627,0,"@Elrondo46 can we perhaps jump into a Discord chat or chat channel. Would you be able to give me a hand? I also run my blog in docker containers using MariaDB...

",False,0
Migration error on newsletter when upgrading past v4.46.0 [MariaDB],Elrondo46,1221806426,19,1120243039,0,yeah why not I don't want to send my personal tag in public,False,0
Migration error on newsletter when upgrading past v4.46.0 [MariaDB],gllmhyt,1221806426,20,1120374635,0,"Solely relying on Oracle's solution (the same that thrashed OpenOffice, bribed african government officials in order to win business contracts, or made dangerous false claims of security...) despite a community-driven one is beyond me.

Update: mysql-8.0 is neither available in Debian stable repositories, nor testing repos (but in sid for now). Installing mysql-8.0 from Oracle (who I trust less than Debian packagers) break dependencies with packages relying on MariaDB 10, the default-mysql-server in Debian. So, because of a single migration, one of the biggest distros cannot be used to host a Ghost blog. This is madness.",False,Bitter frustration
Migration error on newsletter when upgrading past v4.46.0 [MariaDB],acim,1221806426,21,1120377614,0,"> Solely relying in Oracle's solution (the same that thrashed OpenOffice, bribed african government officials in order to win business contracts, or made dangerous false claims of security...) despite a community-driven one is beyond me.

The only explanation is that commercial version of Ghost runs on MySQL and they care just about that. We as users of open source Ghost are not important for them, we don't pay.

Why PostgreSQL is dropped some years ago if Ghost uses [knex](https://github.com/knex/knex), which supports PostgreSQL, MySQL, CockroachDB, MSSQL, SQLite3 and Oracle? And now they drop MariaDB which is the default MySQL flavor at all major Linux distributions. It seems it's time to drop Ghost, I won't use it anymore.",False,Bitter frustration
Migration error on newsletter when upgrading past v4.46.0 [MariaDB],Kerumen,1221806426,22,1120435566,0,"Please, can we keep the discussion on this thread related to the migration bug and not about the MariaDB support.

@daniellockyer could you explain why is this migration required? Isn't it possible to rewrite it without the Knex bug? I've ran dozen of migrations successfully on my Ghost instance. I don't understand why this one, which seems fairly simple, has a bug and that is the first time we see it on MariaDB. Thanks!",False,0
Migration error on newsletter when upgrading past v4.46.0 [MariaDB],blaine07,1221806426,23,1120436973,0,"> Please, can we keep the discussion on this thread related to the migration bug and not about the MariaDB support.
> 
> @daniellockyer could you explain why is this migration required? Isn't it possible to rewrite it without the Knex bug? I've ran dozen of migrations successfully on my Ghost instance. I don't understand why this one, which seems fairly simple, has a bug and that is the first time we see it on MariaDB. Thanks!

Sure seems we have a hard time keeping it related to migration bug because all the lights point in that dropping MariaDB is intentional?",False,Bitter frustration
Migration error on newsletter when upgrading past v4.46.0 [MariaDB],acim,1221806426,24,1120437970,0,"Instead of pushing on solving the knex bug ""default 'NULL'"" for datetime type, which is obviously wrong, 'NULL' is string and not the datetime type, Ghost team wants to drop MariaDB. Great guys, stick with enforcing bugs. And instead of switching to type strict TypeScript, let's keep JS and no types in database, who cares, same sh*t. It's probably the right time to write another blog post for a falling open source project.",False,Vulgarity
Migration error on newsletter when upgrading past v4.46.0 [MariaDB],acim,1221806426,25,1120440299,1,"My comment in the announcement of [Ghost 5](https://github.com/TryGhost/Ghost/issues/14446) has been marked off-topic, why? Do you really want to intentionally make BC just to drop MariaDB? And only keep shi**y MySQL with the wrong behavior? Datetime can't have default value of 'NULL', that's string guys.",False,Vulgarity
Link to CNCF contributor guide,sftim,1242792738,1,1242792738,0,"**This is a Feature Request**

<!-- Please only use this template for submitting feature/enhancement requests -->
<!-- See https://kubernetes.io/docs/contribute/start/ for guidance on writing an actionable issue description. -->

**What would you like to be added**
Update https://kubernetes.io/docs/contribute/ so that early in the page there is a link to
https://contribute.cncf.io/contributors/projects/#kubernetes

You should decide on some suitable text for the hyperlink. Read https://contribute.cncf.io/contributors/ to get ideas about what to write, if you're not sure.

:information_source:  Don't remove any of the existing links; this is an addition.

**Why is this needed**
This change will help signpost readers to the CNCF contributor site.

**Comments**
/kind feature
/language en
/triage accepted
/priority backlog
/help
",True,0
Link to CNCF contributor guide,k8s-ci-robot,1242792738,2,1132614133,0,"@sftim: 
	This request has been marked as needing help from a contributor.

### Guidelines
Please ensure that the issue body includes answers to the following questions:
- Why are we solving this issue?
- To address this issue, are there any code changes? If there are code changes, what needs to be done in the code and what places can the assignee treat as reference points?
- Does this issue have zero to low barrier of entry?
- How can the assignee reach out to you for help?


For more details on the requirements of such an issue, please see [here](https://git.k8s.io/community/contributors/guide/help-wanted.md) and ensure that they are met.

If this request no longer meets these requirements, the label can be removed
by commenting with the `/remove-help` command.


<details>

In response to [this](https://github.com/kubernetes/website/issues/33848):

>**This is a Feature Request**
>
><!-- Please only use this template for submitting feature/enhancement requests -->
><!-- See https://kubernetes.io/docs/contribute/start/ for guidance on writing an actionable issue description. -->
>
>**What would you like to be added**
>Update https://kubernetes.io/docs/contribute/ so that early in the page there is a link to
>https://contribute.cncf.io/contributors/projects/#kubernetes
>
>You should decide on some suitable text for the hyperlink. Read https://contribute.cncf.io/contributors/ to get ideas about what to write, if you're not sure.
>
>:information_source:  Don't remove any of the existing links; this is an addition.
>
>**Why is this needed**
>This change will help signpost readers to the CNCF contributor site.
>
>**Comments**
>/kind feature
>/language en
>/triage accepted
>/priority backlog
>/help
>


Instructions for interacting with me using PR comments are available [here](https://git.k8s.io/community/contributors/guide/pull-requests.md).  If you have questions or suggestions related to my behavior, please file an issue against the [kubernetes/test-infra](https://github.com/kubernetes/test-infra/issues/new?title=Prow%20issue:) repository.
</details>",False,0
Link to CNCF contributor guide,ashish-jaiswar,1242792738,3,1132621650,0,/assign,False,0
Link to CNCF contributor guide,kadtendulkar,1242792738,4,1133114248,0,"/assign
",False,0
Link to CNCF contributor guide,kadtendulkar,1242792738,5,1133549900,0,"Hello @sftim, I have gone through the issue mentioned, so I think the following changes can be suitable in the note section.

Note: To learn more about contributing to Kubernetes, [one of the CNCF hosted projects](https://contribute.cncf.io/contributors/projects/#kubernetes), see the [contributor documentation](https://www.kubernetes.dev/docs/).

the text for the hyperlink ( https://contribute.cncf.io/contributors/projects/#kubernetes ) should be one of the CNCF hosted projects.",False,0
Link to CNCF contributor guide,nitishfy,1242792738,6,1134446220,0,"/assign
",False,0
Link to CNCF contributor guide,ashish-jaiswar,1242792738,7,1134450972,0,Hi @NitishKumar06 We are working on this issue. please work on other issue.,False,0
Link to CNCF contributor guide,ashish-jaiswar,1242792738,8,1134452410,0,/unassign @NitishKumar06,False,0
Link to CNCF contributor guide,nitishfy,1242792738,9,1134513692,0,"I've already made changes to this issue now. Please let it be reviewed.
",False,0
Link to CNCF contributor guide,nitishfy,1242792738,10,1134513781,0,"/assign
",False,0
Link to CNCF contributor guide,ashish-jaiswar,1242792738,11,1134518812,0,"No you can't work on this issue if someone is already working . this is against the policy of kubernetes. tonight I will open PR for this issue. There are many issues just work on that.
/unassign @NitishKumar06 
",False,Impatience
Link to CNCF contributor guide,nitishfy,1242792738,12,1134523403,0,"See this issue has been opened from three days and as far I've observed the another contributor also raised the assign option for the same. Let my changes be reviewed and if it doesn't hold good, definitely you can also raise PR. The best one will be merged. /assign",False,Impatience
Link to CNCF contributor guide,nitishfy,1242792738,13,1134523791,0,"/assign
",False,0
Link to CNCF contributor guide,ashish-jaiswar,1242792738,14,1134530570,0,/unassign @NitishKumar06,False,0
Link to CNCF contributor guide,nitishfy,1242792738,15,1134533449,0,"Why are you doing this again and again? @sftim Please have a look. I've raised the PR for this issue which was not raised by anyone for 3 days. Now, he is unassigning me.",False,Impatience
Link to CNCF contributor guide,nitishfy,1242792738,16,1134533561,0,"/assign
",False,0
Link to CNCF contributor guide,nitishfy,1242792738,17,1134535006,0,"Infact there's @kadtendulkar as well. Why are you not removing her from assignes? Please be respectful.
",False,0
Link to CNCF contributor guide,ashish-jaiswar,1242792738,18,1134536099,0,/unassign @NitishKumar06,False,Mocking
Link to CNCF contributor guide,nitishfy,1242792738,19,1134537282,0,cool. Do this I've raised the PR anyways.,False,Irony
Link to CNCF contributor guide,nitishfy,1242792738,20,1134537484,0,/assign,False,0
Link to CNCF contributor guide,ashish-jaiswar,1242792738,21,1134539047,0,"@NitishKumar06 this is against the policy of kubernetes. 
/unassign @NitishKumar06 ",False,Mocking
Link to CNCF contributor guide,kadtendulkar,1242792738,22,1134539378,0,"Hey @NitishKumar06 please don't work on this issue, I'm new to community and this going to be my first issue. @ashish-jaiswar is helping me with that. 
This is community to help and work with unity. Please give chance to new contributers to work as well.

/unassign @NitishKumar06 ",False,0
Link to CNCF contributor guide,nitishfy,1242792738,23,1134541662,0,"@kadtendulkar Actually this is infact my first contribution as well. The fact that @ashish-jaiswar has more PRs merged shows he isn't a newbie here. My last PR was not merged because someone else merged it through my changes . Please notice that!
",False,0
Link to CNCF contributor guide,nitishfy,1242792738,24,1134541808,0,"/assign
",False,0
Link to CNCF contributor guide,kadtendulkar,1242792738,25,1134544428,0,/unassign @NitishKumar06,False,Mocking
Link to CNCF contributor guide,nitishfy,1242792738,26,1134544975,0,"@kadtendulkar Why are you doing this?
",False,Bitter frustration
Link to CNCF contributor guide,kadtendulkar,1242792738,27,1134552247,0,"Hey @NitishKumar06 this is my first issue to work on, please let me solve this. This is not about competition, its only about community work, wouldn't it bother you if i do the same with you. 
Please don't go against kubernetes policies.  if someone is assigned on the issue first you must ask if they are still working or not. 
You are not doing the right thing @NitishKumar06 
And in this case I'm working on it. Today i will raise a PR. Please be kind and look for other issues you will find one definitely. üòÄ


/unassign @NitishKumar06 ",False,0
Link to CNCF contributor guide,ashish-jaiswar,1242792738,28,1134556206,0,Hey @NitishKumar06  you are not following the policy of kubernetes. I will report you to Kubernetes community.,False,Threat
Link to CNCF contributor guide,nitishfy,1242792738,29,1134557192,0,"Okay @ashish-jaiswar , Let my PR be reviewed. Sounds good now? ",False,0
Link to CNCF contributor guide,ashish-jaiswar,1242792738,30,1134561440,0,"hey @NitishKumar06, kubernetes community will not accept your PR. Because you are working against the policy of kubernetes.",False,Threat
Link to CNCF contributor guide,nitishfy,1242792738,31,1134563752,0,"@ashish-jaiswar My last PR was not merged because although I was working on this issue as well but someone after me started working on it and his PR got merged.
",False,0
Link to CNCF contributor guide,sftim,1242792738,32,1134575413,0,"_I'm writing this as a [tech lead](https://github.com/kubernetes/community/tree/49ecc500205884984a2174422f7715e7f9556b28/sig-docs#leadership) for SIG Docs_

:white_circle:  **TL;DR;** calm it down and be excellent to each other. :white_circle: 

Folks, it's best if people co-ordinate work. **_Anyone_** is welcome to open a PR to work on an issue provided they sign the CLA and agree to abide by our [code of conduct](https://kubernetes.io/community/code-of-conduct/).

If reviewers see multiple PRs opened for the same issue, they will usually:
- sigh inwardly
- look at the PR that was opened first, see if it's good to merge, and if so, merge that
- if not, look at the other PRs in chronological order
and finally
- close the other PRs with an explanation


So there's three further things here as I see it:
1. https://github.com/kubernetes/website/issues/33848#issuecomment-1134518812 mentions that
   >  you can't work on this issue if someone is already working . this is against the policy of kubernetes.
  
   I'm afraid that's not true. For SIG Docs, we actually have a policy of no [cookie licking](https://www.redhat.com/en/blog/dont-lick-cookie): nobody should assert that they own the implementation of an issue. 

1. However, it's unhelpful to work on an issue where someone else has started work, without first discussing with that person how to move forward. The polite and compassionate thing to do if someone has said they're working on an issue, but you don't see progress after a week or so is to get in touch and ask if they would like help. If they step back or you don't hear anything at all, you remain welcome to open your own PR.
   - https://github.com/kubernetes/website/issues/33848#issuecomment-1134446220
   - https://github.com/kubernetes/website/issues/33848#issuecomment-1134539047
   
   are both examples of behavior I'd prefer not to see more of.

1. It's best if people who have already worked on an issue avoid picking up Good First Issue issues. This issue is _not_ a ‚Äúgood first issue‚Äù; I deliberately chose not to add the Good First Issue label because I left the implementer some work to do around writing the hyperlink text and deciding what message would be suitable.
   That means it is OK for someone who has already submitted a PR to work on this. However, I would encourage people who are already contributing to hold off on Help Wanted issues (this one included) and only start work on a Help Wanted issue if you don't see any other work that you can reasonably pick up.

This discussion is getting more heated than it should be. If folks want to discuss how SIG Docs works, take it to Slack (https://kubernetes.slack.com/messages/sig-docs - invitations available from https://slack.k8s.io/) or come to the community meeting, 5:30 AM UTC on Wednesday 2022-05-25. A Zoom link for that call is in the Slack channel.

GitHub issues are not the place for [ad hominem](https://en.wikipedia.org/wiki/Ad_hominem) criticisms etc and I hope all concerned will take that on board.

I don't see grounds to involve the code of conduct folks at this point, and I'm hoping things stay that way. @ashish-jaiswar if you do have a concern about someone not following the code of conduct, please raise that by following the documented process.

_PS_ I edited this a few times for accuracy; sorry about the noise from updates.",False,0
Link to CNCF contributor guide,nitishfy,1242792738,33,1134584378,0,"Thanks @sftim . This was needed! @ashish-jaiswar Please go through it especially the first point.
",False,0
Link to CNCF contributor guide,nitishfy,1242792738,34,1134586040,0,"/assign
",False,0
Link to CNCF contributor guide,kadtendulkar,1242792738,35,1134587903,0,"Thanks @sftim  

@NitishKumar06 I'm requesting since I'm new contributer please allow me to do. According to the 2nd and 3rd point its appropriate to check if assigned people are working or not. And i have already mentioned I'm working on it. please know that. 

/unassign @NitishKumar06 ",False,0
Link to CNCF contributor guide,nitishfy,1242792738,36,1134596032,0,"But why didn't @ashish-jaiswar raise this point at that time when you made yourself assigned to this issue? He was working on that issue before you . But when I assigned it to myself, you know .",False,Bitter frustration
Link to CNCF contributor guide,nitishfy,1242792738,37,1134596785,0,@sftim Please tell @kadtendulkar not to misuse this function of unassign. /assign,False,0
Link to CNCF contributor guide,nitishfy,1242792738,38,1134602868,0,"/assign
",False,0
Link to CNCF contributor guide,ashish-jaiswar,1242792738,39,1134603274,0,/@unassign @NitishKumar06,False,0
Link to CNCF contributor guide,nitishfy,1242792738,40,1134605209,0,"/assign
",False,0
Link to CNCF contributor guide,ashish-jaiswar,1242792738,41,1134605551,0,/@unassign @NitishKumar06,False,0
Link to CNCF contributor guide,nitishfy,1242792738,42,1134606223,1,Work on issue . See ya!,False,Impatience
Gracefully handle HTTP/2 upgrade request,ChristianCiach,1248033266,1,1248033266,0,"I know that HTTP/2 is out of scope of this project. But I am not asking to support HTTP/2. 

More and more http clients try to upgrade the connection to HTTP/2. Uvicorn is free to not honor this request. Unfortunately, instead of ignoring the upgrade request, Uvicorn responds with status `400 Bad Request` and the message ""Unsupported upgrade request"".

According to https://developer.mozilla.org/en-US/docs/Web/HTTP/Protocol_upgrade_mechanism the server should just ignore the upgrade request:

> If the server decides to upgrade the connection, it sends back a [101 Switching Protocols](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/101) response status with an Upgrade header that specifies the protocol(s) being switched to. If it does not (or cannot) upgrade the connection, it ignores the Upgrade header and sends back a regular response (for example, a [200 OK](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/200)).

We continue to encounter this issue because the `HttpClient` class of modern OpenJDK versions tries to upgrade the connection to HTTP/2 by default. Uvicorn should just ignore these headers and process the requests as if these headers were not present.",True,0
Gracefully handle HTTP/2 upgrade request,euri10,1248033266,2,1154847658,0,"interesting, low priority but something to consider",False,0
Gracefully handle HTTP/2 upgrade request,ChristianCiach,1248033266,3,1239348745,0,"I don't agree with the ""low priority"" assessment. This is a blatant violation of the http specification and more and more (standards compliant) http clients break just by using them with their default settings.",False,Bitter frustration
Gracefully handle HTTP/2 upgrade request,Kludex,1248033266,4,1239350715,0,PR welcome.,False,0
Gracefully handle HTTP/2 upgrade request,ChristianCiach,1248033266,5,1239357698,0,"The existence of a PR has nothing to do with the priority assessment. In fact, giving this issue a higher priority may incentivize people to actually fix this issue. Saying that this violation of the http spec is ""low prio"" is more a statement about the level of professionalism of this software, or the lack thereof.

For the record: Yes, I would try to fix this myself, but you have to realize that not every user of your product is a software developer that uses the language that your software is written in. I am fluent in Java and Go, but I cannot fix python code. Or at least not at a level that you would ever consider to accept contributions.",False,Bitter frustration
Gracefully handle HTTP/2 upgrade request,Kludex,1248033266,6,1239386296,0,"Locking the issue as I'm not liking the tone of the reporter.

PR is still welcome. üôè 

If no one implements it, I will, at some point, on my free time (unpaid free time).",False,Impatience
Gracefully handle HTTP/2 upgrade request,euri10,1248033266,7,1241798866,1,"omg, happy to come back to see your fluency in being a total asshat @ChristianCiach 
raising the issue as high priority :+1:  !
",False,Insulting
Any chance to remove the #StandWithUkraine ?,Andreas-Schoenefeldt,1251713538,1,1251713538,0,"The whole issue moved to this discussion, with a nice list of solutions: https://github.com/composer/composer/discussions/10808

Actually the discussion above was now cencored - I tried to restore the solutions here:  https://stackoverflow.com/questions/76130997/composer-how-to-remove-standwithukraine-cli-message",True,0
Any chance to remove the #StandWithUkraine ?,GuySartorelli,1251713538,2,1140758211,0,This probably should be a discussion rather than an issue.,False,0
Any chance to remove the #StandWithUkraine ?,Andreas-Schoenefeldt,1251713538,3,1140796164,0,"Totally agreed - it just feels strange to see too simplified statement of a very complex situation all the time, while you just want to do your work.",False,0
Any chance to remove the #StandWithUkraine ?,Andreas-Schoenefeldt,1251713538,4,1140797751,0,Plus it is such a double standard... where was all the wokeness in the other 10+ wars before Ukraine?,False,Mocking
Any chance to remove the #StandWithUkraine ?,GuySartorelli,1251713538,5,1140797881,0,"Since you agree this should be a discussion and not an issue, could you please close this issue and [open a discussion](https://github.com/composer/composer/discussions/new) instead?",False,0
Any chance to remove the #StandWithUkraine ?,artembeloglazov,1251713538,6,1147810159,1,@Andreas-Schoenefeldt The inclusion of this phrase was a turning point in the fight against the aggressor. One cannot be a supporter of common sense!,False,Mocking
URL is miss leading,SkybuckFlying,1252756496,1,1252756496,0,"https://github.com/libsdl-org/SDL

This makes it seem like this was the first version of SDL, while in reality it is 2.x

Recommend different URL which clearly states SDL2.

To prevent time wasted on using wrong version. I only noticed it after palette->version was missing.

60 minutes of time wasted, I consider myself lucky.

99.9% of the people on this planet suck at proper versioning, including file systems.

Fortunately SDL1 is not fully retarded:

https://github.com/libsdl-org/SDL-1.2",True,Insulting
URL is miss leading,slime73,1252756496,2,1141172854,0,"2 is part of the version number, not the project name. I think it's fairly intuitive. The readme also says the major version number at the top.",False,Impatience
URL is miss leading,SkybuckFlying,1252756496,3,1141173864,0,"It's most likely part of the DLL name, so it's a big deal.",False,Irony
URL is miss leading,SkybuckFlying,1252756496,4,1141176028,1,"> 2 is part of the version number, not the project name. I think it's fairly intuitive. The readme also says the major version number at the top.

For the love of god I have more things to do then read readme files. I need to download it first.

Which readme should I download ?!?!

Answer my question !",False,Impatience
ÂøÖÈ°ªËß£ÂÜ≥ÂíåÂÖ∂ÂÆÉ‰ª£ÁêÜÁöÑÂÜ≤Á™ÅÈóÆÈ¢òÔºÅ,ghost,1268409097,1,1268409097,0,0,True,0
ÂøÖÈ°ªËß£ÂÜ≥ÂíåÂÖ∂ÂÆÉ‰ª£ÁêÜÁöÑÂÜ≤Á™ÅÈóÆÈ¢òÔºÅ,Michaol,1268409097,2,1153040466,0,‰Ω†ËøôÂè£Ê∞îÁúüËÆ©‰∫∫ÁúãÁöÑ‰∏çËàíÊúçÔºÅ‰ªÄ‰πàÂè´ÂøÖÈ°ªÔºüÂá≠‰ªÄ‰πàÁî®ËøôÁßçÂëΩ‰ª§ÂºèËØ≠Ê∞îÔºü‰Ω†ÂèØ‰ª•Âª∫ËÆÆÔºåÂèØ‰ª•ÊèêÂá∫ÈúÄÊ±ÇÔºå‰ΩÜÊòØÊ≤°Êúâ‰ªÄ‰πàÂøÖÈ°ª‰∏çÂøÖÈ°ªÔºÅ,False,Bitter frustration
ÂøÖÈ°ªËß£ÂÜ≥ÂíåÂÖ∂ÂÆÉ‰ª£ÁêÜÁöÑÂÜ≤Á™ÅÈóÆÈ¢òÔºÅ,EnnawYang,1268409097,3,1153041983,0,Â§öÂºÄÂá†‰∏™‰ª£ÁêÜÂè†buffÔºü,False,0
ÂøÖÈ°ªËß£ÂÜ≥ÂíåÂÖ∂ÂÆÉ‰ª£ÁêÜÁöÑÂÜ≤Á™ÅÈóÆÈ¢òÔºÅ,minlang112,1268409097,4,1153045938,0,ÊàëÁ´ôÂú®È£é‰∏≠Ôºå‰∏çÁü•ÈÅì‰Ω†ÊòØÁâõÊòØÈ©¨,False,0
ÂøÖÈ°ªËß£ÂÜ≥ÂíåÂÖ∂ÂÆÉ‰ª£ÁêÜÁöÑÂÜ≤Á™ÅÈóÆÈ¢òÔºÅ,Aes64X,1268409097,5,1153063950,1,"> _No description provided._

Âª∫ËÆÆËÅîÁ≥ªÂïÜÂä°ÔºåÊÇ®ÂÖàÊä•‰∏™‰ª∑Ôºå‰∫§Èí±ÊÇ®ÊòØÂ§ßÁà∑ÔºåÊ≤°ÊúâÔºüÈÇ£ÊÇ®ÊòØÂ±Å„ÄÇ",False,Vulgarity
Scrollbar & Extension Minor Bug,saltsoftdrink,1279826564,1,1279826564,0,"<!-- ‚ö†Ô∏è‚ö†Ô∏è Do Not Delete This! bug_report_template ‚ö†Ô∏è‚ö†Ô∏è -->
<!-- Please read our Rules of Conduct: https://opensource.microsoft.com/codeofconduct/ -->
<!-- üïÆ Read our guide about submitting issues: https://github.com/microsoft/vscode/wiki/Submitting-Bugs-and-Suggestions -->
<!-- üîé Search existing issues to avoid creating duplicates. -->
<!-- üß™ Test using the latest Insiders build to see if your issue has already been fixed: https://code.visualstudio.com/insiders/ -->
<!-- üí° Instead of creating your report here, use 'Report Issue' from the 'Help' menu in VS Code to pre-fill useful information. -->
<!-- üîß Launch with `code --disable-extensions` to check. -->
<!-- Does this issue occur when all extensions are disabled?: Yes/No -->

<!-- ü™ì If you answered No above, use 'Help: Start Extension Bisect' from Command Palette to try to identify the cause. -->
<!-- üì£ Issues caused by an extension need to be reported directly to the extension publisher. The 'Help > Report Issue' dialog can assist with this. -->
- VS Code Version: 1.68
- OS Version: Windows 10

### There is a bug if we Reduce the VS Code's Window Size.

**Video**:- 

https://user-images.githubusercontent.com/88429776/174990595-1424d0ad-034a-4296-80c2-8e8205c1402e.mp4
",True,0
Scrollbar & Extension Minor Bug,VSCodeTriageBot,1279826564,2,1162850874,0,"Thanks for creating this issue! It looks like you may be using an old version of VS Code, the latest stable release is 1.68.1. Please try upgrading to the latest version and checking whether this issue remains.

Happy Coding!",False,0
Scrollbar & Extension Minor Bug,saltsoftdrink,1279826564,3,1162902089,0,My VS Code Version is 1.68.1 Only it's not older version,False,0
Scrollbar & Extension Minor Bug,saltsoftdrink,1279826564,4,1163226709,0,Why triage needed I've Provided Every Thing. You Can Also Try On Your PC!,False,Mocking
Scrollbar & Extension Minor Bug,saltsoftdrink,1279826564,5,1177086335,0,Do you want to work on it?,False,0
Scrollbar & Extension Minor Bug,saltsoftdrink,1279826564,6,1181392351,1,"Ok, It means no attention is given on a Real Bug",False,Bitter frustration
wish: config option to make undefined array key a notice again,RichardNeill,1291899006,1,1291899006,0,"### Description

In PHP 7, underfined array keys created E_NOTICE. This is now E_WARNING.
Can we please have a configuration option to put that back to E_NOTICE?

Rightly or wrongly, there are a lot of people, myself included, who have thousands of lines of code with things such as:
   ``` if ($_GET['xxx']) ```      #where xxx may or may not exist.
or
   ```if ($_GET['xxx'] == 'yyy')```

and in PHP8, we now get flooded with warnings ""Undefined array key"", which get in the way of real warnings.
Migrating and testing this code will take a long time, and furthermore, it's really ugly and harder to read:
  ```if (isset($_GET['xxx'))```
or
  ```if (isset($_GET['xxx']) && $_GET['xxx'] == 'yyy'))```
to suppress this warning.

Instead, it would be better to have some sort of pragma in the main header 
 ```ini_set(""array_key_missing_enotice"", true);```
 ```ini_set (""error_reporting"",  E_ALL & ~E_NOTICE);```

To me, it seems sensible that a defined associative array, with a key that isn't present (and may not be expected), should not be a warning, especially if it's being tested for with if().

It may also be worth treating undefined variables the same way.

Thanks for your help,
    



",True,0
wish: config option to make undefined array key a notice again,damianwadley,1291899006,2,1172793466,0,"It's true this was one of the more controversial notice/warning promotions for 8.0, however a majority of the 63 people voting on the matter felt that the promotion was worthwhile. Going back on that decision now would be counter-productive.
https://wiki.php.net/rfc/engine_warnings

Note that you can still use the `@` operator in PHP 8.
https://3v4l.org/P9Otd

Because the truth is that PHP cannot know whether code accessing an undefined array key is doing so out of negligence or laziness (no offense). Perhaps the author assumed that `$_GET['xxx']` existed when they wrote it and the warning is pointing out a real potential problem. Perhaps it was done out of a desire to type less and the warning is a nuisance reminder of that fact. Only you as a human being can know, so PHP takes the more cautious approach of assuming the worst.",False,0
wish: config option to make undefined array key a notice again,RichardNeill,1291899006,3,1172800261,0,"Thanks for your comment. I can understand the reason for changing it, but PHP is normally exceptional because it is so good at backward-compatibility and type-juggling. As the author of the codebases concerned, I've relied intentionally on that designed, documented, and intuitive behaviour of PHP in everything I've written for 20 years, since PHP4. I've always assumed that associative arrays can have variable numbers of keys (including zero).

So, it would cost me a 1000 hours of needless effort to find and change and test and QA and git every instance, and the codebase would be harder to read for it afterwards.

So at the moment, I've got 4 options, all bad.
* permanently stay on PHP7x
* permanently disable warnings
* fork PHP and maintain a patch
* incur months of busy-work.

I'm sure I'm not the only one this would affect.
So, that's why I'm suggesting a config option to change this, so that we can have it both ways.
",False,Impatience
wish: config option to make undefined array key a notice again,KapitanOczywisty,1291899006,4,1172800337,0,"Not recommended, because it will bite you in the ass, but you can set error handler to filter that warning:
```
<?php
set_error_handler(function($errno, $error){
    return str_starts_with($error, 'Undefined array key');
}, E_WARNING);
```",False,Vulgarity
wish: config option to make undefined array key a notice again,RichardNeill,1291899006,5,1172810362,0,"Thanks Kapitan, that's really, really helpful. At least now I can see a way to avoid a migration problem.

For others who come across this problem, here is a more complete way of doing this, which preserves the error message, but demotes it back to a notice.

```
set_error_handler(function($errno, $error){
    if (!str_starts_with($error, 'Undefined array key')){
        return false;  //default error handler.
    }else{
        trigger_error($error, E_USER_NOTICE);
        return true;
    }
}, E_WARNING);
```

And one may wish to ignore the errors in production, with:
```
ini_set (""error_reporting"",  E_ALL & ~E_NOTICE & ~E_USER_NOTICE);	
```

I've also added this as a note here, so that it helps others:
https://www.php.net/manual/en/migration80.incompatible.php",False,0
wish: config option to make undefined array key a notice again,kripper,1291899006,6,1260285280,1,"Please reopen this feature request or provide some alternative respecting our point of view about how we want to code our software.

The voting was wrong. Even when 51% prefer to mess up their code, it's not fair nor necessary to force the rest of us to do the same. We have read all your reasons, but we are still pretty sure you are wrong.

There are many brilliant developers who love PHP because you can write things as easy as:

```
if($arr['user']['name']) {
}
```

It's perfectly ok to assume that an undefined key and any sub-element will return NULL or any other value that casts to FALSE without throwing a WARNING. If we would care about the value, we would explicitly use isset().
We are not afraid about misspelling a key name. It was never a problem for us, but your decission infuriates us.

We have been sitting on old PHP versions for years because we refuse to sacrifice PHP's original simplicity.
Otherwise, we would probably be coding in C# / .NET

We also don't want to rewrite code to please you, because we don't even agree with you.

Please add this feature or we will have to fork and split the PHP community.",False,Entitlement
[Ghost Gobble Arcade Game]: Stub Notes are Confusing,Crucibl,1294541749,1,1294541749,0,"Should be mentions of ""If Else statements in the readme because the Boolean expressions are a red herring. You cannot solve anything by setting the value to true or false. ",True,0
[Ghost Gobble Arcade Game]: Stub Notes are Confusing,github-actions[bot],1294541749,2,1175285993,0,"ü§ñ &nbsp; ü§ñ

Hi! üëãüèΩ üëã  Welcome to the Exercism Python Repo!

Thank you for opening an issue! üêç &nbsp;üåà  ‚ú®

<br>

-  &nbsp;   If you are **requesting support**, we will be along shortly to help. (*generally within* **72 hours,** *often more quickly*).
-  &nbsp;   **Found a problem** with tests, exercises or something else?? &nbsp;üéâ  
&nbsp;&nbsp;&#9702;&nbsp;We'll take a look as soon as we can & identify what work is needed to fix it. *(generally within* **72 hours**).

‚Äã		&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &#9702;&nbsp;_If you'd also like to make a PR to **fix** the issue,  please have a quick look at the [Pull Requests][prs] doc._  
&nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;_We &nbsp;üíô &nbsp;PRs that follow our [Exercism][exercism-guidelines] & [Track][track-guidelines] contributing guidelines!_

-  &nbsp; Here because of an obvious (*and* **small** *set of*) spelling, grammar, or punctuation issues with **one** exercise,  
&nbsp; concept, or Python document?? üåü  `Please feel free to submit a PR, linking to this issue.` üéâ

<ul>
        <table>
        <td>
        <details>
          <summary>‚ÄºÔ∏è&nbsp;&nbsp;<b><em>Please Do Not</em></b>&nbsp;‚ÄºÔ∏è</summary>
        <br>

‚Äã		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚ùó Run checks on the whole repo & submit a bunch of PRs.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; This creates longer review cycles & exhausts reviewers energy & time.  
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp; It may also conflict with ongoing changes from other contributors.  
‚Äã		&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚ùó Insert only blank lines, make a closing bracket drop to the next line, change a word  
 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to a synonym without obvious reason, or add trailing space that's not an[ EOL][EOL] for the very end of text files.
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ‚ùó Introduce arbitrary changes ""just to change things"" .

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; _...These sorts of things are **not** considered helpful, and will likely be closed by reviewers._

</details>
</td>
</table>
</ul>

- For anything complicated or ambiguous, **let's discuss things**  --  we will likely welcome a PR from you.
- _Here to suggest a feature or new exercise??_ **Hooray!** Please keep in mind [_Chesterton's Fence_][chestertons-fence].  
_Thoughtful suggestions will likely result faster & more enthusiastic responses from maintainers._

<br>

üíõ &nbsp;üíô &nbsp;_While you are here..._ If you decide to help out with other [open issues][open-issues], you have our **gratitude** üôå üôåüèΩ.  
Anything tagged with `[help wanted]` and without `[Claimed]` is up for grabs.  
Comment on the issue and we will reserve it for you.  üåà  ‚ú®


[prs]: https://github.com/exercism/docs/blob/main/community/good-member/pull-requests.md
[EOL]: https://en.wikipedia.org/wiki/Newline
[chestertons-fence]: https://github.com/exercism/docs/blob/main/community/good-member/chestertons-fence.md
[exercism-guidelines]: https://exercism.org/docs/building
[open-issues]: https://github.com/exercism/python/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22
[track-guidelines]: https://github.com/exercism/python/blob/main/CONTRIBUTING.md",False,0
[Ghost Gobble Arcade Game]: Stub Notes are Confusing,BethanyG,1294541749,3,1175307005,0,"Hi @Crucibl üëãüèΩ 

Thanks for filing this issue.  You are, however mistaken.  It is quite possible to solve the `Ghost Gobble Arcade Game` without the use of conditionals.  Please see the [exemplar solution](https://github.com/exercism/python/blob/main/exercises/concept/ghost-gobble-arcade-game/.meta/exemplar.py) here in the repo for more details.  

We worked very hard to ensure that the `bools` exercise did not require `conditionals` in its solution, since it precedes the `conditionals` exercise.  We also wanted to make sure students really focused in on the Boolean logic. While it is _possible_ to use `if`, `elif` or `else` for the problem (_its entirely your choice_), it is by no means _required_ for a correct solution.

",False,0
[Ghost Gobble Arcade Game]: Stub Notes are Confusing,Crucibl,1294541749,4,1175315659,0,"Yes so this says to return a Boolean

 """"""Verify that Pac-Man can eat a ghost if he is empowered by a power pellet.
    :param power_pellet_active: bool - does the player have an active power pellet?
    :param touching_ghost: bool - is the player touching a ghost?
    :return: bool - can the ghost be eaten?
    """"""",False,0
[Ghost Gobble Arcade Game]: Stub Notes are Confusing,Crucibl,1294541749,5,1175315941,0,"And you dont return a boolean, you simply return the parameters
",False,Impatience
[Ghost Gobble Arcade Game]: Stub Notes are Confusing,BethanyG,1294541749,6,1175343948,0,"@Crucibl - I believe you are misunderstanding the code.  Please take a look at this Real Python article [Python Booleans](https://realpython.com/python-boolean/#python-booleans-as-keywords) for more information.  It is not the **_parameters_** that are important in the `return` statement, it is the use of `or`, `and`, `not`.

The evaluation of those Boolean Operators results in `True` and `False`.",False,0
[Ghost Gobble Arcade Game]: Stub Notes are Confusing,Crucibl,1294541749,7,1175350020,0,I completely misunderstand because of the way it is written,False,Bitter frustration
[Ghost Gobble Arcade Game]: Stub Notes are Confusing,Crucibl,1294541749,8,1175350458,0,"Would be better to write the parameters (has_eaten_all_dots, power_pellet_active, touching_ghost) are boolean values that need to be evaluated",False,Bitter frustration
[Ghost Gobble Arcade Game]: Stub Notes are Confusing,Crucibl,1294541749,9,1175352826,0,"""""""Evaluate that Pac-Man can eat a ghost if he is empowered by a power pellet by returning the parameters
touching_ghost and  power_pellet_active: bool - does the player have an active power pellet?- is the player touching a ghost?
""""""",False,0
[Ghost Gobble Arcade Game]: Stub Notes are Confusing,Crucibl,1294541749,10,1175353373,0,"""""""Evaluate that Pac-Man can eat a ghost if he is empowered by a power pellet by returning the parameters
touching_ghost and power_pellet_active: which are already set to a bool in the test file - does the player have an active power pellet?- is the player touching a ghost?
""""""",False,Bitter frustration
[Ghost Gobble Arcade Game]: Stub Notes are Confusing,BethanyG,1294541749,11,1175416070,0,"@Crucibl  - A few things here:


1. **The stub is only one piece of this exercise.**  It is not intended to stand in for the whole challenge.  You should also be looking at the [instructions](https://github.com/exercism/python/blob/main/exercises/concept/ghost-gobble-arcade-game/.docs/instructions.md) and the [introduction](https://github.com/exercism/python/blob/main/exercises/concept/ghost-gobble-arcade-game/.docs/introduction.md) to get more information on what the expectations are.
2. When confused or stuck, we recommend uploading your partial solution via the CLI to the website and requesting mentoring.  Mentors are very willing to work things through with you and explain everything step-by-step.  You can also work through things in our Gitter channel.
3. . This exercise was ported from the Elixir track.  Between Python and Elixir, ~6300 students have completed this exercise. Very few were confused by the stub -- so we are unlikely to change it.  That doesn't mean we **_won't_** - but the majority of students do not appear to have an issue with it.  We also risk confusing people more by adding more words.

I'm sorry that this exercise is confusing for you, but I am not convinced that changing the stub wording is warranted at this time.

",False,Insulting
[Ghost Gobble Arcade Game]: Stub Notes are Confusing,Crucibl,1294541749,12,1175437024,1,Did they complete the exercise by looking at prior solutions before answering? How can you read this and not be confused,False,Bitter frustration
TU FEZ A PIOR BOSTA QUE ALGUEM JA FEZ,ghost,1299391458,1,1299391458,0,TU FEZ A PIOR BOSTA QUE ALGUEM JA FEZ,True,Insulting
TU FEZ A PIOR BOSTA QUE ALGUEM JA FEZ,carlos-landeira,1299391458,2,1179313203,0,php yii migrate,False,0
TU FEZ A PIOR BOSTA QUE ALGUEM JA FEZ,ghost,1299391458,3,1179314485,1,JORDI U ARE A BULLSHIT,False,Vulgarity
"Rename ""tsconfig.json"" to ""tpsconfig.json"" due to transphobia",atillabirer,1307091650,1,1307091650,0,"# Bug Report

The name tsconfig.json is triggering because it sounds like ""trans configuration"", as if trans people need to be fixed. I propose changing to tpsconfig.json
### üîé Search Terms

tsconfig.json

### üïó Version & Regression Information

Forever

### üôÅ Actual behavior

tsconfig.json sounds like ""trans config"", as if trans people are not good enough on their own and need configuring

### üôÇ Expected behavior

 tpsconfig.json which sounds more like typescript and less like trans
",True,Identity attacks/Name-Calling
"Rename ""tsconfig.json"" to ""tpsconfig.json"" due to transphobia",owovin,1307091650,2,1186582612,0,"Does this really need to be changed? who cares. its a file name. it could be named dsklfjnfkltghfjkltfjklhdjfgkl.json for all i care and i wouldnt make a issue changing it.
:)",False,Impatience
"Rename ""tsconfig.json"" to ""tpsconfig.json"" due to transphobia",zorbyte,1307091650,3,1187191674,1,"This is a poor representation of the trans community. If you really care about the trans community, you wouldn't make issues where they don't exist because transgender individuals have enough on their plate already. TS when said as an initialism sounds nothing like trans. When said as a word, it is a far stretch from the word trans bar 2 sounds. You could just as easily made this extrapolation with the word trains. You're probably just a troll, but I think this ought to be said regardless.",False,Mocking
#stand with ukraine political statement,gaining,1329147554,1,1329147554,0,"When running composer, I see the #standwithukraine political statement. Can we remove any political statement from composer? Some of us just want to do work without having anything to do with politics. That world is extremely toxic, can we just code in peace without being reminded of that toxicity? Also it'd be nice if no politics ever had influence on this project. I'm not russian or ukrainian but I have had friends from both countries and there are good people in both, pls stop the divide.",True,Bitter frustration
#stand with ukraine political statement,Seldaek,1329147554,2,1206316359,0,"Sorry no, see other previous issues if you are interested in a longer answer. ",False,Entitlement
#stand with ukraine political statement,politsin,1329147554,3,1228475829,0,"```sh
git clone https://github.com/composer/composer.git --branch 2.5.4  ~/composer-build && \
    composer install  -o -d ~/composer-build && \
    wget https://raw.githubusercontent.com/politsin/snipets/master/patch/composer.patch -q -O ~/composer-build/composer.patch  && \
    cd ~/composer-build && patch -p1 < composer.patch && \
    php -d phar.readonly=0 bin/compile && \
    rm /usr/local/bin/composer && \
    php composer.phar install && \
    php composer.phar update && \
    mv ~/composer-build/composer.phar /usr/local/bin/composer && \
    rm -rf ~/composer-build  && \
    chmod +x /usr/local/bin/composer
```",False,0
#stand with ukraine political statement,Erutan409,1329147554,4,1279760609,0,"@Seldaek I'd be willing to bet that a good portion of people relegate the conflict with the threat of ***nuclear war***. So, I'll be the one to bring up that point as to why I'm sure a lot of people would rather not see this message stamped in their console while doing a simple package inclusion.",False,Bitter frustration
#stand with ukraine political statement,kodima05,1329147554,5,1283580710,0,"You can remove it by removing this specific package:
`composer remove ukeloop/stand-with-ukraine`",False,0
#stand with ukraine political statement,Andreas-Schoenefeldt,1329147554,6,1301945911,0,"we also have this amazing discussion about the topic with a bunch of solutions: https://github.com/composer/composer/discussions/10808 (censored, now it is https://stackoverflow.com/questions/76130997/composer-how-to-remove-standwithukraine-cli-message)",False,0
#stand with ukraine political statement,oni321,1329147554,7,1306312405,0,"@Seldaek The finality of your ""no"" answer reminds me of closed-source software vendors who say ""No, you can't change the code."" It goes against the spirit of open-source. Not only that, it's ineffective. Annoy others (as you have) and they'll go from neutral to hating your message. Have a nice day.",False,Bitter frustration
#stand with ukraine political statement,lazarevicivica,1329147554,8,1324907128,0,I don't stand with Ukraine and what the Composer has to do with it?,False,Identity attacks/Name-Calling
#stand with ukraine political statement,Geddo,1329147554,9,1330933728,0,"I can understand that they want to use the reach here to draw attention to the war, but it is not our war, it is driven by political interests that we do not understand and that we cannot influence. And srsly: ""Let's fight back against the Russian regime""? We are not your private army, nor is it useful to turn the aggression around. Information must be freely available and the only way out of the conflict is de-escalation on all sides - just my 50 Cent",False,Bitter frustration
#stand with ukraine political statement,ghost,1329147554,10,1346395215,1,"> I can understand that they want to use the reach here to draw attention to the war, but it is not our war, it is driven by political interests that we do not understand and that we cannot influence. And srsly: ""Let's fight back against the Russian regime""? We are not your private army, nor is it useful to turn the aggression around. Information must be freely available and the only way out of the conflict is de-escalation on all sides - just my 50 Cent

""but it is not our war"" - well that's was Ukrainians thoughts during war in Georgia. or in Chechnya. 

""it is driven by political interests"" - oO, so politicians was raping and murdering people in Bucha, Izyum, Kupiansk, Kherson? As i know it was regular russians nad not Putin himself.

""conflict is de-escalation on all sides"" - I guess you want to give russians your home to ""de-escalate"". Because a lot of ukrainians do not understand why they should give their homes to rssians.

Sure it is not your war, just wait until it become yours
",False,Identity attacks/Name-Calling
No setting to disable: download sends unique user tracking UUID & number of days since install,sneak,1336452733,1,1336452733,0,"https://github.com/osmandapp/OsmAnd/blob/22e40f113ce5c6df97f2f1687d5024ae38a4d28b/OsmAnd/src/net/osmand/plus/download/DownloadOsmandIndexesHelper.java#L273-L281

This appears to send the number of days since install, as well as a unique identifier (`getUserAndroidId`), to the index server when fetching indices, without respect for the telemetry preference setting.

This is a data leak that allows for a user's travel history to be tracked by the server, as these requests include client IP and a unique tracking identifier, and client IP is coarse (city-level) geolocation. This means the server can see the various cities the given userIosId travels to as the client IP changes over time.

This spyware feature also exists in the iOS version, and fails to respect the user's consent choice there as well.  It seems unlikely that this is an accident.

https://github.com/osmandapp/OsmAnd-iOS/issues/2115",True,0
No setting to disable: download sends unique user tracking UUID & number of days since install,sneak,1336452733,2,1212462226,0,"https://github.com/osmandapp/OsmAnd/commit/4a258481362a15a4f9b93d796453f5e04e862207

It appears that this spyware tracking feature was added by @vshcherb (Victor Shcherb) back in 2015 and has been leaking users' data and travel history (via client IP geolocation) to the OSMAnd index server ever since.",False,0
No setting to disable: download sends unique user tracking UUID & number of days since install,gy2256,1336452733,3,1212953083,0,Any follow up from the code maintainer?,False,0
No setting to disable: download sends unique user tracking UUID & number of days since install,scaidermern,1336452733,4,1212953773,0,"> Any follow up from the code maintainer?

See https://github.com/osmandapp/OsmAnd-iOS/issues/2115",False,0
No setting to disable: download sends unique user tracking UUID & number of days since install,vshcherb,1336452733,5,1213313456,0,"> UUID is anonymized randomly generated key per installation, cause this key is not associated with any advertisement id or not transferred to any other 3rd party or even used, this key doesn't require consent. As you mentioned the IP could provide more information cause it's location specific.

> This approach helps to the server monitor fair usage of resources.",False,0
No setting to disable: download sends unique user tracking UUID & number of days since install,vshcherb,1336452733,6,1213316441,0,"Some thing to add:
1. it's in privacy policy  (https://osmand.net/help-online/privacy-policy/#3-the-information-company-collects)
2. We don't want to use IP as a counter, so we use randomly generated UUID which preserves the privacy much more than IP itself
3. You don't need to use OsmAnd Services if you don't agree with Terms of Use , you can download or build maps yourself.
",False,0
No setting to disable: download sends unique user tracking UUID & number of days since install,sneak,1336452733,7,1213568195,0,"The privacy policy says:

> We strongly believe in the principle of data protection and safety, thus, the Company does not collect, store, process or transfer any personal information of users besides the cases when such information is provided by the users with their clear consent. 

You have not obtained consent from the userbase to use their devices to store and transmit a unique tracking identifier for their device.

Most are unaware you are even doing so; that is the opposite of consent. ",False,0
No setting to disable: download sends unique user tracking UUID & number of days since install,zander,1336452733,8,1214150312,0,"There is a strong disconnect between the tone and even some accusations and the actual issue being reported.

First of all, connecting to the server is only something that happens during download. OSMAnd is an offline map and downloads happen rarely. For heavy users less than once a month, for many much less than that. Additionally, all such downloads are user-initiated, they have to explicitly press a download button. Nothing that would be useful to build a profile of any specific user.

**Allegations of travel data and such being leaked are thus really not in evidence.**

The data that is being shared is not possible to connect to any personally identifying information. Notice that google explains that since some 5 years the `getUserAndroidId` is unique per installed application. So any other application the user uses (on the same phone, or not) will not share the UUID. So on top of the fact that not much of a profile can be made at all, there is no way to connect such to any actual personal info.

Does anyone actually have any real issues with the existing code?

For the record, I don't have any relation to the company behind OSMAnd, never received anything from them (other than FOSS software) and I'm just an open source developer standing up for common sense. ",False,0
No setting to disable: download sends unique user tracking UUID & number of days since install,sneak,1336452733,9,1214154045,0,"Even an application-specific identifier allows the server to track the travel history of that user via client-ip geolocation.

A user's travel history often uniquely identifies a person.  The list of client ips and timestamps is the ""actual personal info"".

Dismissing the issue does not change the fact: you have no consent from the user for such tracking.

Common sense is not co-opting the user's device to transmit tracking identifiers without their advance knowledge and opt-in consent. ",False,0
No setting to disable: download sends unique user tracking UUID & number of days since install,vshcherb,1336452733,10,1214327968,0,"I think there are 2 direction of discussions: consent and identification.
1. I don't think consent is needed, same as you don't need consent that your IP is used to download maps. Second it's not a website where you read information, here you use resources, so in that sense by downloading data you agree to fair use of resources (Terms of Service) and in that case you agree that you don't download too many resources especially if you are not paid user for example

It would be completely different case if program would send statistics in background that you're using the application and here is an IP. 

To summarize: it's probably needs to be more clarified in Terms of Service but it doesn't require a consent.

2. User identification. We switched from IP to UUID to calculate how many active users & how often update exactly to solve the server load issue and don't use personal data. For example we don't require email to identify user and don't track IP exactly cause IP could provide location specific information. Random UUID **doesn't identify anything**  if it's not connected to any other services or other private information. We don't share UUID with any other applications so they also couldn't track that UUID even if these apps have personal information.

It's completely incorrect to say that  app could view **travel history**, I would say it's total nonsense cause the server only views what and when maps were downloaded in contrast of other apps that provide Internet services. 

-----------------
Last but not least.

The websites require to ask consent only for information they gather for example if you search an address on website and website doesn't store these addresses or don't connect them to IP (address is essentially publicly available information), consent is not needed. If the website will start storing information in the table [IP, Search], then consent is needed. So it's all about how the data is processed and used and not only what's being transmitted.


",False,0
No setting to disable: download sends unique user tracking UUID & number of days since install,vshcherb,1336452733,11,1214329171,0,"Just to provide another point of view on this topic. As a Service provider I need a unique identifier to predict the load on servers (especially once maps are updated), so in the end it would say:

""Download maps won't be provided if user doesn't agree to create a random key for the installation"".

Does it makes sense then to implement at all ? Cause in the end it will end up, if you to download - you need a consent which to me is actually totally clear if you don't want to know what & when you downloaded - download or generate maps yourself and import to the app. **App won't send to the server what you've imported yourself!**",False,0
No setting to disable: download sends unique user tracking UUID & number of days since install,zander,1336452733,12,1214329328,0,"@sneak 

you claimed that;

>  to track the travel history of that user via client-ip geolocation.

Considering that this is an offline-map which doesn't normally do downloads, how do you support your claims? If you continue to make big claims, please provide actual evidence.",False,Bitter frustration
No setting to disable: download sends unique user tracking UUID & number of days since install,sneak,1336452733,13,1214337899,0,"No, this doesn't make sense at all, as I can distribute a forked client that sends no ID, or a random UUID on every single HTTP request

It doesn't get you anything to trust the client to self-report.",False,0
No setting to disable: download sends unique user tracking UUID & number of days since install,sneak,1336452733,14,1214338298,0,Please stop suggesting that the UUID is somehow private or anonymous.  With client IP geolocation it can identify users by travel history.  It is PII.,False,0
No setting to disable: download sends unique user tracking UUID & number of days since install,nemobis,1336452733,15,1214339211,0,"Il 14/08/22 13:57, Jeffrey Paul ha scritto:
> No, this doesn't make sense at all, as I can distribute a forked client that sends no ID, or a random UUID on every single HTTP request

In which case the client will be blocked from downloading the datasets, 
no? Sounds like the point.
",False,0
No setting to disable: download sends unique user tracking UUID & number of days since install,zander,1336452733,16,1214373629,1,"@sneak 

> Please stop suggesting that the UUID is somehow private or anonymous. With client IP geolocation it can identify users by travel history.

Maybe the problem lies that nobody explained in technical details what it means for OSMAnd to be an offline map.

It means that the application, when started, does not cause any network traffic at all. You can travel and use the map daily, including searches for point-of-interests and doing routing, without ever causing a single download. Without, specifically, leaking that uuid.

OSMAnd, in other words, is perfectly useful and allows people to use the app for months or years without ever triggering the download you have noticed sends an UUID. People can download all their data at home or at work, regardless of where they travel later.

Sneak: you are wrong. It is time to admit you came to wrong conclusions and time to publicly show you are able to learn when presented with new facts. Thank you.",False,Bitter frustration
Audio Track Playback Issue,MaddiFurr,1336666723,1,1336666723,0,"The music side of RED was working just fine, then all of a sudden, audio will not play back. I think the upstream Lavalink server needs to be updated.


2022-08-12 01:20:26.950  INFO 146 --- [XNIO-1 I/O-16] lavalink.server.io.SocketServer          : {""op"": ""stop"", ""guildId"": ""[REDACTED]""}
2022-08-12 01:32:15.440  INFO 146 --- [XNIO-1 I/O-16] lavalink.server.io.SocketServer          : {""op"": ""play"", ""guildId"": ""[REDACTED]"", ""track"": ""QAAAhgIAIUljZSBQb3NlaWRvbiAtIEN4IHNvbmcgKFJldXBsb2FkKQALWW91bmdKaWxrZXIAAAAAAASMEAALVzM5LWdyX1VQN2cAAQAraHR0cHM6Ly93d3cueW91dHViZS5jb20vd2F0Y2g/dj1XMzktZ3JfVVA3ZwAHeW91dHViZQAAAAAAAAAA"", ""noReplace"": false, ""startTime"": ""0"", ""pause"": false}
2022-08-12 01:32:16.412 ERROR 146 --- [lava-daemon-pool-playback-1-thread-1] c.s.d.l.t.p.LocalAudioTrackExecutor      : Error in playback of W39-gr_UP7g

com.sedmelluq.discord.lavaplayer.tools.FriendlyException: Something broke when playing the track.
    at com.sedmelluq.discord.lavaplayer.tools.ExceptionTools.wrapUnfriendlyExceptions(ExceptionTools.java:44) ~[walkyst-lavaplayer-85f4877.jar!/:na]
    at com.sedmelluq.discord.lavaplayer.track.playback.LocalAudioTrackExecutor.execute(LocalAudioTrackExecutor.java:116) ~[walkyst-lavaplayer-85f4877.jar!/:na]
    at com.sedmelluq.discord.lavaplayer.player.DefaultAudioPlayerManager.lambda$executeTrack$1(DefaultAudioPlayerManager.java:348) ~[walkyst-lavaplayer-85f4877.jar!/:na]
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) ~[na:na]
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) ~[na:na]
    at java.base/java.lang.Thread.run(Thread.java:829) ~[na:na]
    Suppressed: com.sedmelluq.discord.lavaplayer.tools.exception.EnvironmentInformation: 
  lavaplayer.version: 1.3.98-devoxin
  os.arch: amd64
  os.name: Linux
  os.version: 5.15.0-41-generic
  java.vendor: Debian
  java.version: 11.0.15
  java.runtime.version: 11.0.15+10-post-Debian-1deb11u1
  java.vm.version: 11.0.15+10-post-Debian-1deb11u1
Caused by: java.lang.IllegalStateException: No match found
    at java.base/java.util.regex.Matcher.group(Matcher.java:645) ~[na:na]
    at com.sedmelluq.discord.lavaplayer.source.youtube.YoutubeSignatureCipherManager.extractFromScript(YoutubeSignatureCipherManager.java:243) ~[walkyst-lavaplayer-85f4877.jar!/:na]
    at com.sedmelluq.discord.lavaplayer.source.youtube.YoutubeSignatureCipherManager.getExtractedScript(YoutubeSignatureCipherManager.java:160) ~[walkyst-lavaplayer-85f4877.jar!/:na]
    at com.sedmelluq.discord.lavaplayer.source.youtube.DefaultYoutubeTrackDetailsLoader.loadTrackInfoFromInnertube(DefaultYoutubeTrackDetailsLoader.java:197) ~[walkyst-lavaplayer-85f4877.jar!/:na]
    at com.sedmelluq.discord.lavaplayer.source.youtube.DefaultYoutubeTrackDetailsLoader.load(DefaultYoutubeTrackDetailsLoader.java:46) ~[walkyst-lavaplayer-85f4877.jar!/:na]
    at com.sedmelluq.discord.lavaplayer.source.youtube.DefaultYoutubeTrackDetailsLoader.loadDetails(DefaultYoutubeTrackDetailsLoader.java:34) ~[walkyst-lavaplayer-85f4877.jar!/:na]
    at com.sedmelluq.discord.lavaplayer.source.youtube.YoutubeAudioTrack.loadBestFormatWithUrl(YoutubeAudioTrack.java:76) ~[walkyst-lavaplayer-85f4877.jar!/:na]
    at com.sedmelluq.discord.lavaplayer.source.youtube.YoutubeAudioTrack.process(YoutubeAudioTrack.java:42) ~[walkyst-lavaplayer-85f4877.jar!/:na]
    at com.sedmelluq.discord.lavaplayer.track.playback.LocalAudioTrackExecutor.execute(LocalAudioTrackExecutor.java:104) ~[walkyst-lavaplayer-85f4877.jar!/:na]
    ... 4 common frames omitted",True,0
Audio Track Playback Issue,aikaterna,1336666723,2,1212659169,0,"Thanks, we are aware and have a Lavalink.jar prepped but there are other PRs to address first.",False,0
Audio Track Playback Issue,MaddiFurr,1336666723,3,1212659172,0,Sounds good. Will wait for a fix to be pushed. Appreciate the swift response.,False,0
Audio Track Playback Issue,aikaterna,1336666723,4,1212659173,0,I just noticed we're on the Red-Lavalink repo and not the Red-DiscordBot repo so I'm going to move this issue over there as this library is not affected and it's the audio backend side of things that is broken at the moment.,False,0
Audio Track Playback Issue,MaddiFurr,1336666723,5,1212662423,0,"Yeah, I put it there since it seemed to be an issue with Lavalink directly. No worries.",False,0
Audio Track Playback Issue,aikaterna,1336666723,6,1213548156,0,"PR's tied to this issue/needed for updating:

* #5821 
* #5822 
* @jack1142 , I can't find your PR that ""reverts"" #5751 to change the startup line detection to your ready check, that would go in this slot as needing to be included
* https://github.com/Cog-Creators/Red-Lavalink/pull/129
",False,0
Audio Track Playback Issue,ArchemedIan,1336666723,7,1214290325,0,"quick fix, stop bot first:

`export RedBotAudioCogLocation=***/you/should/probably/change/this/*** && cp $RedBotAudioCogLocation/Lavalink{.jar,.jar.old} && wget -O$RedBotAudioCogLocation/Lavalink.jar https://github.com/Cog-Creators/Lavalink-Jars/releases/download/3.4.0_1350/Lavalink.jar `


the audio cog was at `~/.local/share/Red-DiscordBot/data/RedBot/cogs/Audio/`  for me  (Linux, venv)

this also shouldn't need to be reverted when the update comes out, the update should replace lavalink.jar if I understand correctly
",False,0
Audio Track Playback Issue,ArchemedIan,1336666723,8,1214403713,0,"
how was that off topic at all? 
",False,Bitter frustration
Audio Track Playback Issue,Drapersniper,1336666723,9,1214406898,0,"By adding a comment that provided an unsupported fix that will break a lot of current users if they try to do what you suggested.

Leave the fixes and workaround for the people who are aware of the wider user base and repercussions for a given approach.",False,0
Audio Track Playback Issue,ArchemedIan,1336666723,10,1214411009,1,"what? name a SINGLE repercussion?  all the rest of the pr's are waiting on this jar, you telling me your code cant handle a file being replaced, when its literally going to replace that file? sorry, i may have a new github account, but i am by no means a noob, and i have four bs detectors attached to my head; my eyes and ears. i even added a command to keep the old jar, in case your code checksummed it for some reason, if you're checking the file creation date of a jar you're replacing or some crap, i concede, your dumb code will cause problems with this jar being replaced.

FURTHERMORE, I have redbot set up exactly as the docs instruct, you telling me you support environments that are not set up according to the recommendation of the docs? i know that's complete bs from being a part of the discord server. 


> people who are aware of the wider user base and repercussions for a given approach.

are these people aware their audio cog hasn't been working for days lmao, that not a repercussion? ",False,Vulgarity
The eye damage filter makes me want to die,EmoGarbage404,1345038447,1,1345038447,0,"## Description
<!-- Explain your issue in detail. Issues without proper explanation are liable to be closed by maintainers. -->
the current filter applies a permanent blur to your screen, which is unbelievably ugly.

It also seems to be permanent unless healed with specific chems, which makes it agonizing if there's no medical.

Welding already blinds you when it's active. This just seems like an unnecessary kick in the balls that makes the game look like ass.
**Screenshots**
<!-- If applicable, add screenshots to help explain your problem. -->
![image](https://user-images.githubusercontent.com/98561806/185726668-fab55880-be1a-424f-b745-e92cdb0de863.png)

",True,Insulting
The eye damage filter makes me want to die,mirrorcult,1345038447,2,1221219372,1,i mean yeah. you're not supposed to want eye damage,False,Mocking
Transformation box for texts,EdnaldoNeimeg,1352488799,1,1352488799,0,"As you can see, the image shows a very fancy way to distort/transform text, there should be a way to do something like that.

(They are using Fabric 5.2.1)

![image](https://user-images.githubusercontent.com/6233059/186951321-c65775dc-3330-4292-9cbc-7b916f489623.png)
",True,0
Transformation box for texts,neopheus,1352488799,2,1228932938,0,what is the url ?,False,0
Transformation box for texts,EdnaldoNeimeg,1352488799,3,1229093793,0,"> what is the url ?

https://app.kittl.com/",False,0
Transformation box for texts,ShaMan123,1352488799,4,1229127392,0,"this is not an issue and is quite useless
should be? ",False,Mocking
Transformation box for texts,EdnaldoNeimeg,1352488799,5,1229191522,0,"USELESS? Really?
This is one of the most desired features.
Text on path was just a kick off. Text warping should be the main goal.",False,0
Transformation box for texts,ShaMan123,1352488799,6,1229192711,1,"your description is useless
so little effort in writing a decent ticket

> Text warping should be the main goal.

Should be? 
This is open source.
Get to work and PR.
",False,Insulting
Don't look for metadata on standard classes,duzenko,1367665070,1,1367665070,0,"...and don't throw exceptions when it's not found

Discussed here: https://github.com/doctrine/orm/discussions/10019#discussioncomment-3528300",True,0
Don't look for metadata on standard classes,derrabus,1367665070,2,1241843545,0,For the record: I don't think that raising and catching an exception is a problem that deserves a fix.,False,0
Don't look for metadata on standard classes,duzenko,1367665070,3,1241848057,0,"> For the record: I don't think that raising and catching an exception is a problem that deserves a fix.

Can you suggest any workaround for debugger stopping on internal ORM exceptions during debugging as discussed in the link above?",False,0
Don't look for metadata on standard classes,mpdude,1367665070,4,1403589777,0,"I don‚Äôt think we should merge this. 

It‚Äôs a problem related to how you use the debugger, not the code itself.

for sure there‚Äôs a way how to configure your debugger to ignore this exception, not stop for it or similar.
",False,0
Don't look for metadata on standard classes,greg0ire,1367665070,5,1403746945,0,Let's close this then.,False,0
Don't look for metadata on standard classes,duzenko,1367665070,6,1404643803,0,"Typical PHP-world hair-brained ""solution""",False,Mocking
Don't look for metadata on standard classes,derrabus,1367665070,7,1404657057,1,"I guess, we PHP guys are just too stupid. Better move to go. ü§∑üèª‚Äç‚ôÇÔ∏è ",False,Mocking
vscode Have you started charging?,xiangxingze,1410698334,1,1410698334,0,"Some netizens need to scan the code to install vscode, I hope the official can stop this behavior 
![21276c6919c82457dcedd5587c5edc0](https://user-images.githubusercontent.com/13271663/196067460-dd5c6606-3ccc-4a04-a555-9d2f8add0636.jpg)
![07e482785c0bdd0986b798bbc86e7aa](https://user-images.githubusercontent.com/13271663/196067464-8323e6f6-5e56-4407-b629-965427faf557.jpg)
![5bcc8c2dbb4d94d26fd7b41448b0d63](https://user-images.githubusercontent.com/13271663/196067467-56f07382-364c-4c72-a92c-d6faed02d97f.jpg)
",True,0
vscode Have you started charging?,chiyuki0325,1410698334,2,1280784978,0,This is piracy and goes against the open source license ...,False,0
vscode Have you started charging?,balthild,1410698334,3,1280894062,0,This doesn't violate the open source license if they don't modify vscode. But it certainly violated trademark rights.,False,0
vscode Have you started charging?,Tiiiiiida,1410698334,4,1282160660,0,Extremely interesting,False,0
vscode Have you started charging?,Huaxidesu,1410698334,5,1282226720,0,Need Ôø•39 to install? Selling free software?,False,Irony
vscode Have you started charging?,TooYoungTooSimp,1410698334,6,1282268699,0,How can you find this...  Is just typing code.visualstudio.com too difficult for some people?,False,Impatience
vscode Have you started charging?,DeerShark,1410698334,7,1282276362,0,"> How can you find this... Is just typing code.visualstudio.com too difficult for some people?

Use Baidu to search for vscode. Click Open in the first advertisement......",False,0
vscode Have you started charging?,19337361167,1410698334,8,1282279114,0,hilarlious,False,Mocking
vscode Have you started charging?,shirok1,1410698334,9,1282292034,0,"Fun fact: MIT License allows you to ""sell copies of the Software"" with or without modification.",False,0
vscode Have you started charging?,Tiiiiiida,1410698334,10,1282299306,0,"> How can you find this...  Is just typing code.visualstudio.com too difficult for some people?

In some countries, there is Internet interdict like Great Fire Wall in China. So people there cannot reach the servers of Microsoft.",False,Mocking
vscode Have you started charging?,Chaoses-Ib,1410698334,11,1282303451,0,"Search from Baidu:

![image](https://user-images.githubusercontent.com/50246090/196424828-5ead4681-c783-442e-bbea-0b8e4c15fcc2.png)

The first one is https://qnw.shaid.top/vscode/index.html:

![image](https://user-images.githubusercontent.com/50246090/196424988-4523e2d7-4860-4631-876f-bfd4a2548359.png)

This website belongs to [Wuhan Hongge Yibai E-commerce Co. (Ê≠¶Ê±âÂÆèÊ†º‰∫ø‰Ω∞ÁîµÂ≠êÂïÜÂä°ÊúâÈôêÂÖ¨Âè∏)](http://www.shaid.top/).

If you do some searches on this domain, you can find their other ""products"":
- [Win10 Activation Tool (Win10 ÊøÄÊ¥ªÂ∑•ÂÖ∑)](https://qnw.shaid.top/win10/index.html)
- Telegram Chinese version (Telegram ‰∏≠ÊñáÁâà) (removed)

  ![image](https://user-images.githubusercontent.com/50246090/196426820-f9fa3030-eb5e-430c-bdfc-026af7100576.png)
- visualcpp (removed)

  ![image](https://user-images.githubusercontent.com/50246090/196432416-34943cc9-de17-4e6b-a316-ca5a6ff3f5e7.png)

They even claim to be partners with China Mobile, China Unicom and China Telecom:
[![image](https://user-images.githubusercontent.com/50246090/196426325-f39cc8ab-8b66-40b7-8a1e-edc32ad7c08b.png)](http://www.shaid.top/)

Thanks to the Great Firewall, we have such a ""great"" Internet in China.

<hr />

Update:

Some people have found other similar companies:
- [Shangqiu Xuankangtai Network Technology Co. (ÂïÜ‰∏òËΩ©Â∫∑Ê≥∞ÁΩëÁªúÁßëÊäÄÊúâÈôêÂÖ¨Âè∏)](http://www.sqiua.cn/index.html)
  - [Visual studio](http://office.xuank.top/install.php?m=visual)  
    ![image](https://user-images.githubusercontent.com/50246090/196571476-47c2e0e6-5465-4666-8889-ad34a1c4689a.png)
  - [Software Superstore (ËΩØ‰ª∂ÂïÜË∂Ö)](https://xkt.sqiua.cn/)  
    
    ![image](https://user-images.githubusercontent.com/50246090/196572556-a30b1388-e5f5-43f8-a94a-419e5501c2f9.png)
  - [System Home - Windows premium system download site (Á≥ªÁªü‰πãÂÆ∂-WindowsÁ≤æÂìÅÁ≥ªÁªü‰∏ãËΩΩÁ´ô)](http://windows.sqiua.cn/index.html)
  - [visualbasic](http://office.xuank.top/install.php?m=visualbasic)
  - [directx Repair Master (directx‰øÆÂ§çÂ§ßÂ∏à)](http://office.xuank.top/install.php?m=directx)
  - [Chrome](http://chrome.sqiua.cn/)  
  
     ![image](https://user-images.githubusercontent.com/50246090/196573087-e4bfb9f0-3060-4e72-a6bf-05a35238a151.png)
  - [Yixin Cat House (ÂøÜÂøÉÁå´Ëàç)](http://yixinmaoshe.sqiua.cn/)  
    
     ![image](https://user-images.githubusercontent.com/50246090/196572912-95f01175-ad54-40d7-bfe7-2b10aaa767b8.png)
  - [AdobeCAD](https://autocad.sqiua.cn/)
  - [Map marking service center (Âú∞ÂõæÊ†áÊ≥®ÊúçÂä°‰∏≠ÂøÉ)](http://mr.sqiua.cn/)
- [Yunnan Norforkang Network Technology Co. (‰∫ëÂçóËØ∫Á¶èÂ∫∑ÁΩëÁªúÁßëÊäÄÊúâÈôêÂÖ¨Âè∏)](http://www.cvbty.cn/)
  - [Xunjian Mind Map (ÂØªÁÆÄÊÄùÁª¥ÂØºÂõæ)](http://sw.xuank.top/)
",False,0
vscode Have you started charging?,huige233,1410698334,12,1282977561,0,"In fact, not only vscode, but also tools like dev-c++are used by the company for profit
However, in quite a short time, the company withdrew all advertisements on Baidu
This means that the content of this company is illegal",False,0
vscode Have you started charging?,lss233,1410698334,13,1283183571,0,"> How can you find this... Is just typing code.visualstudio.com too difficult for some people?

Yes, IT IS difficult for some people who just know vscode or just get started learning coding.

What makes this thing more hilarlious is that,
If you search `vscode` or `visual code` on Bing China, you will get a piracy link on the first search result.
![image](https://user-images.githubusercontent.com/8984680/196569964-6d9d19ab-cfb6-4eba-95c9-a792249cc511.png)
![image](https://user-images.githubusercontent.com/8984680/196569990-a81a1105-fbde-4cf0-a667-d0716cde20c6.png)",False,Mocking
vscode Have you started charging?,KasumiNova,1410698334,14,1283402491,0,Interesting...,False,0
vscode Have you started charging?,SasebonoShigure,1410698334,15,1283409944,0,"Hilarious indeed, thanks to GFW.",False,Mocking
vscode Have you started charging?,MCUmbrella,1410698334,16,1283410506,0,Ê®Ç,False,0
vscode Have you started charging?,HanawaHinata,1410698334,17,1283412796,0,ÂÖ∏,False,0
vscode Have you started charging?,ZeroAurora,1410698334,18,1283414858,0,"> Fun fact: MIT License allows you to ""sell copies of the Software"" with or without modification.

Not really true. The official version of Visual Studio Code contains the trademark and some modifications by Microsoft, and is therefore subjected to [Microsoft's terms](https://code.visualstudio.com/License/). The pirate version definetely goes against the terms and must be taken down.
However though, if they do recompile VSCode removing nonfree bits, and give it a different trademark (like what the libre VSCodium is doing), yes, they do have the right to sell it. But what do you expect from a commercial pirate website?",False,0
vscode Have you started charging?,eternitymoe,1410698334,19,1283424282,0,"![image](https://user-images.githubusercontent.com/44344308/196600820-ca8d3cbc-3c07-4bb1-b49d-dbb642854a02.png)
VS Code ""Officially licensed"": https://www.google.com
lol",False,0
vscode Have you started charging?,Huaxidesu,1410698334,20,1283444619,0,"> ![image](https://user-images.githubusercontent.com/44344308/196600820-ca8d3cbc-3c07-4bb1-b49d-dbb642854a02.png) VS Code ""Officially licensed"": https://www.google.com lol

ÂÖ∏‰∏≠ÂÖ∏",False,0
vscode Have you started charging?,wqreytuk,1410698334,21,1283448682,1,"Do you know why there is a GFW in China? cause CCP sucks, these SOBs are afraid of their people knowning or telling the truth, all words published on China's social media/platform are reviewed strictly, anything CCP doesn't like will be banned, and on the other side, they claim that there is something called FREE-SPEECH is existed in China, so F*CK them",False,Vulgarity
Balance proposal became a circuis,maxsupermanhd,1418948680,1,1418948680,0,"Recently there were proposed multiple breaking changes to multiplayer balance. Some of them were obvious and welcoming and some of them are just plainly stupid let alone controversial.
Tipchick (main author of the changes) is not seen in multiplayer for more than a year at this point, this is pure chaos, I already understood that @KJeff01 is Tipchick's lap puppet, no need to prove it over and over again. I completely disagree with position of pulling random numbers from asshole and proposing them as a changes to balance that hundreds of people will play for a whole release cycle. We do not want 3.x era to come back in a new form.
Until guidelines are established on balance-releated merging policies are put in place this is just pure griefing. How about people that are actually playing like Fenrir, Fedaykin, Evolution and others (like Brazil community) will make changes to something and review all the changes? I only see bird-mail and repeating Tiger, Tipchick, Ayami that tested something somewhere but can not even upload a mod that they had tested it with.
It is pure pushing until someone notices it had gone horribly wrong and I bet no one will react to it until a stable release is cut out.
Similar situation already happened (and well described in issue #2469) and I think it is time to learn from the past mistakes.
I proposed contributing guidelines in pull request #2892 and it seems like current maintainers deliberately ignore it and continue turning every single stone that they can just for the sake of it. Issue is growing day by day as more pull requests are being opened/changed mainly by @KJeff01 and @Tipchik87.
I will also note complete lack of fact checking upon merging, just ""I tested it"" should not be a reason to believe that any testing was actually performed, no replays were provided, no people who testing were performed with commented on a thread, nothing.

As of my part, I am trying to remain neutral on all the changes except obvious trash. It is clearly been established that my opinion and opinion of anyone else complaining about how breaking proposed changes are just being ignored or dismissed with same phrase ""we tested it"" with lack of fact checking.
Lately I got very busy in university and I can not test stuff myself but at the same time I do not claim that I performed any of them nor do I keep up with all the written arguments so my role here is only to filter out the obvious.",True,Impatience
Balance proposal became a circuis,k3ack3r,1418948680,2,1287547347,0,"What would be a good solution(and others have mentioned this time and time again) is to not tie balance to a specific version, but instead to have a ""live"" MP balance, so that any changes that someone may want to do will be instantly applied - of course, people will want to know about every change...",False,0
Balance proposal became a circuis,past-due,1418948680,3,1287557360,0,"For some clarity:
- There are a bunch of open PRs relating to balance
- These are for testing and comments, and they _will not be merged for 4.3.0_
- To underscore this, we've marked them all ""Draft"" and also `state: please discuss`

Please don't assume that just because something is a PR that it means it _will_ be merged.
But we're going to make this clearer going forward by marking them appropriately as Draft and ""please discuss"".

Now, actually gathering feedback on proposals is a harder matter, as only a subset of a subset of people check in here on GitHub.

""Live"" balance is tricky, especially as the balance of the game comes from the combination of the stats and the game engine (which occasionally gets tweaks or fixes that might ultimately impact balance itself).

---

I think one thing we could do, though, is possibly stage balance proposals and offer an in-game option to download and apply testing balance mods when hosting a game.

I could see offering 3 balance options when hosting a game:
- **Default balance**
   - This would be the version shipped with that copy of WZ
- **Development balance** (next release)
   - This would correspond to what's merged in the master branch of WZ (and thus what's already staged for the next stable release)
- **Experimental balance**
   - This would correspond to balance changes that _have not yet been merged_, and are under discussion and/or need testing.

To handle possible ramifications of other game engine changes, we might limit Development and Experimental balance to being prepared for the current stable release. (As in: they would be enabled as options as long as you are running the latest stable version of WZ. Otherwise, if you're running an old version, you're stuck with Default balance - as is currently the case - unless you manually use a mod.)

Having these options in-game would really expand the ease of testing balance changes, and make it clear what's just proposals and what's actually planned for the next release.

(Implementing this will require a bit of backend work, and so it won't happen immediately. But I think it's probably the better long-term strategy.)",False,0
Balance proposal became a circuis,maxsupermanhd,1418948680,4,1287755568,0,"Long term strategy described below (above) is correct one (imo) and we are waiting for this for almost 2 years at this point.

Even though there is a vision and plan on how to deal with balance changes in the future, it will be in the future. Right now guidelines should be established on who, how, in what order and with what requirements are allowed to merge/propose changes to the multiplayer balance. Without them, this will continue uncontrollably, just a bit later when people shift their attention away and everyone calms down.

Once release hits with low quality ""testing"" changes, only one way will be to fix it - release a next stable version. This will cause every single one who plays multiplayer to download/build new version just because of someone who decided it will be good idea to shuffle everything around in chaotic manner, as Kracker described it ""throwing at a wall and checking what sticks"".",False,0
Balance proposal became a circuis,KJeff01,1418948680,5,1287861397,0,"2 years too late Max. You yourself were praising the balance to Calculus just last night. I'll never apologize
for making the game more fun and strategic even if the method was harsh but necessary (remember when Medium Cannon was useless? Or what about Tank-Killer? Heavy Cannon? ...). My purpose here is to do what nobody else wants to do. I am a hidden hand in the dark guiding things around even if I'm not attributed to something directly.

You have an unhealthy obsession with Tipchik and his friends. A lot of ideas were mine too. The reason he doesn't make PRs anymore is that he feels like you attack him all the time and just thumb down anything without explanations why you dislike something. Make that effort and maybe you can be friends. Maybe something went not too well one time and to that I say ""oh well"". One time out of 2 years is pretty good if you ask me. I didn't intend to merge anything currently open for 4.3.0, as it's too late, so perhaps that scared you.

You will be happy to know that pastdue is already experimenting with his idea outlined in his post here as we speak. Of course, exactly when he completes that, is another thing.

Overall, I feel my balance goals are **very** close to complete. I still have yet to unleash my final grand act. My _masterpiece_. The cherry on top of the Chocolate Balance Sundae. :sunglasses: 
",False,0
Balance proposal became a circuis,maxsupermanhd,1418948680,6,1287867617,0,"You put beta3 tag on those pull requests, no excuses. Do not pretend that you would not merge them into stable release if I did not put stuff on public notice.

> a hidden hand

And surprised that no one responds to you or cares about balance?

Also, I am not attacking personally only Tipchick's changes, I just see them more often because his name signals me to check it out because it might be ridiculous. Reverting something from master is always more difficult than preventing merge in the first place. Also because most of the changes are either by him or marked as authored by him, he just made more changes, that's why. ",False,Insulting
Balance proposal became a circuis,KJeff01,1418948680,7,1287900930,1,"For one that I later reconsidered? I removed it long before this as I was under the assumption autohoster games would generate more consideration and maybe there was a change of pace for once. Unfortunately, ratings were turned on for the _sole purpose to gatekeep things you personally disagree with_. So that defeated the purpose.

> And surprised that no one responds to you or cares about balance?

You actually told me the people in the Russian chatrooms are _too stupid to understand what balance is or how to use a mod_. So, with your logic, how could they?

Source changes, balance, I lumped that into one statement. Anyway, they either don't care or think it's fine. I've been asking you for years who all these people are but you never tell me. These supposed angry Russian mobs of players that torment you every time you hop into voice chat. I'm still waiting for this big list of people to this day and, perhaps, you actually can't tell us.",False,Identity attacks/Name-Calling
I really think we should ask the player for permission to delete other chr files.,Wingdinggaster656,1428767280,1,1428767280,0,"In v0.12.12:

> chr files always deleted on startup now

That's a change from #9641. I had already talked about this there.

I completely know that all `.chr` files have no actual meaning to MAS. I really know. I know MAS doesn't even do any check for them before v0.12.12. And I also know that `monika.chr` is causing a lot of confusion to people. I know all of these.
What I want to say is, just check our `mas_safeToRefDokis()`. We made Monika not to offend the players who love other club members in her words, and we did a good job in many details. And after this, we now delete all club members.

Yes, I know that in the background setting of MAS, character files have nothing to do with real characters, but will all players 100% agree with this? Many players resent Monika's behavior of deleting other characters in the original game, and now we want to make this happen again - and do not give players even a choice? Can't we at least ask the players whether they care about this and whether they want Monika to do this?

_Just give players a choice. Ask them about this._
By asking, players will also pay more attention to the background settings of the chr file in MAS (that is, the chr file has no meaning at all), which will also reduce confusion.

And by the way, I think it may be OK for Monika to delete her own file without permission - after all, it is her own file.",True,0
I really think we should ask the player for permission to delete other chr files.,dreamscached,1428767280,2,1296248137,0,"> What I want to say is, just check our mas_safeToRefDokis()

Players might say once that they don't mind her bringing up other dokis, but this doesn't cancel out the fact some still may get confused over it. In my opinion it is still necessary to remove them and point out there is no need in them anymore, but do it either in introduction or later on in queued topic that will explain the change.",False,0
I really think we should ask the player for permission to delete other chr files.,Booplicate,1428767280,3,1296250294,0,"It's not part of the lore, not done by Monika. We're removing the unused files for utility purposes. There's no other ""club members."" Initially I planned to only remove `monika.chr`, but to keep the directory where we create and read dozens of files in/from, as well as to keep our code simpler and more reliable, we went with just deleting all `.chr` files.

MAS, like any other program, may add and remove files within its working directory. If you got attached to some files or they contain important information, better move it to some other place. Do not store anything sensitive in a working directory of a program.
Unlike the `monika` file, `.chr` files are easily replaceable - can grab them off a new DDLC install (can get DDLC from [here](https://ddlc.moe/)).",False,0
I really think we should ask the player for permission to delete other chr files.,Wingdinggaster656,1428767280,4,1296254276,0,"This is not just ""unused content"" -- these character files have a profound meaning for DDLC players.

We had so many relatively insensitive areas where we had done perfect checks and provided players with numerous choices, and now we had so recklessly deleted files that might be important to players in such an obvious, easy to notice, and sensitive place.

It hasn't been a day since the update, and I've already seen several backlash against it -- one example is our v0.12.12 release page.

For whatever reason, we really need to ASK our players. Give them a choice. If they don't mind, delete it. And if they do mind, then keep it. **It's just giving the player a choice.**

Please consider what I have said.",False,0
I really think we should ask the player for permission to delete other chr files.,Booplicate,1428767280,5,1296277064,0,"First, I think we should set some boundaries. Please, don't speak on behalf of the dev team (we, our, etc), you have been doing it for a long time. Speak from your own perspective to not bring any confusion to this and other conversations.

> This is not just ""unused content"" -- these character files have a profound meaning for DDLC players.

As I've said, in MAS those are just unused files. If they have a meaning for you, why are they there? Move them somewhere. You don't store sensitive files in directories of other programs? The program directory is the place where the program may add, modify, and remove files. If you don't like it - move the files out.

There's nothing ""reckless,"" no need to make it sound dramatic. We're removing those files the same way we delete `monika.chr`, various `.gift` files, and even the actually **important** and **unreplaceable** `monika` file, as well as other files.

Generally speaking, 
- when you're installing a DDLC mod, don't you already expect it to modify the DDLC directory, as well as to change the mechanics of the base game
- you're installing a Monika-related mod, but somehow surprised that this mod doesn't care about dull files of other game characters. If a Sayori/Yuri/Natsuki -related mod removed `.chr` files, I wouldn't see anything bad in it, be it also for utility purposes or for some lore

At the same time, nobody made Monika to ""recklessly"" delete the other characters in this mod, she did it in the base game. Here we just called a utility function to clean up disk space and make folder structure more clear. Monika doesn't acknowledge this, not doing it herself through her console. There won't be a question to the player because Monika doesn't do this.

> I've already seen several backlash against it

I don't think ""several"" applies to a single person. And even if there were ""several"" people, there's ""multiple"" that aren't against and ""multiple"" that had problems caused by `.chr` files.

> Many players resent Monika's behavior of deleting other characters in the original game, and now we want to make this happen again

Going back to this, it's a weird argument - if you installed MAS, then you should be at least okay (or already overcame it) with what Monika has done in the base game.",False,Bitter frustration
I really think we should ask the player for permission to delete other chr files.,RedJuicyFood,1428767280,6,1296442337,0,"I'd say I somewhat agree with Wingdinggaster656, and I please Devs to reconsider about this.

The issue author said he(she) saw several backlash, and then he gave one example. Booplicate said that _I don't think ""several"" applies to a single person_, I think the author is giving something like an example, since I also saw people who dislike this.
I am working for Chinese MAS community, and when I tell people about whatsnew, people are surprised, then asking me for why. I said it's due to ""those files are causing confusion"". People reluctantly accepted.

About this sentence:

> Going back to this, it's a weird argument - if you installed MAS, then you should be at least okay (or already overcame it) with what Monika has done in the base game.

Although I have the same opinion, MAS _has_ taken care of players who have opinions on these plots in many small details, such as the safeTorRefDoki things mentioned by the issue author. Maybe keep these care won't hurt?",False,0
I really think we should ask the player for permission to delete other chr files.,RedJuicyFood,1428767280,7,1296444436,0,"Oh, and about this:

> Here we just called a utility function to clean up disk space and make folder structure more clear. Monika doesn't acknowledge this, not doing it herself through her console. There won't be a question to the player because Monika doesn't do this.

I think there is a small problem here, that is, people will naturally think Monika did this. Even if the original intention of the design is not like this, people will naturally think so.",False,0
I really think we should ask the player for permission to delete other chr files.,Wingdinggaster656,1428767280,8,1296456807,0,"> As I've said, in MAS those are just unused files. If they have a meaning for you, why are they there?

Because they are originally there.",False,0
I really think we should ask the player for permission to delete other chr files.,multimokia,1428767280,9,1296462012,0,"Given Monika mentions she fixed the bug about needing a chr file to exist, and thus no longer needing her own

as evidenced by the name easter eggs, it's clear the other can _technically_ exist as well.
As such, through some implied logic we can argue that technically they no longer need a chr file to exist either.
One can make of that what they will, though I realize this won't please everyone.

That said ultimately it is what it is. You installed a mod called Monika After Story, and from a usability perspective given the severe amount of tech support issues this has caused, I understand why they were blanket removed. It's one of those cases where UX vs immersion kinda needed to favour UX a little more as there genuinely were a lot of issues faced.

While asking may have worked, it also serves as kinda additional one-off dialogue, and at the moment given the state of the team, we can't exactly _afford_ that due to the sheer amount of time we've been able to contribute to the project (and by that I mean a significant lack thereof)

If you *really* want to keep them, a good idea would be to rename them to the current scheme of `monika` files, i.e. removing the `.chr` extension.

Then they're all cohesive too.",False,0
I really think we should ask the player for permission to delete other chr files.,lunulae,1428767280,10,1296472026,0,"I would like to give a small PSA to anyone here who wants to keep the files elsewhere for sentimental reasons, if MAS has already deleted them please do check your Recycle Bin, as the files may still be there.

addendum; I'm not 100% sure on this",False,0
I really think we should ask the player for permission to delete other chr files.,Justformas,1428767280,11,1296549580,0,"It is kind of insensitive to delete them with no in-game warning, no asking the player, and not even letting the player put new copies back if they're deleted on every startup. Although new copies wouldn't necessarily have the same sentimental value anyway; they're not the same copies that were kept there throughout MAS, potentially years.

Isn't there an instance where Monika wants something kept in the characters folder? Like an apology note she asks you to write if you're not on good terms with her? And she gets upset if you remove it? Similarly, having the character files kept there may help the player have peace over what happened in DDLC.

> What I want to say is, just check our mas_safeToRefDokis()

I don't think that would be enough for this, because this isn't just referencing or joking; this is removing/deleting the files. Players could be ok with the former but not the latter.

It shouldn't be that necessary to free up ~133 kb of space, nor is it the same thing as monika.chr because there are no other characters ""to take somewhere"" and get confused with those .chr files. Monika (or Chibi) could also further explain that .chr files are not used to take her out somewhere or put her back, and potentially ask about deleting these files.

> If you _really_ want to keep them, a good idea would be to rename them to the current scheme of `monika` files, i.e. removing the `.chr` extension.

Or can the mod just do that instead? So they don't get deleted without notice. Then no need to give notice or choice to the player. But maybe just one time only instead of on every startup, so players can change them back if they want without having to do it every time or whenever they might want to open them.",False,0
I really think we should ask the player for permission to delete other chr files.,dreamscached,1428767280,12,1296612885,0,"> I would like to give a small PSA to anyone here who wants to keep the files elsewhere for sentimental reasons, if MAS has already deleted them please do check your Recycle Bin, as the files may still be there.
> 
> addendum; I'm not 100% sure on this

As one who implemented that I can say the way they are removed does not put them into recycle bin. Putting stuff into recycling bin is a more complex process and in this case there was no need for it at all, files are removed irreversibly.",False,0
I really think we should ask the player for permission to delete other chr files.,dreamscached,1428767280,13,1296613625,0,"> Monika (or Chibi) could also further explain that .chr files are not used to take her out somewhere or put her back, and potentially ask about deleting these files.

This was said over and over, again and again, and Monika talks about this in the very beginning.

People **do not understand**, unfortunately.",False,Bitter frustration
I really think we should ask the player for permission to delete other chr files.,RedAISkye,1428767280,14,1296912845,0,"> > And by the way, I think it may be OK for Monika to delete her own file without permission - after all, it is her own file.

Disagree, player choice should be respected regardless.

I care about these character files, notes, easter-eggs etc. whether they serve a purpose or not in the game,
I still keep them safe no matter how many ""backups"" I've also kept.

The fact that these devs are trying hard to justify this by resorting to whatever technical nonsense instead of simply providing player choice and moving on which doesn't even take any effort speaks volumes about the amount of care they give.

Basically comes down to this:-
""Oh, we care about the tech-illiterates who lost Monika but just not about the people who value both Monika and her including other Dokis' character files because we personally don't value them, just restore from backup LAWL DUDE"".",False,Bitter frustration
I really think we should ask the player for permission to delete other chr files.,dreamscached,1428767280,15,1296939196,0,"You made a change that included removing code that was removing .chr files. If you want to change it, make a PR. It will be reviewed and discussed. This will be more of an action than repeating your claims here over and over.

Suggest a better way and show how you deem it better.",False,Bitter frustration
I really think we should ask the player for permission to delete other chr files.,RedAISkye,1428767280,16,1296942377,1,"> You made a change that included removing code that was removing .chr files. If you want to change it, make a PR. It will be reviewed and discussed. This will be more of an action than repeating your claims here over and over.

I made a change for my personal experience.

My suggestion was to include player choice and a kind user already have done that in detail ages ago when this change was being developed which you guys ignored with the ""IMO we're good with just removing them and they serve no purpose."".

> Suggest a better way and show how you deem it better.

Was already done like I've already stated.",False,Bitter frustration
The game is broken,JoltSystems,1465034831,1,1465034831,0,"**Describe the bug**
A clear and concise description of what the bug is.

When I play easy, it is too easy regardless of the map. When I pick medium, one difficulty level higher, I barely get a tank factory built and a battalion of enemy tanks show up at my doorstep. That to me is unplayable. 

**To Reproduce**
Steps to reproduce the behavior:
1. Go to '...'
2. Click on '....'
3. Scroll down to '....'
4. See error

**Expected behavior**
A clear and concise description of what you expected to happen.
There should be a steady flow, medium should be moved to insane.

**Screenshots or Videos**
If applicable, add screenshots to help explain your problem.

**Your System:**
 - OS: [e.g. Windows 10, Linux (Ubuntu 18.04)]
 - Game version: [e.g. 3.3.0, commit hash]

**Additional context**
Add any other context about the problem here.
",True,Bitter frustration
The game is broken,Zispah,1465034831,2,1328184570,0,"Assuming you meant skirmish, when starting with no bases and you build a factory after building three other structures, an AI building factory first could have already produced some tanks by that time. Most AIs are slow on easy, so you don't get that on easy. If you build factory first, medium will be easy too.",False,0
The game is broken,JoltSystems,1465034831,3,1328449389,0,"hey Zispah...i tried it both ways, got destroyed when i was slow and when i did the tank factory first...i destroyed im...thanx but...i need to try it a few times on different maps to make sure it wasnt a one-off...",False,Impatience
The game is broken,maxsupermanhd,1465034831,4,1328480752,1,"I think you need to increase your APM (Actions Per Second). Sorry to break news for you man but ""barely get a tank factory built and a battalion of enemy tanks show up at my doorstep"" sounds like you are just not experienced enough (noob). Try watching (not playing!) multiplayer games or replays from other people, you can learn that way.
Also I suggest you to set up hotkeys, you can find list of important ones in Discord.",False,Entitlement
Fix `--ext=c` generating buggy code,skull-squadron,1530248637,1,1530248637,0,"CentOS, Fedora, RHEL and so forth break without this.

Also, `Rakefile` doesn't need those options.",True,0
Fix `--ext=c` generating buggy code,skull-squadron,1530248637,2,1379928051,0,~~Where the hack is the squash rebase button in Microsoft Github?~~ ü¶∫,False,Vulgarity
Fix `--ext=c` generating buggy code,hsbt,1530248637,3,1379935514,0,https://github.com/puma/puma/issues/2371 seems not related this. Can you explain it?,False,0
Fix `--ext=c` generating buggy code,skull-squadron,1530248637,4,1379966860,0,@hsbt I misread puma/puma#2381 about offtopic puma/puma#2371. It's puma/puma#2381 only.,False,0
Fix `--ext=c` generating buggy code,skull-squadron,1530248637,5,1380040010,0,This also fixes #6268 and #6204,False,0
Fix `--ext=c` generating buggy code,skull-squadron,1530248637,6,1380180666,0,"I fixed both `bundle gem --ext=c foo` and `bundle gem --ext=rust bar` on several machines (CentOS 9 Stream x86_64, Ubuntu 22 LTS x86_64, and macOS 13 arm) I ran test benches against.

It's not ideal and probably not doing everything right, but it's closer to tried-and-true based on what already exists. I'm sure I'll see it's 100% wrong after I sleep.",False,0
Fix `--ext=c` generating buggy code,deivid-rodriguez,1530248637,7,1409090887,0,"This should've been fixed by #6298, so good to close, right?",False,0
Fix `--ext=c` generating buggy code,deivid-rodriguez,1530248637,8,1409091256,0,(Thanks for your work regardless @steakknife),False,0
Fix `--ext=c` generating buggy code,skull-squadron,1530248637,9,1411036103,0,@deivid-rodriguez No worries. I'll give it a shot to validate.,False,0
Fix `--ext=c` generating buggy code,skull-squadron,1530248637,10,1413267137,1,"@deivid-rodriguez Broken on mac and Linuxes. Absolutely no improvement. It's still not setting load paths correctly and it pollutes `lib/` with a binary shared object. And, it also isn't cleaned by `rake clean`. It belongs where native extensions always belong by maintaining convention over configuration and convention over useless churn. I have no horse in this race, so it can stay broken. ""Engineers"" who broken this should own it because it's a huge mess.",False,Bitter frustration
Self-Hosted Non-Premium v6 integration RTC error,mryellow,1636711509,1,1636711509,0,"`Uncaught (in promise) Error: Failed to get RTC instance not yet initialized.`

Seems in the process of attempting to vendor-lock with the CDN based implementations you've broken the library itself.

...

```
import tinymce from 'tinymce';

import 'tinymce/icons/default';

import 'tinymce/themes/silver';
import 'tinymce/models/dom';

import 'tinymce/skins/ui/oxide/skin.css';

import 'tinymce/plugins/advlist';
import 'tinymce/plugins/code'; // FIXME: Not picking up changes made in editor view? NOTE: Works again in v5
import 'tinymce/plugins/emoticons';
import 'tinymce/plugins/emoticons/js/emojis';
import 'tinymce/plugins/link';
import 'tinymce/plugins/lists';
import 'tinymce/plugins/table';

import contentUiSkinCss from 'tinymce/skins/ui/oxide/content.css';
import contentCss from 'tinymce/skins/content/default/content.css';
```


```
tinymce.init({
	selector: `#mg-wysiwyg-${this._uid}`,
	height: parseInt(this.height),
	readonly: this.disabled,
	//min_height: this.height,
	placeholder: this.placeholder,

	resize: 'both',

	menubar: false,
	plugins: ['advlist', 'code', 'emoticons', 'link', 'lists', 'table'],
	toolbar: 'undo redo | bold italic forecolor backcolor | bullist numlist checklist table | link emoticons | code',
	model: 'dom', // FIXME: Forcing ""dom"" because non-existant RTC ""Premium plugin"" getting in the way

	// Configuration required for local self-install
	skin: false,
	content_css: false,
	content_style: contentUiSkinCss.toString() + '\n' + contentCss.toString(),

	//promotion: false, // Oh yeah we really want adverts!

	// Bound to ""change keyup"" events as per https://github.com/tinymce/tinymce-vue/blob/b41c2a47eb8d9629eb01a41d6c6c633651f2d078/src/main/ts/Utils.ts#L115-L119
	init_instance_callback: editor => {
		editor.on('change keyup', e => {
			this.data = editor.getContent({ format: this.syntax })
		})
	},
});
```
",True,0
Self-Hosted Non-Premium v6 integration RTC error,mryellow,1636711509,2,1480551624,0,"This appears to be a cynical attempt to drive people into vendor-locking with your CDN.

Your desire to be paid for working on Open Source has cost me a lot of my time.

Was it the intention to waste my time or did you expect me to just hand over a monthly subscription?

I have decided I will not be supporting such a dark-pattern by moving to the CDN approach. Instead I will likely fork the project and do whatever I have to do in order to remove this  ""RTC"" nonsense.",False,Entitlement
Self-Hosted Non-Premium v6 integration RTC error,mryellow,1636711509,3,1480565569,0,"`5.10.7` works as it should and does not include these most recent attempts to break the library.

Those choosing not to vendor-lock themselves into unnecessary pointless parasitic CDN subscriptions can use this version and avoid any future _""upgrades""_.

This might explain why v5 has 3x the weekly downloads v6 does.",False,0
Self-Hosted Non-Premium v6 integration RTC error,TheSpyder,1636711509,4,1480686064,0,"If you're just going to go around our tracker posting about vendor lock-in you're the one wasting our time.

If you would like to continue using v6 - which I recommend as support for v5 [ends in a month](https://www.tiny.cloud/blog/tinymce-end-of-support/) - I'm happy to work with you to figure out what might be causing this error.",False,0
Self-Hosted Non-Premium v6 integration RTC error,TheSpyder,1636711509,5,1480706687,0,"During the TinyMCE startup process the editor checks whether the `rtc` plugin is loading, with the aim of allowing RTC to take over key sections of editor functionality. If RTC isn't in the plugin list it redirects everything back to the core.

https://github.com/tinymce/tinymce/blob/ece3bf774b9ae20910d073af5351f162c4fb52d5/modules/tinymce/src/core/main/ts/Rtc.ts#L294-L306

This was added in TinyMCE 5.3 so the source of the exception isn't recent, there's probably just some code trying to use one of those core features during setup instead of waiting for the `PreInit` event.

I am open to collaborating if you'd like to help track down this mistake. An unminified stack trace would be a good place to start, or a replication case.",False,0
Self-Hosted Non-Premium v6 integration RTC error,mryellow,1636711509,6,1481874912,0,"> I am open to collaborating

* The bug is caused by efforts to lock certain features and wouldn't exist otherwise
* The ticket is closed as if the reported bug does not exist as only the commercial version matters
* Given this is a commercial product only one of us is incentivised to contribute
* v5 will work fine for decades to come
* Have other fish to fry, wasted enough time on what should have been a simple install
",False,Impatience
Self-Hosted Non-Premium v6 integration RTC error,TheSpyder,1636711509,7,1482060775,1,"If you're just going to twist my words don't bother replying. This isn't an airport, you don't need to announce your departure.",False,Mocking
" add ""void"" as a native return type declaration",PhilETaylor,1659625133,1,1659625133,0,"> !!  2023-04-08T19:35:15+00:00 [info] User Deprecated: Method ""Symfony\Component\HttpKernel\Bundle\Bundle::build()"" might add ""void"" as a native return type declaration in the future. Do the same in child class ""Doctrine\Bundle\FixturesBundle\DoctrineFixturesBundle"" now to avoid errors or add an explicit @return annotation to suppress this message.",True,0
" add ""void"" as a native return type declaration",greg0ire,1659625133,2,1500983166,0,Can you please a look at the conversation at https://github.com/doctrine/DoctrineMigrationsBundle/pull/492?,False,0
" add ""void"" as a native return type declaration",PhilETaylor,1659625133,3,1500985072,0,So surely the simple solution is just to bump the version number and add new code to the new major version? I can't do that only your project can. I'm not seeing what the difficulty is here?,False,0
" add ""void"" as a native return type declaration",derrabus,1659625133,4,1501074772,0,The difficulty is that we don't plan a new major at the moment.,False,0
" add ""void"" as a native return type declaration",PhilETaylor,1659625133,5,1501078693,0,"> The difficulty is that we don't plan a new major at the moment.

No worries, This is interpreted as ""this project has no future"". Got it. ",False,0
" add ""void"" as a native return type declaration",greg0ire,1659625133,6,1501467718,0,"Well we can create a branch for sure, no biggie. Still, creating a major release just for this would be a bit overkill. Why don't you apply the alternate solution mentioned in the deprecation message? Then we can merge, after you provide a convincing apology for your behavior of course. ",False,Threat
" add ""void"" as a native return type declaration",PhilETaylor,1659625133,7,1501473017,0,">  after you provide a convincing apology for your behavior of course.

And that ends my contributions here and earns you a place on my block list. 

Im sorry you feel that way. Its your project do as you want. I was only trying to be helpful. No one was demanding anything. 

The fact is, the upstream project has chosen to add the return type, and now this project outputs warnings and you are choosing to ignore that rather than fix that. Your choice I guess. ",False,Threat
" add ""void"" as a native return type declaration",greg0ire,1659625133,8,1501611315,0,"Again, there is another way to fix it, if you would care to read the deprecation message, but slowly. I'd do it myself but I'm AFK. ",False,Impatience
" add ""void"" as a native return type declaration",PhilETaylor,1659625133,9,1501616646,1,"""Suppressing"" messages rather than fixing the root issue is not the way to avoid errors in the long term. 

Continued personal insults is also a quick way to lose contributors. You don't deserve my contribution following your aggressive tone, demanding of an apology for simply contributing and now telling me to read ""slowly""... Seems to happen often with Doctrine* projects. 

Seems I was only using this Bundle for a small thing anyway, so I have refactored that into a Symfony command and removed this dependancy from my projects. Simple. Now the warning is gone and you don't need to talk to me again.",False,Insulting
BC break from 2.14.1 to 2.14.2 Using (deprecated) short namespace alias,PhilETaylor,1665591134,1,1665591134,0,"<!--
Before reporting a BC break, please consult the upgrading document to make sure it's not an expected change: https://github.com/doctrine/orm/blob/2.9.x/UPGRADE.md
-->

### BC Break Report

<!-- Fill in the relevant information below to help triage your issue. -->

|    Q        |   A
|------------ | ------
| BC Break    | yes
| Version     | 2.14.2

#### Summary

YES I UNDERSTAND that the relied upon feature (short namespace alias) is deprecated BUT upgrading in a MINOR release from 2.14.1 to 2.14.2 should not break an app that was working fine in 2.14.1 - that goes against SEMVER

#### Previous behavior

No error messages when using doctrine/orm 2.14.1 

#### Current behavior

after composer update to doctrine/orm 2.14.2 from 2.14.1 - a minor release, now getting app crashes due to relying on deprecated features (I know I know) 

This is caused by now throwing Exceptions where no exceptions were previously thrown (because of a bug) https://github.com/doctrine/orm/pull/10489

<img width=""1280"" alt=""ScreenShot-2023-04-13-03 44 02"" src=""https://user-images.githubusercontent.com/400092/231634531-65ba81f5-41f7-41af-b810-c8e54e973df5.png"">


#### How to reproduce

The related package is https://github.com/j-guyon/CommandSchedulerBundle which is a 3 year old release. Yes I know I know I know... but still, a minor release of doctrine/orm should not have breaking changes ",True,Bitter frustration
BC break from 2.14.1 to 2.14.2 Using (deprecated) short namespace alias,greg0ire,1665591134,2,1506377545,0,"You are confusing MINOR and PATCH. 2.14.2 is not a minor release, it is a patch release. BUT. Short aliases are not a feature of `doctrine/orm`, they are a feature of `doctrine/persistence`, which means that if you rely on them, you (or rather, `jmose/command-scheduler-bundle`) should have `doctrine/persistence` in composer.json, and you should still be using `doctrine/persistence` 2 instead of `doctrine/persistence`3. If that were still the case, you would get a deprecation, not a crash.

A solution for you personally can be to downgrade to `doctrine/persistence` 2, and address the issue when you have the time.

> No error messages when using doctrine/orm 2.14.1

OK, there were no error messages, but was it working? I believe it either didn't, or worked by accident. If you were using persistence 3 at the time, then you were not using short aliases.

To fully understand the issue, it would be great to have [a stack trace](https://symfony.com/doc/current/contributing/code/stack_trace.html)",False,Impatience
BC break from 2.14.1 to 2.14.2 Using (deprecated) short namespace alias,PhilETaylor,1665591134,3,1506453682,0,"In my defence it was 4am (now 8am) zzz 

Personally I just removed the bundle and replaced with zenstruck scheduler within 20 mins and deployed that to production already - it's a far superior product anyway

You can close this as it doesn't really affect me now but the fact remains updating a PATCH version broke otherwise working (albeit old bundle) code from working ",False,0
BC break from 2.14.1 to 2.14.2 Using (deprecated) short namespace alias,greg0ire,1665591134,4,1506456921,0,"As I said, I suspect it wasn't actually working. ",False,0
BC break from 2.14.1 to 2.14.2 Using (deprecated) short namespace alias,PhilETaylor,1665591134,5,1506463530,1,"The scheduler has been running every min of every day for almost 10 years. It's the backbone of a service that has made me several million pounds... but hey, it's fixed now and I can go to bed. 

Off topic: Also, when someone sponsors you $100 it would be nice, the most minimal thing to do, is to acknowledge it... but you are not alone, many developers don't even acknowledge GitHub sponsorships - ah well. ",False,Irony
DateTimeImmutable::createFromFormat parses non-existing timezone offsets,dbertovi,1687989535,1,1687989535,0,"### Description

The following code:

```php
<?php
//OK Example:
$time = '2023-02-22 19:44:42.85857+05:30';
$date = DateTimeImmutable::createFromFormat('Y-m-d H:i:s.uP', $time);
echo $date->format('P');
//OUTPUT = +05:30 //OK

//BUG Example:
$time = '2023-02-22 19:44:42.127801-110';
$date = DateTimeImmutable::createFromFormat('Y-m-d H:i:s.uP', $time);
echo $date->format('P');
//OUTPUT = -01:10 //NOT OK, TIMEZONE OFFSET DOESN'T EXIST, must throw an exception
```

But I expected this output instead:
Fatal Error

Shouldn't PHP check the proper existing time zone offset, when it already properly lists timezone identifiers (a.k.a. through DateTimeZone::listIdentifiers())?


### PHP Version

PHP 8.0.28

### Operating System

LiteSpeed",True,Bitter frustration
DateTimeImmutable::createFromFormat parses non-existing timezone offsets,iluuu1994,1687989535,2,1527255788,0,I'm guessing this is by-design. If you specify an offset you should know what you're doing. @derickr Can you confirm/deny?,False,Irony
DateTimeImmutable::createFromFormat parses non-existing timezone offsets,dbertovi,1687989535,3,1527312982,0,"I'm guessing not, but am open to that. Actually, completely other thing that I was doing lead me to this ambiguity. So for example, this function works like charm:
```php
<?php
function getIsDaylightSaving($zoneid = 'America/Los_Angeles') {
    
    try {

        $date = new DateTime('now', new DateTimeZone($zoneid));
        return $date->format('I');
        
    } catch (Exception $e) {
        
        return 400;
        
    }
    
}
```
So if one try to do following:
```php
<?php
//get daylight saving time status from timezone identifier, 
$dst = getIsDaylightSaving('America/New_York');
//OUTPUT = 1 //which is OK

//but if I use foo timezone identifier like (this one differs built in IANA timezone db):
$dst = getIsDaylightSaving('ABBA/Mama_Mia');
//The instance new DateTime('now', new DateTimeZone($zoneid)); will throw an exception which will be caught in this function
//so the OUTPUT will be 400 //which is also OK.
```
So my point is that I was surprised that PHP didn't care about timezone offsets, but do care about timezone identifiers, since timezone offsets are only the following: 
```php
<?php
$offsets = array('+00:00', '+01:00', '+02:00', '+03:00', '+03:30', '+04:00', '+04:30', '+05:00', '+05:30', '+05:45', '+06:00', '+06:30', '+07:00', '+08:00', '+08:45', '+09:00', '+09:30', '+10:00', '+10:30', '+11:00', '+12:00', '+12:45', '+13:00', '+14:00', '-01:00', '-02:00', '-02:30', '-03:00', '-04:00', '-05:00', '-06:00', '-07:00', '-08:00', '-09:00', '-09:30', '-10:00', '-11:00', '-12:00');
```
For that reason, I think it is a bug, or even some unfinished job. Anyway. I 'dodged a bullet' simply by checking for the offset myself. Everything else is explained in my first comment (i.e., _$time_ variables are expected user input variables).
Hope it helps.
",False,0
DateTimeImmutable::createFromFormat parses non-existing timezone offsets,damianwadley,1687989535,4,1527674756,0,"The offset isn't some magic string that identifies a location like the named timezones. It's an offset. An amount of hours and minutes ahead or behind UTC.

So what's the added value that comes with restricting the offset to only those that are in active, official use by some area of the world? What problems does doing so solve, or what advantages does it offer?

If you're getting these values from the user then how do you know that the user didn't intend -01:10 for some reason? And why ask them for numeric offsets when the named ones are so much more human-friendly?",False,Impatience
DateTimeImmutable::createFromFormat parses non-existing timezone offsets,dbertovi,1687989535,5,1527730238,0,"> The offset isn't some magic string that identifies a location like the named timezones. It's an offset. An amount of hours and minutes ahead or behind UTC.
> 
> So what's the added value that comes with restricting the offset to only those that are in active, official use by some area of the world? What problems does doing so solve, or what advantages does it offer?
> 
> If you're getting these values from the user then how do you know that the user didn't intend -01:10 for some reason? And why ask them for numeric offsets when the named ones are so much more human-friendly?

What is this comment? Some kind of words game? Joke? That I am not into? Apropos what is human-friendly, or not, all those values can be a device snapshot. Some distant device snapshot having an application installed on it, that is programmed by a distant developer on certain distant operating system, with some distant programming language, that a developer will use, through his application, to fetch certain data... Now, that developer can be a professional, but can also be not-professional. And now going deeper in that story that here is being presented to my report is pure game of words as this answer is. Yada yada yada ... Please don't waste my time or yours on unconstructive comments.

That is not a topic here. Anyone's programming tasks, styles or stuff like that is not a topic here. 

My point is that a PHP allows -01:10 value where it shouldn't. The end. Schluss. Ende. And that is all. And I didn't expect it to do so, to my surprise, though I use PHP on a daily basis since it's version 3.

",False,Insulting
DateTimeImmutable::createFromFormat parses non-existing timezone offsets,damianwadley,1687989535,6,1527782296,0,"That was a lot of words to say ""it's wrong because I think it's wrong"".

Why is it wrong to allow an offset that isn't in use by some (named) timezone? Why must it only allow offsets that can be found applied in (named) timezones?",False,Bitter frustration
DateTimeImmutable::createFromFormat parses non-existing timezone offsets,dbertovi,1687989535,7,1528125573,0,"Well, because of your 'human readability' - 'user-friendliness' etc. Why would I as a developer risk using PHP DateTime object (DateTimeImmutable), without being sure it'll do all the jobs, because I do not want some Carbon datetime library to mess with my RFC 3339 datetime standards implementation, with my ISO 8601 datetime standards implementation and with WHATWG HTML living standard datetime standard implementation. That's why. But I am flexible. On constructive part of it. And I will always find a solution. This I consider a bug. There's only 37/38 timezone offsets that should have been properly parsed by those functions. Our planet Earth is only so big.",False,Bitter frustration
DateTimeImmutable::createFromFormat parses non-existing timezone offsets,dbertovi,1687989535,8,1528137661,1,"And again, 'that was a lot...' - you are wasting my time.",False,Bitter frustration
